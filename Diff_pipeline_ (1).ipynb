{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl==0.9.0"
      ],
      "metadata": {
        "id": "vGDo1zQu8H2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a5b38e-a9fc-42ce-ccc8-365d98a9ff5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl==0.9.0\n",
            "  Downloading dgl-0.9.0-cp39-cp39-manylinux1_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from dgl==0.9.0) (1.22.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from dgl==0.9.0) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from dgl==0.9.0) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dgl==0.9.0) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dgl==0.9.0) (4.65.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.9/dist-packages (from dgl==0.9.0) (3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl==0.9.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl==0.9.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl==0.9.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl==0.9.0) (2022.12.7)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph representation"
      ],
      "metadata": {
        "id": "k0qbJ6GXW31W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import dgl\n",
        "import torch.nn as nn\n",
        "import dgl.nn.pytorch as dglnn\n",
        "\n",
        "class DiffusionNN(nn.Module):\n",
        "    def __init__(self, num_feats, num_classes, num_layers, hidden_dim):\n",
        "        super(DiffusionNN, self).__init__()\n",
        "        \n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        self.diffusion_layers = nn.ModuleList()\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "\n",
        "        # Graph convolutional layers\n",
        "        for i in range(num_layers):\n",
        "            if i == 0:\n",
        "                self.conv_layers.append(dglnn.GraphConv(num_feats, hidden_dim))\n",
        "            else:\n",
        "                self.conv_layers.append(dglnn.GraphConv(hidden_dim, hidden_dim))\n",
        "        \n",
        "        # Diffusion layers\n",
        "        for i in range(num_layers):\n",
        "            self.diffusion_layers.append(dglnn.GraphConv(hidden_dim, hidden_dim, norm='both', weight=False))\n",
        "\n",
        "        # Fully connected layers\n",
        "        for i in range(num_layers):\n",
        "            if i == 0:\n",
        "                self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            else:\n",
        "                self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        \n",
        "        self.fc_layers.append(nn.Linear(hidden_dim, num_classes))\n",
        "        \n",
        "    \n",
        "    def forward(self, g, x=None):\n",
        "        if x is None:\n",
        "            h = g.ndata['feat']\n",
        "        else:\n",
        "            h = x\n",
        "\n",
        "        # Graph convolutional layers\n",
        "        for conv_layer in self.conv_layers:\n",
        "            h = conv_layer(g, h)\n",
        "            h = torch.relu(h)\n",
        "\n",
        "        # Diffusion layers\n",
        "        for diffusion_layer in self.diffusion_layers:\n",
        "            h = diffusion_layer(g, h)\n",
        "            h = torch.relu(h)\n",
        "\n",
        "        # Fully connected layers\n",
        "        for fc_layer in self.fc_layers:\n",
        "            h = fc_layer(h)\n",
        "            h = torch.relu(h)\n",
        "\n",
        "        return h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZFp3U9wWytj",
        "outputId": "22e0ba1a-308a-480c-b6f1-9d456e4c1dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code for testing on DNA sequences with G-quadruplexes\n",
        "# Load DNA sequences with G-quadruplexes (assume each sequence is a string)\n",
        "sequences = [\n",
        "    'GGGGGAGGGGGA',\n",
        "    'GGGGGAGGGAGGGG',\n",
        "    'GGGGGAGGGAGGGGGGGGGGAGGGGGA'\n",
        "]\n",
        "\n",
        "# Convert sequences to graphs (assume each nucleotide is a node)\n",
        "graphs = []\n",
        "for sequence in sequences:\n",
        "    num_nodes = len(sequence)\n",
        "    g = dgl.DGLGraph()\n",
        "    g.add_nodes(num_nodes)\n",
        "    g.ndata['feat'] = torch.eye(num_nodes)  # one-hot encoding of nucleotides\n",
        "    for i in range(num_nodes-1):\n",
        "        g.add_edge(i, i+1)\n",
        "    g.add_edge(num_nodes-1, 0)\n",
        "    graphs.append(g)\n",
        "\n",
        "# Create DiffusionNN model\n",
        "model = DiffusionNN(num_feats=12, num_classes=2, num_layers=3, hidden_dim=64)\n",
        "\n",
        "# Test forward pass on first graph\n",
        "output = model(graphs[0])\n",
        "print(output)"
      ],
      "metadata": {
        "id": "q3obxCbh8Hxu",
        "outputId": "a2688e2b-efab-4579-abc9-0f4e2609d0c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0578, 0.0000],\n",
            "        [0.0600, 0.0000],\n",
            "        [0.0611, 0.0000],\n",
            "        [0.0602, 0.0000],\n",
            "        [0.0586, 0.0000],\n",
            "        [0.0608, 0.0000],\n",
            "        [0.0599, 0.0000],\n",
            "        [0.0617, 0.0000],\n",
            "        [0.0597, 0.0000],\n",
            "        [0.0603, 0.0000],\n",
            "        [0.0600, 0.0000],\n",
            "        [0.0617, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n",
            "/usr/local/lib/python3.9/dist-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
            "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n",
            "/usr/local/lib/python3.9/dist-packages/dgl/backend/pytorch/tensor.py:352: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), \"Cannot convert view \" \\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vfVvtCzfCLAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "o0M7DMgGCxGe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset class\n",
        "class DNAData(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "        return sequence, label\n",
        "\n",
        "# Load DNA sequences with G-quadruplexes (assume each sequence is a string)\n",
        "sequences = [\n",
        "    'GGGGGAGGGGGA',\n",
        "    'GGGGGAGGGAGGGG',\n",
        "    'GGGGGAGGGAGGGGGGGGGGAGGGGGA',\n",
        "    'AGGGGGG',\n",
        "    'GGGGAGGGGGG',\n",
        "    'GGGGGGGAGGGG'\n",
        "]\n",
        "\n",
        "# Assign labels to sequences (assume first 3 are positive examples and last 3 are negative examples)\n",
        "labels = np.array([1, 1, 1, 0, 0, 0])\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_sequences, val_sequences, train_labels, val_labels = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create dataset and dataloader objects for train and validation sets\n",
        "train_dataset = DNAData(train_sequences, train_labels)\n",
        "val_dataset = DNAData(val_sequences, val_labels)\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create DiffusionNN model\n",
        "model = DiffusionNN(num_feats=12, num_classes=2, num_layers=3, hidden_dim=64)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "\n",
        "    # Train\n",
        "    model.train()\n",
        "    for i, (sequences, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Convert sequences to graphs\n",
        "        graphs = []\n",
        "        for sequence in sequences:\n",
        "            num_nodes = len(sequence)\n",
        "            g = dgl.DGLGraph()\n",
        "            g.add_nodes(num_nodes)\n",
        "            g.ndata['feat'] = torch.eye(num_nodes)  # one-hot encoding of nucleotides\n",
        "            for i in range(num_nodes-1):\n",
        "                g.add_edge(i, i+1)\n",
        "            g.add_edge(num_nodes-1, 0)\n",
        "            graphs.append(g)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(graphs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item() * batch_size\n",
        "\n",
        "        # Backward pass and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_acc += torch.sum(preds == labels)\n",
        "\n",
        "    # Calculate average loss and accuracy for train set\n",
        "    train_loss /= len(train_dataset)"
      ],
      "metadata": {
        "id": "-UFb2HoDCKlP",
        "outputId": "db54d082-3556-472e-eecc-c9852d2ec571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-8722719f7162>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-26838efdd744>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndata'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DNA sequences with G-quadruplexes (assume each sequence is a string)\n",
        "sequences = [    'GGGGGAGGGGGA',    'GGGGGAGGGAGGGG',    'GGGGGAGGGAGGGGGGGGGGAGGGGGA']\n",
        "\n",
        "# Convert sequences to graphs (assume each nucleotide is a node)\n",
        "graphs = []\n",
        "for sequence in sequences:\n",
        "    num_nodes = len(sequence)\n",
        "    g = dgl.DGLGraph()\n",
        "    g.add_nodes(num_nodes)\n",
        "    g.ndata['feat'] = torch.eye(num_nodes)  # one-hot encoding of nucleotides\n",
        "    for i in range(num_nodes-1):\n",
        "        g.add_edge(i, i+1)\n",
        "    g.add_edge(num_nodes-1, 0)\n",
        "    graphs.append(g)\n",
        "\n",
        "# Create DiffusionNN model\n",
        "model = DiffusionNN(num_feats=4, num_classes=2, num_layers=3, hidden_dim=64)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    \n",
        "    for graph in graphs:\n",
        "        # Forward pass\n",
        "        outputs = model(graph)\n",
        "        \n",
        "        # Compute loss\n",
        "        labels = torch.tensor([0, 1])\n",
        "        loss = criterion(outputs.unsqueeze(0), labels.unsqueeze(0))\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        # Compute accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        if predicted[0] == 1:\n",
        "            epoch_acc += 1\n",
        "            \n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # Print epoch information\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}: Loss: {epoch_loss/len(graphs):.4f}, Accuracy: {epoch_acc/len(graphs):.4f}')"
      ],
      "metadata": {
        "id": "QzDOxIMQFFIJ",
        "outputId": "b57047b3-f0e0-4bd3-9524-4330c3a7132a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-8f8798cffaf9>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-26838efdd744>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Graph convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/dgl/nn/pytorch/conv/graphconv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mrst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                     \u001b[0mrst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_norm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (12x12 and 4x64)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise on sequence"
      ],
      "metadata": {
        "id": "vkplBbX-SPZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Define the DNA sequence\n",
        "seq = \"AGCTATCGAGGGCTAGCATGCTAGCATCGACTAGCTACG\"\n",
        "\n",
        "# Create a one-hot encoded representation of the sequence\n",
        "nucleotides = \"ACGT\"\n",
        "seq_one_hot = np.zeros((len(seq), 4 ))\n",
        "for i, base in enumerate(seq):\n",
        "    if base in nucleotides:\n",
        "        seq_one_hot[i, nucleotides.index(base)] = 1\n",
        "\n",
        "# Add Gaussian noise to the one-hot encoded sequence\n",
        "noise_std = 0.1\n",
        "noise = torch.randn(seq_one_hot.shape) * noise_std\n",
        "seq_noisy = seq_one_hot + noise.numpy()\n",
        "\n",
        "# Convert the noisy sequence back to a string representation\n",
        "seq_noisy_str = \"\"\n",
        "for i in range(seq_noisy.shape[0]):\n",
        "    if np.argmax(seq_noisy[i]) == 0:\n",
        "        seq_noisy_str += \"A\"\n",
        "    elif np.argmax(seq_noisy[i]) == 1:\n",
        "        seq_noisy_str += \"C\"\n",
        "    elif np.argmax(seq_noisy[i]) == 2:\n",
        "        seq_noisy_str += \"G\"\n",
        "    elif np.argmax(seq_noisy[i]) == 3:\n",
        "        seq_noisy_str += \"T\"\n",
        "\n",
        "print(\"Original sequence:\", seq)\n",
        "print(\"Noisy sequence:\", seq_noisy_str)"
      ],
      "metadata": {
        "id": "U7uL__J1LK27",
        "outputId": "b2ee9a45-70f2-4177-ce2b-b9d536d1ae96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sequence: AGCTATCGAGGGCTAGCATGCTAGCATCGACTAGCTACG\n",
            "Noisy sequence: AGCTATCGAGGGCTAGCATGCTAGCATCGACTAGCTACG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "g--_S1ue1L57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a diffusion"
      ],
      "metadata": {
        "id": "1kkpai2z0_gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionProcess(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, timesteps):\n",
        "        super(DiffusionProcess, self).__init__()\n",
        "        self.timesteps = timesteps\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for i in range(self.timesteps):\n",
        "            noise = torch.randn_like(x) / np.sqrt(self.timesteps)\n",
        "            x += noise\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Define the DNN model\n",
        "class DNNGenerator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, timesteps):\n",
        "        super(DNNGenerator, self).__init__()\n",
        "        self.diffusion = DiffusionProcess(input_dim, output_dim, hidden_dim, num_layers, timesteps)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.diffusion(x)\n",
        "\n",
        "def train_dna(model, optimizer, criterion, train_loader, test_loader, num_epochs):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "    roc_scores = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_total = 0\n",
        "        train_correct = 0\n",
        "        \n",
        "        for i, (seqs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model(seqs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item() * seqs.size(0)\n",
        "            train_total += seqs.size(0)\n",
        "            train_correct += (torch.argmax(outputs, 1) == torch.argmax(labels, 1)).sum().item()\n",
        "            \n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}\".format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "        \n",
        "        train_loss /= train_total\n",
        "        train_acc = train_correct / train_total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        test_total = 0\n",
        "        test_correct = 0\n",
        "        y_true = []\n",
        "        y_scores = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for seqs, labels in test_loader:\n",
        "                outputs = model(seqs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                test_loss += loss.item() * seqs.size(0)\n",
        "                test_total += seqs.size(0)\n",
        "                test_correct += (torch.argmax(outputs, 1) == torch.argmax(labels, 1)).sum().item()\n",
        "                \n",
        "                y_true += labels.cpu().numpy().tolist()\n",
        "                y_scores += torch.softmax(outputs, dim=1)[:, 1].cpu().numpy().tolist()\n",
        "        \n",
        "        test_loss /= test_total\n",
        "        test_acc = test_correct / test_total\n",
        "        test_losses.append(test_loss)\n",
        "        test_accs.append(test_acc)\n",
        "        \n",
        "        roc_score = roc_auc_score(y_true, y_scores)\n",
        "        roc_scores.append(roc_score)\n",
        "        \n",
        "        print(\"Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.4f}, ROC AUC: {:.4f}\".format(epoch+1, num_epochs, train_loss, train_acc, test_loss, test_acc, roc_score))\n",
        "    \n",
        "    # Plot the loss and accuracy curves\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(test_losses, label=\"Test Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss Curves\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_accs, label=\"Train Acc\")\n",
        "    plt.plot(test_accs, label=\"Test Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy Curves\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot the ROC curve\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_scores)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "    plt.title(\"ROC Curve (AUC = {:.4f})\".format(roc_score))\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.show()\n",
        "    \n",
        "    return model, train_losses, test_losses, train_accs, test_accs, roc_scores"
      ],
      "metadata": {
        "id": "mUMpHO3OSOQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters\n",
        "input_dim = 4 # One-hot encoded sequence\n",
        "output_dim = 4 # One-hot encoded sequence\n",
        "hidden_dim = 512 # Hidden layer size\n",
        "num_layers = 4 # Number of layers in the diffusion process\n",
        "timesteps = 100 # Number of diffusion steps\n",
        "batch_size = 32\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 10\n",
        "\n",
        "# Create a DNN generator instance\n",
        "generator = DNNGenerator(input_dim, output_dim, hidden_dim, num_layers, timesteps)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Create a dummy dataset loader\n",
        "train_data = torch.randn((1000, input_dim))\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "test_data = torch.randn((1000, input_dim))\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Train the generator\n",
        "train_dna(generator, optimizer, criterion, train_loader, test_loader, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "BtCZScAY1Ehr",
        "outputId": "208e400c-26ae-4336-de99-8f6d59f2285d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-395260b453c9>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain_dna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6b471373e22f>\u001b[0m in \u001b[0;36mtrain_dna\u001b[0;34m(model, optimizer, criterion, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate quadruplexes"
      ],
      "metadata": {
        "id": "TE-LVNlOXLkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Set the length of the DNA sequences\n",
        "seq_length = 50\n",
        "\n",
        "# Set the number of sequences for training and testing\n",
        "num_train = 1000\n",
        "num_test = 200\n",
        "\n",
        "# Generate random DNA sequences with G-quadruplexes for training\n",
        "train_seqs = []\n",
        "train_labels = []\n",
        "for i in range(num_train):\n",
        "    seq = \"\"\n",
        "    for j in range(seq_length):\n",
        "        base = random.choice([\"A\", \"C\", \"G\", \"T\"])\n",
        "        seq += base\n",
        "        if j >= 3 and seq[j-3:j+1] in [\"GGGG\", \"CCCC\"]:\n",
        "            # Add a G-quadruplex\n",
        "            seq = seq[:j-3] + seq[j-3:j+1].lower() + seq[j+1:]\n",
        "    train_seqs.append(seq)\n",
        "    if \"g\" in seq.lower():\n",
        "        train_labels.append(1)  # Positive label if G-quadruplex is present\n",
        "    else:\n",
        "        train_labels.append(0)  # Negative label if G-quadruplex is absent\n",
        "\n",
        "# Generate random DNA sequences with G-quadruplexes for testing\n",
        "test_seqs = []\n",
        "test_labels = []\n",
        "for i in range(num_test):\n",
        "    seq = \"\"\n",
        "    for j in range(seq_length):\n",
        "        base = random.choice([\"A\", \"C\", \"G\", \"T\"])\n",
        "        seq += base\n",
        "        if j >= 3 and seq[j-3:j+1] in [\"GGGG\", \"CCCC\"]:\n",
        "            # Add a G-quadruplex\n",
        "            seq = seq[:j-3] + seq[j-3:j+1].lower() + seq[j+1:]\n",
        "    test_seqs.append(seq)\n",
        "    if \"g\" in seq.lower():\n",
        "        test_labels.append(1)  # Positive label if G-quadruplex is present\n",
        "    else:\n",
        "        test_labels.append(0)  # Negative label if G-quadruplex is absent"
      ],
      "metadata": {
        "id": "paePytcgOeCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs"
      ],
      "metadata": {
        "id": "WYTNFJbzOqif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the one-hot encoding dictionary\n",
        "one_hot_dict = {\n",
        "    \"A\": [1, 0, 0, 0],\n",
        "    \"C\": [0, 1, 0, 0],\n",
        "    \"G\": [0, 0, 1, 0],\n",
        "    \"T\": [0, 0, 0, 1],\n",
        "    \"a\": [1, 0, 0, 0],\n",
        "    \"c\": [0, 1, 0, 0],\n",
        "    \"g\": [0, 0, 1, 0],\n",
        "    \"t\": [0, 0, 0, 1]\n",
        "}\n",
        "\n",
        "# Convert the DNA sequences to one-hot encoding\n",
        "def dna_to_one_hot(seq):\n",
        "    one_hot_seq = []\n",
        "    for base in seq:\n",
        "        one_hot_seq.append(one_hot_dict[base])\n",
        "    return np.array(one_hot_seq)\n",
        "\n",
        "train_seqs_one_hot = np.array([dna_to_one_hot(seq) for seq in train_seqs])\n",
        "test_seqs_one_hot = np.array([dna_to_one_hot(seq) for seq in test_seqs])"
      ],
      "metadata": {
        "id": "PEfmQ_UbPT-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs_one_hot"
      ],
      "metadata": {
        "id": "0qZj7XHNG6bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the diffusion neural network\n",
        "class DiffusionNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(DiffusionNet, self).__init__()\n",
        "        self.diffusion = nn.Sequential(\n",
        "            nn.Linear(4, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 4)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.diffusion(x).view(-1, seq_length, 4)\n",
        "\n",
        "# Set the device to use for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the training parameters\n",
        "batch_size = 32\n",
        "num_epochs = 1\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Create the DataLoader for training\n",
        "train_dataset = TensorDataset(torch.Tensor(train_seqs_one_hot), torch.Tensor(train_labels))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize the diffusion neural network and the optimizer\n",
        "diffusion_net = DiffusionNet(input_size=seq_length*4, hidden_size=256, output_size=seq_length*4).to(device)\n",
        "optimizer = optim.Adam(diffusion_net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the loss function and the accuracy function\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "accuracy_fn = lambda output, target: (output.sigmoid().round() == target).sum().item() / target.numel()\n",
        "\n",
        "# Train the diffusion neural network\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "for epoch in range(num_epochs):\n",
        "    # Train for one epoch\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        # Move the batch to the device\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # Generate noisy inputs with diffusion\n",
        "        noise = torch.randn_like(batch_x).to(device)\n",
        "        noisy_batch_x = diffusion_net(noise)\n",
        "\n",
        "        # Compute the loss and the accuracy\n",
        "        loss = loss_fn(noisy_batch_x, batch_x)\n",
        "        accuracy = accuracy_fn(noisy_batch_x, batch_x)\n",
        "\n",
        "        # Backpropagate and update the parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * batch_x.shape[0]\n",
        "        train_acc += accuracy * batch_x.shape[0]\n",
        "\n",
        "    # Compute the average training loss and accuracy for the epoch\n",
        "    train_loss /= len(train_dataset)\n",
        "    train_acc /= len(train_dataset)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    \n",
        "    # Print the training loss and accuracy for the epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "# Generate noisy DNA sequences with G-quadruplexes regions\n",
        "num_samples = 5\n",
        "with torch.no_grad():\n",
        "    for i in range(num_samples):\n",
        "        # Generate a random input noise\n",
        "        noise = torch.randn(1, seq_length*4).to(device)\n",
        "\n",
        "        # Generate a noisy DNA sequence with diffusion\n",
        "        noisy_seq = diffusion_net(noise).sigmoid().round().cpu().numpy()[0]\n",
        "\n",
        "        # Decode the noisy sequence to a DNA sequence\n",
        "        decoded_seq = []\n",
        "        for j in range(0, seq_length*4, 4):\n",
        "            base = noisy_seq[j:j+4]\n",
        "            if np.array_equal(base, [1, 0, 0, 0]):\n",
        "                decoded_seq.append('A')\n",
        "            elif np.array_equal(base, [0, 1, 0, 0]):\n",
        "                decoded_seq.append('C')\n",
        "            elif np.array_equal(base, [0, 0, 1, 0]):\n",
        "                decoded_seq.append('G')\n",
        "            elif np.array_equal(base, [0, 0, 0, 1]):\n",
        "                decoded_seq.append('T')\n",
        "        decoded_seq = ''.join(decoded_seq)\n",
        "\n",
        "        # Print the noisy and decoded sequences\n",
        "        print(f\"Noisy sequence {i+1}: {noisy_seq}\")\n",
        "        print(f\"Decoded sequence {i+1}: {decoded_seq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "9lBU0F5HQwqO",
        "outputId": "8133f859-c676-4637-ae9f-d8f03af370a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 32/32 [00:03<00:00, 10.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Train Loss: 0.5867, Train Acc: 0.7351\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4a8810582c49>\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Generate a noisy DNA sequence with diffusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mnoisy_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffusion_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Decode the noisy sequence to a DNA sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4a8810582c49>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Set the device to use for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x200 and 4x256)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "8ZDXQ-_7IPp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_DNA_seq(length):\n",
        "    \"\"\"Function to generate a random DNA sequence of given length\"\"\"\n",
        "    bases = ['A', 'C', 'G', 'T']\n",
        "    seq = ''.join(np.random.choice(bases, size=length))\n",
        "    return seq\n",
        "\n",
        "def add_gaussian_noise(seq, mean, std):\n",
        "    \"\"\"Function to add Gaussian noise to a given DNA sequence\"\"\"\n",
        "    noise = np.random.normal(mean, std, len(seq))\n",
        "    noisy_seq = seq + noise\n",
        "    return noisy_seq\n",
        "\n",
        "def generate_noisy_and_clean_seqs(num_seqs, seq_length, noise_mean, noise_std):\n",
        "    \"\"\"Function to generate noisy and clean DNA sequences\"\"\"\n",
        "    noisy_seqs = []\n",
        "    clean_seqs = []\n",
        "    \n",
        "    for i in range(num_seqs):\n",
        "        # Generate clean DNA sequence\n",
        "        clean_seq = generate_DNA_seq(seq_length)\n",
        "        \n",
        "        # Add Gaussian noise to clean sequence\n",
        "        noisy_seq = add_gaussian_noise(clean_seq, noise_mean, noise_std)\n",
        "        \n",
        "        # Append to lists\n",
        "        noisy_seqs.append(noisy_seq)\n",
        "        clean_seqs.append(clean_seq)\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    noisy_seqs = np.array(noisy_seqs)\n",
        "    clean_seqs = np.array(clean_seqs)\n",
        "    \n",
        "    return noisy_seqs, clean_seqs"
      ],
      "metadata": {
        "id": "C2iX60a5Id24"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_noisy_and_clean_seqs(seq_length, num_seqs, noise_mean, noise_stddev):\n",
        "    noisy_seqs = []\n",
        "    clean_seqs = []\n",
        "    for i in range(num_seqs):\n",
        "        # Generate clean sequence\n",
        "        clean_seq = np.random.choice(['A', 'T', 'C', 'G'], size=seq_length)\n",
        "        clean_seq = np.array(clean_seq, dtype='|S1').astype(float)\n",
        "        \n",
        "        # Add Gaussian noise to clean sequence\n",
        "        noise = np.random.normal(loc=noise_mean, scale=noise_stddev, size=seq_length)\n",
        "        noisy_seq = clean_seq + noise\n",
        "        \n",
        "        # Append sequences to lists\n",
        "        clean_seqs.append(clean_seq)\n",
        "        noisy_seqs.append(noisy_seq)\n",
        "        \n",
        "    return np.array(noisy_seqs), np.array(clean_seqs)"
      ],
      "metadata": {
        "id": "eEjfhTyxJI5w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "# Assuming your noisy and clean sequences are stored in noisy_seqs and clean_seqs respectively\n",
        "noisy_seqs_preprocessed = []\n",
        "clean_seqs_preprocessed = []\n",
        "\n",
        "# Set the standard deviation of the Gaussian noise\n",
        "std_dev = 0.5\n",
        "\n",
        "# Loop over each sequence\n",
        "for noisy_seq, clean_seq in zip(noisy_seqs, clean_seqs):\n",
        "    # Convert the sequences to numpy arrays\n",
        "    noisy_seq = np.array(list(noisy_seq))\n",
        "    clean_seq = np.array(list(clean_seq))\n",
        "    \n",
        "    # Apply the Gaussian noise to the noisy sequence\n",
        "    noisy_seq = noisy_seq.astype(float) + np.random.normal(0, std_dev, noisy_seq.shape)\n",
        "    \n",
        "    # # Normalize the sequences\n",
        "    # noisy_seq /= 4.\n",
        "    # clean_seq /= 4.\n",
        "    \n",
        "    # Smooth the noisy sequence with a Gaussian filter\n",
        "    noisy_seq = gaussian_filter(noisy_seq, sigma=1)\n",
        "    \n",
        "    # Add the preprocessed sequences to the lists\n",
        "    noisy_seqs_preprocessed.append(noisy_seq)\n",
        "    clean_seqs_preprocessed.append(clean_seq)\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "noisy_seqs_preprocessed = np.array(noisy_seqs_preprocessed)\n",
        "clean_seqs_preprocessed = np.array(clean_seqs_preprocessed)\n",
        "\n",
        "# Reshape the arrays to be compatible with the U-Net input shape\n",
        "noisy_seqs_preprocessed = np.expand_dims(noisy_seqs_preprocessed, axis=1)\n",
        "clean_seqs_preprocessed = np.expand_dims(clean_seqs_preprocessed, axis=1)"
      ],
      "metadata": {
        "id": "syKZ3kzgWwHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539de18c-44e6-438c-d8ae-cff3a995e573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-f21ed5acb40a>:2: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-Net"
      ],
      "metadata": {
        "id": "tov9DxKFWxgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the U-Net denoiser\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv1d(4, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose1d(512, 256, kernel_size=2, stride=2)\n",
        "        self.conv5 = nn.Conv1d(512, 256, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "        self.up2 = nn.ConvTranspose1d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv6 = nn.Conv1d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm1d(128)\n",
        "        self.up3 = nn.ConvTranspose1d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv7 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm1d(64)\n",
        "        self.conv8 = nn.Conv1d(64, 4, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = nn.functional.relu(self.bn1(self.conv1(x)))\n",
        "        x2 = nn.functional.relu(self.bn2(self.conv2(x1)))\n",
        "        x3 = nn.functional.relu(self.bn3(self.conv3(x2)))\n",
        "        x4 = nn.functional.relu(self.bn4(self.conv4(x3)))\n",
        "\n",
        "        # Decoder\n",
        "        x = nn.functional.relu(self.bn5(self.conv5(torch.cat([self.up1(x4), x3], dim=1))))\n",
        "        x = nn.functional.relu(self.bn6(self.conv6(torch.cat([self.up2(x), x2], dim=1))))\n",
        "        x = nn.functional.relu(self.bn7(self.conv7(torch.cat([self.up3(x), x1], dim=1))))\n",
        "        x = torch.sigmoid(self.conv8(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "79_QIbxGFpFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to use for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the training parameters\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Create the DataLoader for training\n",
        "train_dataset = TensorDataset(torch.Tensor(noisy_seqs_preprocessed), torch.Tensor(clean_seqs_preprocessed))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize the U-Net denoiser and the optimizer\n",
        "unet = UNet().to(device)\n",
        "optimizer = optim.Adam(unet.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Train the U-Net denoiser\n",
        "train_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    # Train for one epoch\n",
        "    train_loss = 0\n",
        "    for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        # Move the batch to the device\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = unet(batch_x)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(y_pred, batch_y)\n",
        "\n",
        "        # Backpropagate and update the parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the training loss\n",
        "        train_loss += loss.item() * batch_x.shape[0]\n",
        "\n",
        "    # Compute the average training loss for the epoch\n",
        "    train_loss /= len(train_dataset)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Print the training loss for the epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "JHsq2qXMGhs8",
        "outputId": "39b59079-e3b7-4457-8eaa-233dac3696ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   0%|          | 0/32 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8fe15488d501>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-daf0ea5c849b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    307\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 309\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    310\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 1, 50, 4]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating "
      ],
      "metadata": {
        "id": "Xp0Ejk7PLcEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequences(num_seqs, seq_len):\n",
        "    \"\"\"Generate random DNA sequences with G-quadruplex regions.\"\"\"\n",
        "    seqs = []\n",
        "    for i in range(num_seqs):\n",
        "        seq = ''\n",
        "        while len(seq) < seq_len:\n",
        "            # Generate random nucleotide\n",
        "            nuc = np.random.choice(['A', 'C', 'G', 'T'])\n",
        "            seq += nuc\n",
        "            # Generate G-quadruplex region\n",
        "            if len(seq) % 4 == 0 and len(seq) < seq_len:\n",
        "                seq += 'GGGG'\n",
        "        seqs.append(seq)\n",
        "    return seqs\n",
        "\n",
        "def preprocess_sequences(seqs):\n",
        "    \"\"\"Preprocess DNA sequences by one-hot encoding.\"\"\"\n",
        "    # Define nucleotide to integer mapping\n",
        "    nuc_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "    # One-hot encode sequences\n",
        "    encoded_seqs = np.zeros((len(seqs), len(seqs[0]), 4))\n",
        "    for i, seq in enumerate(seqs):\n",
        "        for j, nuc in enumerate(seq):\n",
        "            encoded_seqs[i, j, nuc_map[nuc]] = 1\n",
        "    return encoded_seqs"
      ],
      "metadata": {
        "id": "2zxcF7mgMymV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_gaussian_noise(seq, noise_level):\n",
        "    \"\"\"\n",
        "    Add Gaussian noise to a DNA sequence.\n",
        "\n",
        "    Args:\n",
        "    - seq (str): DNA sequence.\n",
        "    - noise_level (float): Standard deviation of the Gaussian noise.\n",
        "\n",
        "    Returns:\n",
        "    - noisy_seq (str): Noisy DNA sequence.\n",
        "    \"\"\"\n",
        "    # Convert DNA sequence to array of integers\n",
        "    seq_int = np.zeros(len(seq), dtype=int)\n",
        "    for i, char in enumerate(seq):\n",
        "        if char == 'A':\n",
        "            seq_int[i] = 0\n",
        "        elif char == 'C':\n",
        "            seq_int[i] = 1\n",
        "        elif char == 'G':\n",
        "            seq_int[i] = 2\n",
        "        elif char == 'T':\n",
        "            seq_int[i] = 3\n",
        "\n",
        "    # Add Gaussian noise\n",
        "    noise = np.random.normal(scale=noise_level, size=len(seq_int))\n",
        "    seq_int_noisy = np.clip(seq_int + noise, 0, 3)\n",
        "\n",
        "    # Convert back to DNA sequence\n",
        "    noisy_seq = ''\n",
        "    for i in seq_int_noisy:\n",
        "        if i == 0:\n",
        "            noisy_seq += 'A'\n",
        "        elif i == 1:\n",
        "            noisy_seq += 'C'\n",
        "        elif i == 2:\n",
        "            noisy_seq += 'G'\n",
        "        elif i == 3:\n",
        "            noisy_seq += 'T'\n",
        "\n",
        "    return noisy_seq"
      ],
      "metadata": {
        "id": "2Xcf83aUNnel"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_seqs = 1000\n",
        "seq_len = 200\n",
        "noise_std = 0.1\n",
        "\n",
        "# Generate sequences with G-quadruplex regions\n",
        "clean_seqs = generate_sequences(num_seqs, seq_len)\n",
        "\n",
        "# Add Gaussian noise to sequences\n",
        "noisy_seqs = add_gaussian_noise(clean_seqs, noise_std)\n",
        "\n",
        "# Preprocess sequences by one-hot encoding\n",
        "clean_seqs_preprocessed = preprocess_sequences(clean_seqs)\n",
        "noisy_seqs_preprocessed = preprocess_sequences(noisy_seqs)"
      ],
      "metadata": {
        "id": "m_gGw1MWMzTA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_seqs_preprocessed"
      ],
      "metadata": {
        "id": "pcVCyHqVM-qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained U-Net denoiser\n",
        "denoiser = UNet()\n",
        "\n",
        "checkpoint = torch.load('unet_checkpoint.pth')\n",
        "denoiser.load_state_dict(checkpoint['model_state_dict'])\n",
        "denoiser.eval()\n",
        "\n",
        "# Generate noisy DNA sequences with G-quadruplex regions using diffusion neural network\n",
        "noisy_seqs = generate_sequences(diffusion_nn, num_seqs=10, seq_length=200)\n",
        "\n",
        "# Preprocess noisy DNA sequences for input to U-Net denoiser\n",
        "noisy_seqs_preprocessed = preprocess_sequences(noisy_seqs)\n",
        "\n",
        "# Denoise the noisy sequences using U-Net denoiser\n",
        "with torch.no_grad():\n",
        "    denoised_seqs_preprocessed = denoiser(torch.from_numpy(noisy_seqs_preprocessed).float())\n",
        "    \n",
        "# Postprocess denoised sequences to obtain final DNA sequences with G-quadruplex regions\n",
        "denoised_seqs = postprocess_sequences(denoised_seqs_preprocessed)\n",
        "\n",
        "# Print the final denoised DNA sequences\n",
        "print(denoised_seqs)"
      ],
      "metadata": {
        "id": "gXBEYRH-LbZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer as denoiser"
      ],
      "metadata": {
        "id": "WsXfC3c9b3io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDenoiser(nn.Module):\n",
        "    def __init__(self, input_dim=4, max_length=1000, d_model=256, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=512, dropout=0.1):\n",
        "        super(TransformerDenoiser, self).__init__()\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_length=max_length)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
        "                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout)\n",
        "        self.fc = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape (batch_size, seq_length)\n",
        "        x = self.embedding(x) * np.sqrt(self.max_length)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x, x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "        \n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_length=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        \n",
        "        # Compute the positional encodings in advance\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "ZwOGrBSfb7HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset class\n",
        "class DNASequenceDataset(Dataset):\n",
        "    def __init__(self, noisy_seqs_preprocessed, clean_seqs_preprocessed):\n",
        "        self.noisy_seqs_preprocessed = noisy_seqs_preprocessed\n",
        "        self.clean_seqs_preprocessed = clean_seqs_preprocessed\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.noisy_seqs_preprocessed)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        noisy_seq = torch.tensor(self.noisy_seqs_preprocessed[idx], dtype=torch.float32)\n",
        "        clean_seq = torch.tensor(self.clean_seqs_preprocessed[idx], dtype=torch.float32)\n",
        "        return noisy_seq, clean_seq\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 10\n",
        "\n",
        "# Generate preprocessed noisy and clean sequences\n",
        "noisy_seqs_preprocessed, clean_seqs_preprocessed = preprocess_sequences(generate_sequences(1000))\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = DNASequenceDataset(noisy_seqs_preprocessed, clean_seqs_preprocessed)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = TransformerDenoiser().cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the Transformer denoiser\n",
        "train_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for i, (noisy_seq, clean_seq) in enumerate(dataloader):\n",
        "        # Move tensors to GPU\n",
        "        noisy_seq = noisy_seq.cuda()\n",
        "        clean_seq = clean_seq.cuda()\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output_seq = model(noisy_seq)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = nn.MSELoss()(output_seq, clean_seq)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    train_losses.append(epoch_loss / len(dataloader))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_losses[-1]}\")"
      ],
      "metadata": {
        "id": "4fIDkIJadbVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerDenoiser()\n",
        "model.load_state_dict(torch.load('transformer_denoiser.pt'))\n",
        "\n",
        "# Generate some DNA sequences using the diffusion neural network\n",
        "noisy_seqs = generate_sequences(10)\n",
        "\n",
        "# Preprocess the DNA sequences by converting them to one-hot encoding\n",
        "noisy_seqs_preprocessed = preprocess_sequences(noisy_seqs)\n",
        "\n",
        "# Convert the preprocessed sequences to PyTorch tensors and pass them through the denoiser\n",
        "with torch.no_grad():\n",
        "    noisy_seqs_tensor = torch.tensor(noisy_seqs_preprocessed).long()\n",
        "    denoised_seqs_tensor = model(noisy_seqs_tensor)\n",
        "\n",
        "# Convert the denoised sequences back to numpy arrays\n",
        "denoised_seqs_preprocessed = denoised_seqs_tensor.cpu().numpy()\n",
        "denoised_seqs = postprocess_sequences(denoised_seqs_preprocessed)\n",
        "\n",
        "# Print the denoised DNA sequences\n",
        "for i in range(len(denoised_seqs)):\n",
        "    print(f\"Noisy sequence: {noisy_seqs[i]}\")\n",
        "    print(f\"Denoised sequence: {denoised_seqs[i]}\")"
      ],
      "metadata": {
        "id": "JLZiHKJlc0yw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}