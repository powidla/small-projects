{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Latest version of notebook for G4 dataset"
      ],
      "metadata": {
        "id": "KnRT3PDxW7l-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary libs"
      ],
      "metadata": {
        "id": "IY-7G9XtXC6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install torchmetrics\n",
        "!pip install livelossplot\n",
        "!pip install pybedtools\n",
        "!pip install Bio\n",
        "!pip install Sophia-Optimizer\n",
        "!pip install memory_efficient_attention_pytorch\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "O-XODUUqW-Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.getpid()\n",
        "from functools import partial\n",
        "from scipy.stats import zscore\n",
        "import torch\n",
        "import copy\n",
        "import itertools\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "from IPython.display import display\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.utils import save_image\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm_notebook\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib\n",
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "import scipy\n",
        "from scipy.special import rel_entr\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "from torch import nn, einsum\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.image as mpimg\n",
        "import glob\n",
        "from PIL import Image\n",
        "from typing import List, Union\n",
        "from torchmetrics.functional import kl_divergence\n",
        "import random\n",
        "import gc\n",
        "from livelossplot import PlotLosses\n",
        "from Bio import SeqIO\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from memory_efficient_attention_pytorch import Attention\n",
        "from accelerate import Accelerator\n",
        "from typing import List, Optional\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hTvAwugiXNoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TjmDaCXyXCcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sophia from https://github.com/kyegomez/Sophia"
      ],
      "metadata": {
        "id": "D_eEhnTDXReZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SophiaG(Optimizer):\n",
        "    \"\"\"\n",
        "    SophiaG optimizer class.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=5e-2, betas=(0.965, 0.99), rho = 0.04,\n",
        "         weight_decay=1e-1, *, maximize: bool = False,\n",
        "         capturable: bool = False, dynamic: bool = False):\n",
        "        \"\"\"\n",
        "        Initialize the optimizer.\n",
        "        \"\"\"\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
        "        if not 0.0 <= rho:\n",
        "            raise ValueError(f\"Invalid rho parameter at index 1: {rho}\")\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "        defaults = dict(lr=lr, betas=betas, rho=rho,\n",
        "                        weight_decay=weight_decay,\n",
        "                        maximize=maximize, capturable=capturable, dynamic=dynamic)\n",
        "        super(SophiaG, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        \"\"\"\n",
        "        Set the state of the optimizer.\n",
        "        \"\"\"\n",
        "        super().__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('maximize', False)\n",
        "            group.setdefault('capturable', False)\n",
        "            group.setdefault('dynamic', False)\n",
        "        state_values = list(self.state.values())\n",
        "        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(state_values[0]['step'])\n",
        "        if not step_is_tensor:\n",
        "            for s in state_values:\n",
        "                s['step'] = torch.tensor(float(s['step']))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_hessian(self):\n",
        "        \"\"\"\n",
        "        Update the hessian.\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group['betas']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = torch.zeros((1,), dtype=torch.float, device=p.device) \\\n",
        "                        if self.defaults['capturable'] else torch.tensor(0.)\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                if 'hessian' not in state.keys():\n",
        "                    state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                state['hessian'].mul_(beta2).addcmul_(p.grad, p.grad, value=1 - beta2)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_exp_avg(self):\n",
        "        \"\"\"\n",
        "        Update the exponential average.\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group['betas']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state = self.state[p]\n",
        "                state['exp_avg'].mul_(beta1).add_(p.grad, alpha=1 - beta1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None, bs=5120):\n",
        "        \"\"\"\n",
        "        Perform a step of the optimizer.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        self.update_hessian()\n",
        "        self.update_exp_avg()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            grads = []\n",
        "            exp_avgs = []\n",
        "            state_steps = []\n",
        "            hessian = []\n",
        "            beta1, beta2 = group['betas']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                params_with_grad.append(p)\n",
        "\n",
        "                if p.grad.is_sparse:\n",
        "                    raise RuntimeError('Hero does not support sparse gradients')\n",
        "                grads.append(p.grad)\n",
        "                state = self.state[p]\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = torch.zeros((1,), dtype=torch.float, device=p.device) \\\n",
        "                        if self.defaults['capturable'] else torch.tensor(0.)\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                if 'hessian' not in state.keys():\n",
        "                    state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avgs.append(state['exp_avg'])\n",
        "                state_steps.append(state['step'])\n",
        "                hessian.append(state['hessian'])\n",
        "\n",
        "                if self.defaults['capturable']:\n",
        "                    bs = torch.ones((1,), dtype=torch.float, device=p.device) * bs\n",
        "\n",
        "            self._sophiag(params_with_grad,\n",
        "                  grads,\n",
        "                  exp_avgs,\n",
        "                  hessian,\n",
        "                  state_steps,\n",
        "                  bs=bs,\n",
        "                  beta1=beta1,\n",
        "                  beta2=beta2,\n",
        "                  rho=group['rho'],\n",
        "                  lr=group['lr'],\n",
        "                  weight_decay=group['weight_decay'],\n",
        "                  maximize=group['maximize'],\n",
        "                  capturable=group['capturable'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _sophiag(self, params: List[Tensor],\n",
        "          grads: List[Tensor],\n",
        "          exp_avgs: List[Tensor],\n",
        "          hessian: List[Tensor],\n",
        "          state_steps: List[Tensor],\n",
        "          capturable: bool = False,\n",
        "          *,\n",
        "          bs: int,\n",
        "          beta1: float,\n",
        "          beta2: float,\n",
        "          rho: float,\n",
        "          lr: float,\n",
        "          weight_decay: float,\n",
        "          maximize: bool):\n",
        "        \"\"\"\n",
        "        SophiaG function.\n",
        "        \"\"\"\n",
        "        if not all(isinstance(t, torch.Tensor) for t in state_steps):\n",
        "            raise RuntimeError(\"API has changed, `state_steps` argument must contain a list of singleton tensors\")\n",
        "\n",
        "        self._single_tensor_sophiag(params,\n",
        "             grads,\n",
        "             exp_avgs,\n",
        "             hessian,\n",
        "             state_steps,\n",
        "             bs=bs,\n",
        "             beta1=beta1,\n",
        "             beta2=beta2,\n",
        "             rho=rho,\n",
        "             lr=lr,\n",
        "             weight_decay=weight_decay,\n",
        "             maximize=maximize,\n",
        "             capturable=capturable)\n",
        "\n",
        "    def _single_tensor_sophiag(self, params: List[Tensor],\n",
        "                         grads: List[Tensor],\n",
        "                         exp_avgs: List[Tensor],\n",
        "                         hessian: List[Tensor],\n",
        "                         state_steps: List[Tensor],\n",
        "                         *,\n",
        "                         bs: int,\n",
        "                         beta1: float,\n",
        "                         beta2: float,\n",
        "                         rho: float,\n",
        "                         lr: float,\n",
        "                         weight_decay: float,\n",
        "                         maximize: bool,\n",
        "                         capturable: bool):\n",
        "        \"\"\"\n",
        "        SophiaG function for single tensor.\n",
        "        \"\"\"\n",
        "        for i, param in enumerate(params):\n",
        "            grad = grads[i] if not maximize else -grads[i]\n",
        "            exp_avg = exp_avgs[i]\n",
        "            hess = hessian[i]\n",
        "            step_t = state_steps[i]\n",
        "\n",
        "            if capturable:\n",
        "                assert param.is_cuda and step_t.is_cuda and bs.is_cuda\n",
        "\n",
        "            if torch.is_complex(param):\n",
        "                grad = torch.view_as_real(grad)\n",
        "                exp_avg = torch.view_as_real(exp_avg)\n",
        "                hess = torch.view_as_real(hess)\n",
        "                param = torch.view_as_real(param)\n",
        "\n",
        "            # update step\n",
        "            step_t += 1\n",
        "\n",
        "            # Perform stepweight decay\n",
        "            param.mul_(1 - lr * weight_decay)\n",
        "\n",
        "            # Decay the first and second moment running average coefficient\n",
        "            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "            if capturable:\n",
        "                step = step_t\n",
        "                step_size = lr\n",
        "                step_size_neg = step_size.neg()\n",
        "\n",
        "                ratio = (exp_avg.abs() / (rho * bs * hess + 1e-15)).clamp(None,1)\n",
        "                param.addcmul_(exp_avg.sign(), ratio, value=step_size_neg)\n",
        "            else:\n",
        "                step = step_t.item()\n",
        "                step_size_neg = - lr\n",
        "\n",
        "                ratio = (exp_avg.abs() / (rho * bs * hess + 1e-15)).clamp(None,1)\n",
        "                param.addcmul_(exp_avg.sign(), ratio, value=step_size_neg)"
      ],
      "metadata": {
        "id": "TyGY_uGWXXfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters"
      ],
      "metadata": {
        "id": "WU1ywMvwXcjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GLOBAL_SEED = 42\n",
        "NUCLEOTIDES = ['A', 'C', 'T', 'G']\n",
        "N_SAMPLES = 1000\n",
        "EPOCHS = 10000\n",
        "SAVE_AND_SAMPLE_EVERY = 5\n",
        "SEQ_SIZE = 300\n",
        "CHANNELS = 1\n",
        "LEARNING_RATE =1e-5\n",
        "TIMESTEPS = 100\n",
        "RESNET_BLOCK_GROUPS = 4\n",
        "# BATCH_SIZE = 16\n",
        "BATCH_SIZE = 16\n",
        "TOTAL_CLASS_NUMBER = 2\n",
        "# gradient_accumulation_steps\n",
        "gradient_accumulation_steps = 8\n",
        "# one hot codes\n",
        "codes = {\n",
        " 'A': [1., 0., 0., 0., 0.],\n",
        " 'T': [0., 1., 0., 0., 0.],\n",
        " 'G': [0., 0., 1., 0., 0.],\n",
        " 'C': [0., 0., 0., 1., 0.],\n",
        " 'N': [0., 0., 0., 0., 1.],\n",
        " }"
      ],
      "metadata": {
        "id": "i2yALA6eXfJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Core code credit: https://github.com/pinellolab/DNA-Diffusion/tree/main"
      ],
      "metadata": {
        "id": "HxqtG4IKXhRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=GLOBAL_SEED):\n",
        "    \"\"\" \"\n",
        "    Seed everything.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def one_hot_encode(seq, nucleotides, max_seq_len):\n",
        "    \"\"\"\n",
        "    One-hot encode a sequence of nucleotides.\n",
        "    \"\"\"\n",
        "    seq_len = len(seq)\n",
        "    seq_array = np.zeros((max_seq_len, len(nucleotides)))\n",
        "    for i in range(seq_len):\n",
        "        seq_array[i, nucleotides.index(seq[i])] = 1\n",
        "    return seq_array\n",
        "\n",
        "\n",
        "def log(t, eps=1e-20):\n",
        "    \"\"\"\n",
        "    Toch log for the purporses of diffusion time steps t.\n",
        "    \"\"\"\n",
        "    return torch.log(t.clamp(min=eps))\n",
        "\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "\n",
        "def cycle(dl):\n",
        "    while True:\n",
        "        for data in dl:\n",
        "            yield data\n",
        "\n",
        "\n",
        "def has_int_squareroot(num):\n",
        "    return (math.sqrt(num) ** 2) == num\n",
        "\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def convert_image_to(img_type, image):\n",
        "    if image.mode != img_type:\n",
        "        return image.convert(img_type)\n",
        "    return image\n",
        "\n",
        "\n",
        "def l2norm(t):\n",
        "    return F.normalize(t, dim=-1)"
      ],
      "metadata": {
        "id": "6K06mLORXvxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EMA:\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.step = 0\n",
        "\n",
        "    def update_model_average(self, ma_model, current_model):\n",
        "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "            old_weight, up_weight = ma_params.data, current_params.data\n",
        "            ma_params.data = self.update_average(old_weight, up_weight)\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
        "        if self.step < step_start_ema:\n",
        "            self.reset_parameters(ema_model, model)\n",
        "            self.step += 1\n",
        "            return\n",
        "        self.update_model_average(ema_model, model)\n",
        "        self.step += 1\n",
        "\n",
        "    def reset_parameters(self, ema_model, model):\n",
        "        ema_model.load_state_dict(model.state_dict())"
      ],
      "metadata": {
        "id": "dOK6fZFVX2tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling_reverse_encoding(number_of_samples, specific_group=False, group_number=None, cond_weight_to_metric=0):\n",
        "    \"\"\"\n",
        "    Stands for reverse encoding\n",
        "\n",
        "    \"\"\"\n",
        "    final_sequences = []\n",
        "    for n_a in tqdm_notebook(range(number_of_samples)):\n",
        "        sample_bs = 16\n",
        "        if specific_group:\n",
        "            sampled = torch.from_numpy(np.array([group_number] * sample_bs))\n",
        "            print('specific')\n",
        "        else:\n",
        "            sampled = torch.from_numpy(np.random.choice([0, 1], sample_bs))\n",
        "\n",
        "        random_classes = sampled.float().cuda()\n",
        "        sampled_images = sample(\n",
        "            model,\n",
        "            classes=random_classes,\n",
        "            image_size=SEQ_SIZE,\n",
        "            batch_size=sample_bs,\n",
        "            channels=1,\n",
        "            cond_weight=cond_weight_to_metric,\n",
        "        )\n",
        "        for n_b, x in enumerate(sampled_images[-1]):\n",
        "            seq_final = f'>seq_test_{n_a}_{n_b}\\n' + ''.join(\n",
        "                [NUCLEOTIDES[s] for s in np.argmax(x.reshape(4, SEQ_SIZE), axis=0)]\n",
        "            )\n",
        "            final_sequences.append(seq_final)\n",
        "\n",
        "    save_motifs_syn = open('synthetic_seqs.fasta', 'w')\n",
        "\n",
        "    save_motifs_syn.write('\\n'.join(final_sequences))\n",
        "    save_motifs_syn.close()"
      ],
      "metadata": {
        "id": "rC5ufjRmYXaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data prep"
      ],
      "metadata": {
        "id": "oVFWFM6mYdPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "\n",
        "    def __init__(self, seqs, c, transform=None):\n",
        "        'Initialization'\n",
        "        self.seqs = seqs\n",
        "        self.c = c\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        image = self.seqs[index]\n",
        "\n",
        "        x = self.transform(image)\n",
        "\n",
        "        y = self.c[index]\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "tCU702WxYeYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and visualize .bed file\n",
        "pos_df = pd.read_csv('GSE107690_K562_High_confidence_peaks.bed',\n",
        " sep='\\t', comment='t', header=None)\n",
        "header = ['chrom', 'chromStart', 'chromEnd']\n",
        "pos_df.columns = header[:len(pos_df.columns)]\n",
        "pos_df['len'] = pos_df.chromEnd - pos_df.chromStart\n",
        "pos_df.head()\n",
        "plt.hist(pos_df.chrom, bins=len(pos_df.chrom.unique()))\n",
        "pos_df.len.quantile(0.5)\n",
        "pos_df.len[pos_df.len > 512].count()\n",
        "pos_df.len[pos_df.len <= 512].count() / pos_df.len.count()\n",
        "pos_df.len.hist(bins=100)"
      ],
      "metadata": {
        "id": "CatEM3l6YfnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quad_len = 512\n",
        "pos_df_filter = pos_df[pos_df.len\n",
        " <= quad_len].copy().reset_index(drop=True)\n",
        "pos_df_filter.len.max()\n",
        "pos_df_filter['add_len'] = quad_len - pos_df_filter.len\n",
        "pos_df_filter['left_add_len'] = (pos_df_filter.add_len\n",
        " / 2).astype('int64')\n",
        "pos_df_filter['right_add_len'] = pos_df_filter.add_len \\\n",
        " - pos_df_filter.left_add_len\n",
        "pos_df_filter['new_chromStart'] = pos_df_filter.chromStart \\\n",
        " - pos_df_filter.left_add_len\n",
        "pos_df_filter['new_chromEnd'] = pos_df_filter.chromEnd \\\n",
        " + pos_df_filter.right_add_len\n",
        "pos_df_filter['new_len'] = pos_df_filter.new_chromEnd \\\n",
        " - pos_df_filter.new_chromStart\n",
        "pos_df_filter.head()"
      ],
      "metadata": {
        "id": "sNCWm1jkYiQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_df_filter[['chrom', 'new_chromStart', 'new_chromEnd'\n",
        " ]].to_csv('G4_Chip_seq_filter_norm_to_500.bed', sep='\\t',\n",
        " header=None, index=None)\n",
        "pos_df_filter[['chrom', 'chromStart', 'chromEnd'\n",
        " ]].to_csv('G4_Chip_seq_filter_500.bed', sep='\\t',\n",
        " header=None, index=None)\n",
        "pos_df_filter.to_csv('G4_Chip_seq_positions.csv')"
      ],
      "metadata": {
        "id": "YAQZjJBOYlXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prep data for unconditioned generation"
      ],
      "metadata": {
        "id": "ki7sg3YLYohg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_quads = []\n",
        "line_num = 0\n",
        "with open('G4_Chip_seq_quadruplex_norm_to_500.fa', 'r') as f:\n",
        "  for line in f:\n",
        "    if line[0] != '>' and len(line) == quad_len + 1:\n",
        "      one_hot = []\n",
        "      for s in line.upper():\n",
        "        if s != '\\n':\n",
        "          one_hot.append(codes[s])\n",
        "      one_hot_quads.append(one_hot)\n",
        "      line_num += 1\n",
        "one_hot_quads_np = np.array(one_hot_quads)\n",
        "one_hot_quads_np.shape\n",
        "np.save('G4_Chip_seq_quadruplex_norm.npy', one_hot_quads_np)"
      ],
      "metadata": {
        "id": "K63ASqHGYncQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For labeled data"
      ],
      "metadata": {
        "id": "zC5Ggv3JYvIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_quads = []\n",
        "line_num = 0\n",
        "with open('G4_Chip_seq_quadruplex_norm_to_500.fa', 'r') as f:\n",
        "  for line in f:\n",
        "    if line[0] != '>' and len(line) == quad_len + 1:\n",
        "      left = pos_df_filter.iloc[line_num].left_add_len\n",
        "      right = left + pos_df_filter.iloc[line_num].len\n",
        "      one_hot = []\n",
        "      for (i, s) in enumerate(line.upper()):\n",
        "        pos = 0.\n",
        "        if left <= i and i < right:\n",
        "          pos = 1.\n",
        "        if s != '\\n':\n",
        "          one_hot.append(codes[s] + [pos])\n",
        "      one_hot_quads.append(np.array(one_hot))\n",
        "      line_num += 1\n",
        "one_hot_quads_np = np.array(one_hot_quads)\n",
        "one_hot_quads_np.shape\n",
        "np.save('G4_Chip_seq_quadruplex_norm_quad_labeled.npy',\n",
        " one_hot_quads_np)"
      ],
      "metadata": {
        "id": "iWrmKw3LYtZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasta_file = \"G4_Chip_seq_quadruplex_norm_to_500.fa\"\n",
        "label_data = np.load(\"G4_Chip_seq_quadruplex_norm_quad_labeled.npy\")"
      ],
      "metadata": {
        "id": "e9uzbzR5Ytbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get seqs from G4 seq dataset\n",
        "sequences = []\n",
        "\n",
        "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "   sequences.append(str(record.seq).upper())\n",
        "\n",
        "raw_dataset = pd.DataFrame({\"raw_sequence\": sequences})\n",
        "\n",
        "raw_dataset.head()"
      ],
      "metadata": {
        "id": "i1kODU7tY2a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get labels\n",
        "last_numbers = [arr[0][0] for arr in label_data]\n",
        "len(last_numbers)"
      ],
      "metadata": {
        "id": "ekJ4_j-TY5xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_numbers = torch.tensor(last_numbers)"
      ],
      "metadata": {
        "id": "3zANcgQqZCQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data transformation to continuous data from $[0;1]$ to $[-1, 1]$ rescaling"
      ],
      "metadata": {
        "id": "yBVPu21wZFQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(\n",
        "    [one_hot_encode(x, NUCLEOTIDES, 512) for x in tqdm_notebook(raw_dataset.raw_sequence[:6543]) if 'N' not in x]\n",
        ")\n",
        "X_train = np.array([x.T.tolist() for x in X_train])\n",
        "X_train[X_train == 0] = -1\n",
        "X_val = np.array(\n",
        "    [one_hot_encode(x, NUCLEOTIDES, 512) for x in tqdm_notebook(raw_dataset.raw_sequence[6543:]) if 'N' not in x]\n",
        ")\n",
        "X_val = np.array([x.T.tolist() for x in X_val])\n",
        "X_val[X_val == 0] = -1"
      ],
      "metadata": {
        "id": "MW5Dbhj3ZD9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = T.Compose([T.ToTensor()])\n",
        "seq_dataset_train = SequenceDataset(seqs=X_train, c=last_numbers[:6540], transform=tf)\n",
        "train_dl = DataLoader(seq_dataset_train, BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "seq_dataset_val = SequenceDataset(seqs=X_val, c=last_numbers[6540:8720], transform=tf)\n",
        "val_dl = DataLoader(seq_dataset_val, BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)"
      ],
      "metadata": {
        "id": "qoSjfMFCZgti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diffusion"
      ],
      "metadata": {
        "id": "wBe-JKnnZkHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scheduler\n",
        "\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "\n",
        "def linear_beta_schedule(timesteps, beta_end=0.005):\n",
        "    beta_start = 0.0001\n",
        "\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
        "\n",
        "\n",
        "# define beta schedule\n",
        "betas = linear_beta_schedule(timesteps=TIMESTEPS, beta_end=0.2)\n",
        "# define alphas\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "# sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "\n",
        "# Conditioned (improved) sampling\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
        "    # print (x.shape, 'x_shape')\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "    # Equation 11 in the paper\n",
        "    # Use our model (noise predictor) to predict the mean\n",
        "    model_mean = sqrt_recip_alphas_t * (x - betas_t * model(x, time=t) / sqrt_one_minus_alphas_cumprod_t)\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # Algorithm 2 line 4:\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "\n",
        "# Sampling based on Denoising Diffusion Implicit Models (https://arxiv.org/abs/2010.02502)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_ddim_sample(model, x, t, t_index, eta=0, temp=1.0):\n",
        "    alpha_t = extract(alphas_cumprod, t, x.shape)\n",
        "    alpha_prev_t = extract(alphas_cumprod_prev, t, x.shape)\n",
        "    sigma = eta * ((1 - alpha_prev_t) / (1 - alpha_t) * (1 - alpha_t / alpha_prev_t)) ** 0.5\n",
        "    sqrt_one_minus_alphas_cumprod = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
        "    pred_x0 = (x - sqrt_one_minus_alphas_cumprod * model(x, time=t)) / (alpha_t**0.5)\n",
        "    dir_xt = (1.0 - alpha_prev_t - sigma**2).sqrt() * model(x, time=t)\n",
        "    if sigma == 0.0:\n",
        "        noise = 0.0\n",
        "    else:\n",
        "        noise = torch.randn((1, x.shape[1:]))\n",
        "    noise *= temp\n",
        "\n",
        "    x_prev = (alpha_prev_t**0.5) * pred_x0 + dir_xt + sigma * noise\n",
        "\n",
        "    return x_prev\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample_guided(model, x, classes, t, t_index, context_mask, cond_weight=0.0):\n",
        "    # adapted from: https://openreview.net/pdf?id=qw8AKxfYbI\n",
        "    # print (classes[0])\n",
        "    batch_size = x.shape[0]\n",
        "    # double to do guidance with\n",
        "    t_double = t.repeat(2)\n",
        "    x_double = x.repeat(2, 1, 1, 1)\n",
        "    betas_t = extract(betas, t_double, x_double.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t_double, x_double.shape)\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t_double, x_double.shape)\n",
        "\n",
        "    # classifier free sampling interpolates between guided and non guided using `cond_weight`\n",
        "    classes_masked = classes * context_mask\n",
        "    classes_masked = classes_masked.type(torch.long)\n",
        "    # print ('class masked', classes_masked)\n",
        "    preds = model(x_double, time=t_double, classes=classes_masked)\n",
        "    eps1 = (1 + cond_weight) * preds[:batch_size]\n",
        "    eps2 = cond_weight * preds[batch_size:]\n",
        "    x_t = eps1 - eps2\n",
        "\n",
        "    # Equation 11 in the paper\n",
        "    # Use our model (noise predictor) to predict the mean\n",
        "    model_mean = sqrt_recip_alphas_t[:batch_size] * (\n",
        "        x - betas_t[:batch_size] * x_t / sqrt_one_minus_alphas_cumprod_t[:batch_size]\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # Algorithm 2 line 4:\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "\n",
        "# Algorithm 2 but save all images:\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, classes, shape, cond_weight):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "    # start from pure noise (for each example in the batch)\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "\n",
        "    if classes is not None:\n",
        "        n_sample = classes.shape[0]\n",
        "        context_mask = torch.ones_like(classes).to(device)\n",
        "        # make 0 index unconditional\n",
        "        # double the batch\n",
        "        classes = classes.repeat(2)\n",
        "        context_mask = context_mask.repeat(2)\n",
        "        context_mask[n_sample:] = 0.0  # makes second half of batch context free\n",
        "        sampling_fn = partial(p_sample_guided, classes=classes, cond_weight=cond_weight, context_mask=context_mask)\n",
        "    else:\n",
        "        sampling_fn = partial(p_sample)\n",
        "\n",
        "    for i in tqdm(reversed(range(0, TIMESTEPS)), desc='sampling loop time step', total=TIMESTEPS):\n",
        "        img = sampling_fn(model, x=img, t=torch.full((b,), i, device=device, dtype=torch.long), t_index=i)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "    return imgs\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, classes=None, batch_size=16, channels=3, cond_weight=0):\n",
        "    return p_sample_loop(model, classes=classes, shape=(batch_size, channels, 4, image_size), cond_weight=cond_weight)"
      ],
      "metadata": {
        "id": "9ZtEmrWnZk6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function"
      ],
      "metadata": {
        "id": "t0GmZsX-Zwl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_sample(x_start, t, noise=None):\n",
        "    \"\"\"\n",
        "    Forward pass with noise.\n",
        "    \"\"\"\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "\n",
        "def p_losses(denoise_model, x_start, t, classes, noise=None, loss_type=\"l1\", p_uncond=0.1):\n",
        "    \"\"\"\n",
        "    Calculate the loss conditioned and noise injected.\n",
        "    \"\"\"\n",
        "    device = x_start.device\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)  #  gauss noise\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)  # this is the auto generated noise given t and Noise\n",
        "\n",
        "    context_mask = torch.bernoulli(torch.zeros(classes.shape[0]) + (1 - p_uncond)).to(device)\n",
        "\n",
        "    # mask for unconditinal guidance\n",
        "    classes = classes * context_mask\n",
        "    classes = classes.type(torch.long)\n",
        "    predicted_noise = denoise_model(x_noisy, t, classes)\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "WCYqsr2EZxfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "U-net denoiser"
      ],
      "metadata": {
        "id": "nlBx5SDAZ4dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building blocks of UNET\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "\n",
        "def Upsample(dim, dim_out=None):\n",
        "    return nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(dim, default(dim_out, dim), 3, padding=1)\n",
        "    )\n",
        "\n",
        "\n",
        "def Downsample(dim, dim_out=None):\n",
        "    return nn.Conv2d(dim, default(dim_out, dim), 4, 2, 1)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
        "        var = torch.var(x, dim=1, unbiased=False, keepdim=True)\n",
        "        mean = torch.mean(x, dim=1, keepdim=True)\n",
        "        return (x - mean) * (var + eps).rsqrt() * self.g\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n",
        "\n",
        "\n",
        "# Building blocks of UNET, positional embeds\n",
        "\n",
        "\n",
        "class LearnedSinusoidalPosEmb(nn.Module):\n",
        "\n",
        "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        assert (dim % 2) == 0\n",
        "        half_dim = dim // 2\n",
        "        self.weights = nn.Parameter(torch.randn(half_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, 'b -> b 1')\n",
        "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
        "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n",
        "        fouriered = torch.cat((x, fouriered), dim=-1)\n",
        "        return fouriered\n",
        "\n",
        "\n",
        "class EmbedFC(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedFC, self).__init__()\n",
        "        '''\n",
        "        generic one layer FC NN for embedding things\n",
        "        '''\n",
        "        self.input_dim = input_dim\n",
        "        layers = [nn.Linear(input_dim, emb_dim), nn.GELU(), nn.Linear(emb_dim, emb_dim)]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Building blocks of UNET, building block modules\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups=8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift=None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Building blocks of UNET, residual part\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2)) if exists(time_emb_dim) else None\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        scale_shift = None\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
        "            scale_shift = time_emb.chunk(2, dim=1)\n",
        "\n",
        "        h = self.block1(x, scale_shift=scale_shift)\n",
        "\n",
        "        h = self.block2(h)\n",
        "\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "\n",
        "# Additional code to the https://github.com/lucidrains/bit-diffusion/blob/main/bit_diffusion/bit_diffusion.py\n",
        "\n",
        "\n",
        "class ResnetBlockClassConditioned(ResnetBlock):\n",
        "    def __init__(self, dim, dim_out, *, num_classes, class_embed_dim, time_emb_dim=None, groups=8):\n",
        "        super().__init__(dim=dim + class_embed_dim, dim_out=dim_out, time_emb_dim=time_emb_dim, groups=groups)\n",
        "        self.class_mlp = EmbedFC(num_classes, class_embed_dim)\n",
        "\n",
        "    def forward(self, x, time_emb=None, c=None):\n",
        "        emb_c = self.class_mlp(c)\n",
        "        emb_c = emb_c.view(*emb_c.shape, 1, 1)\n",
        "        emb_c = emb_c.expand(-1, -1, x.shape[-2], x.shape[-1])\n",
        "        x = torch.cat([x, emb_c], axis=1)\n",
        "\n",
        "        return super().forward(x, time_emb)\n",
        "\n",
        "\n",
        "# Building blocks of UNET, attention part\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), LayerNorm(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads), qkv)\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        v = v / (h * w)\n",
        "\n",
        "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
        "\n",
        "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
        "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32, scale=10):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads), qkv)\n",
        "\n",
        "        q, k = map(l2norm, (q, k))\n",
        "\n",
        "        sim = einsum('b h d i, b h d j -> b h i j', q, k) * self.scale\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "# Core part of UNET\n",
        "\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    \"\"\"\n",
        "    Refer to the main paper for the architecture details https://arxiv.org/pdf/2208.04202.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=IMAGE_SIZE,\n",
        "        dim_mults=(1, 2, 4),\n",
        "        channels=CHANNELS,\n",
        "        resnet_block_groups=8,\n",
        "        learned_sinusoidal_dim=18,\n",
        "        num_classes=10,\n",
        "        class_embed_dim=3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        # if you want to do self conditioning uncomment this\n",
        "        # input_channels = channels * 2\n",
        "        input_channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim)\n",
        "        self.init_conv = nn.Conv2d(input_channels, init_dim, (7, 7), padding=3)\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        time_dim = dim * 4\n",
        "\n",
        "        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinusoidal_dim)\n",
        "        fourier_dim = learned_sinusoidal_dim + 1\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            sinu_pos_emb, nn.Linear(fourier_dim, time_dim), nn.GELU(), nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "\n",
        "        if num_classes is not None:\n",
        "            self.label_emb = nn.Embedding(num_classes, time_dim)\n",
        "\n",
        "        # layers\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "            is_last = ind == (len(in_out) - 1)\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Upsample(dim_out, dim_in) if not is_last else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
        "        self.final_conv = nn.Conv2d(dim, 1, 1)\n",
        "        print('final', dim, channels, self.final_conv)\n",
        "\n",
        "    # Additional code to the https://github.com/lucidrains/bit-diffusion/blob/main/bit_diffusion/bit_diffusion.py mostly in forward method.\n",
        "\n",
        "    def forward(self, x, time, classes, x_self_cond=None):\n",
        "        x = self.init_conv(x)\n",
        "        r = x.clone()\n",
        "\n",
        "        t_start = self.time_mlp(time)\n",
        "        t_mid = t_start.clone()\n",
        "        t_end = t_start.clone()\n",
        "\n",
        "        if classes is not None:\n",
        "            t_start += self.label_emb(classes)\n",
        "            t_mid += self.label_emb(classes)\n",
        "            t_end += self.label_emb(classes)\n",
        "\n",
        "        h = []\n",
        "\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t_start)\n",
        "            h.append(x)\n",
        "\n",
        "            x = block2(x, t_start)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t_mid)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t_mid)\n",
        "\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t_mid)\n",
        "\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block2(x, t_mid)\n",
        "            x = attn(x)\n",
        "\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = torch.cat((x, r), dim=1)\n",
        "        x = self.final_res_block(x, t_end)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DUT7yk9tZ58p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "cSaDq7PIZ60w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "_iasX165Z9K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(\n",
        "    dim=SEQ_SIZE,\n",
        "    channels=CHANNELS,\n",
        "    dim_mults=(1, 2, 4),\n",
        "    resnet_block_groups=RESNET_BLOCK_GROUPS,\n",
        "    num_classes=TOTAL_CLASS_NUMBER,\n",
        ")\n",
        "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "model.to(device)\n",
        "# optimizer = SophiaG(model.parameters(), lr=LEARNING_RATE, betas=(0.965, 0.99), rho = 0.01, weight_decay=1e-1)\n",
        "# optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.99)"
      ],
      "metadata": {
        "id": "lRNA4IjlZ_fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SEQ_SIZE is fixed for number of parameters, optimal value is 200M\n",
        "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "8QXntyaFaB2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, dataloader = accelerator.prepare(model, optimizer, train_dl)\n",
        "ema = EMA(0.999)\n",
        "ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
        "train_loss_values = []\n",
        "val_loss_values = []\n",
        "num_train_batches = 0\n",
        "for epoch in tqdm(range(150)):\n",
        "    model.train()\n",
        "    for step, train_batch in enumerate(dataloader):\n",
        "        with accelerator.accumulate(model):\n",
        "          x, y = train_batch\n",
        "          x = x.type(torch.float32).to(device)\n",
        "          y = y.type(torch.long).to(device)\n",
        "          batch_size = x.shape[0]\n",
        "\n",
        "          t = torch.randint(0, TIMESTEPS, (batch_size,), device=device).long()\n",
        "          train_loss = p_losses(model, x, t, y, loss_type=\"huber\")\n",
        "          optimizer.zero_grad()\n",
        "          train_loss.backward()\n",
        "          optimizer.step()\n",
        "          ema.step_ema(ema_model, model)\n",
        "          num_train_batches += 1\n",
        "          torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=0.2\n",
        "        )\n",
        "\n",
        "    # Val loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    num_val_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_dl:\n",
        "            val_x, val_y = val_batch\n",
        "            val_x = val_x.type(torch.float32).to(device)\n",
        "            val_y = val_y.type(torch.long).to(device)\n",
        "            val_batch_size = val_x.shape[0]\n",
        "\n",
        "            val_t = torch.randint(0, TIMESTEPS, (val_batch_size,), device=device).long()\n",
        "            val_loss += p_losses(model, val_x, val_t, val_y, loss_type=\"huber\").item()\n",
        "            num_val_batches += 1\n",
        "    val_loss /= num_val_batches\n",
        "\n",
        "    # if epoch:\n",
        "    train_loss_values.append(train_loss.item())\n",
        "    val_loss_values.append(val_loss)\n",
        "    print(f\" Epoch {epoch} Train Loss: {train_loss.item()} | Val Loss: {val_loss}\")\n",
        "\n",
        "# Plotting\n",
        "plt.plot(range(len(val_loss_values)), val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation Loss SP-MSE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_vfniOEeaP6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval model trained 150 epoch, takes 8 hours on fast GPU"
      ],
      "metadata": {
        "id": "EM7TgYexaYML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/My Drive/data_for_gans/model_grok.pt\""
      ],
      "metadata": {
        "id": "DLI2J_C9ad32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "SRKWgnqZaeui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_seq ="
      ],
      "metadata": {
        "id": "1VDJDzCqa-dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled = torch.from_numpy(np.random.choice([0, 1], num_seq))\n",
        "random_classes = sampled.to(device)\n",
        "samples = sample(model, classes=random_classes, image_size=512, batch_size=num_seq, channels=1, cond_weight=1)"
      ],
      "metadata": {
        "id": "zj87gQ9Najpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synt_df = sampling_reverse_encoding(num_seq)"
      ],
      "metadata": {
        "id": "1LM5x6kTaknG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last code returns 16 sequences of length of 512 bp"
      ],
      "metadata": {
        "id": "zO_-R_SLalUm"
      }
    }
  ]
}