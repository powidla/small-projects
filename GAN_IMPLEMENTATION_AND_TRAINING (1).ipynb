{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubTXpK9P_yoS"
      },
      "outputs": [],
      "source": [
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybedtools\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "import torch.utils.data\n",
        "from torchsummary import summary\n",
        "import logging\n",
        "from torch import autograd\n",
        "from torch import optim\n",
        "import pprint\n",
        "from sklearn.neighbors import KDTree\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import editdistance\n",
        "import argparse\n",
        "import json\n",
        "import datetime\n",
        "import pickle as pk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybedtools\n",
        "!pip install Bio"
      ],
      "metadata": {
        "id": "xLMUy-VNTdeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d30dc9c-36c0-46c8-e760-24cac39a6316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pybedtools in /usr/local/lib/python3.9/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pysam in /usr/local/lib/python3.9/dist-packages (from pybedtools) (0.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from pybedtools) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from pysam->pybedtools) (0.29.34)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Bio in /usr/local/lib/python3.9/dist-packages (1.5.9)\n",
            "Requirement already satisfied: biopython>=1.80 in /usr/local/lib/python3.9/dist-packages (from Bio) (1.81)\n",
            "Requirement already satisfied: mygene in /usr/local/lib/python3.9/dist-packages (from Bio) (3.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.9/dist-packages (from Bio) (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from Bio) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from Bio) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from Bio) (4.65.0)\n",
            "Requirement already satisfied: gprofiler-official in /usr/local/lib/python3.9/dist-packages (from Bio) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biopython>=1.80->Bio) (1.22.4)\n",
            "Requirement already satisfied: biothings-client>=0.2.6 in /usr/local/lib/python3.9/dist-packages (from mygene->Bio) (0.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->Bio) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch->Bio) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from pooch->Bio) (23.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->Bio) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->Bio) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->Bio) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->Bio) (2.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "codes = {\n",
        " 'A': [1., 0., 0., 0., 0.],\n",
        " 'T': [0., 1., 0., 0., 0.],\n",
        " 'G': [0., 0., 1., 0., 0.],\n",
        " 'C': [0., 0., 0., 1., 0.],\n",
        " 'N': [0., 0., 0., 0., 1.],\n",
        " }"
      ],
      "metadata": {
        "id": "9H7r17-gnVJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, model_size):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.res_block = nn.Sequential(nn.ReLU(True),\n",
        "                                       nn.Conv1d(model_size, model_size, 5, padding=2),\n",
        "                                       nn.ReLU(True), nn.Conv1d(model_size, model_size, 5, padding=2))\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.res_block(input)\n",
        "        return input + 0.3 * output\n",
        "\n",
        "\n",
        "class G4GANGenerator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_size,\n",
        "            seq_len,\n",
        "            onehot_len,\n",
        "            last_channel_is_prob=False,\n",
        "    ):\n",
        "        super(G4GANGenerator, self).__init__()\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.seq_len = seq_len\n",
        "        self.onehot_len = onehot_len\n",
        "        self.last_channel_is_prob = last_channel_is_prob\n",
        "        self.fc1 = nn.Linear(128, self.model_size * self.seq_len)\n",
        "        self.block = nn.Sequential(ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size))\n",
        "        self.conv1 = nn.Conv1d(self.model_size, self.onehot_len, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output = self.fc1(noise)\n",
        "        output = output.view(-1, self.model_size, self.seq_len)\n",
        "        output = self.block(output)\n",
        "        output = self.conv1(output)\n",
        "        output = output.transpose(1, 2)\n",
        "        shape = output.size()\n",
        "        output = output.contiguous()\n",
        "        output = output.view(noise.shape[0] * self.seq_len, -1)\n",
        "        prob_channel = output.shape[-1]\n",
        "        if self.last_channel_is_prob:\n",
        "            prob_channel = -1\n",
        "            output1 = self.softmax(output[:, :prob_channel])\n",
        "            output2 = self.sigmoid(output[:, prob_channel]).view(output.shape[0], 1)\n",
        "            output = torch.cat((output1, output2), 1)\n",
        "        else:\n",
        "            output = self.softmax(output)\n",
        "        return output.view(shape)\n",
        "\n",
        "\n",
        "class G4GANDiscriminator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_size,\n",
        "            seq_len,\n",
        "            onehot_len,\n",
        "    ):\n",
        "        super(G4GANDiscriminator, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.seq_len = seq_len\n",
        "        self.onehot_len = onehot_len\n",
        "        self.block = nn.Sequential(ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size))\n",
        "        self.conv1d = nn.Conv1d(self.onehot_len, self.model_size, 1)\n",
        "        self.linear = nn.Linear(self.seq_len * self.model_size, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.transpose(1, 2)\n",
        "        output = self.conv1d(output)\n",
        "        output = self.block(output)\n",
        "        output = output.view(-1, self.seq_len * self.model_size)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def load_g4gan_generator(\n",
        "        filepath,\n",
        "        model_size=64,\n",
        "        ngpus=1,\n",
        "        num_channels=5,\n",
        "        latent_dim=100,\n",
        "        post_proc_filt_len=512,\n",
        "        upsample=True,\n",
        "        last_channel_is_prob=True,\n",
        "        **kwargs\n",
        "):\n",
        "    model = G4GANGenerator(\n",
        "        model_size=model_size,\n",
        "        ngpus=ngpus,\n",
        "        num_channels=num_channels,\n",
        "        latent_dim=latent_dim,\n",
        "        post_proc_filt_len=post_proc_filt_len,\n",
        "        upsample=upsample,\n",
        "        last_channel_is_prob=last_channel_is_prob,\n",
        "    )\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_g4gan_discriminator(\n",
        "        filepath,\n",
        "        model_size=64,\n",
        "        ngpus=1,\n",
        "        num_channels=5,\n",
        "        shift_factor=2,\n",
        "        alpha=0.2,\n",
        "        **kwargs\n",
        "):\n",
        "    model = G4GANDiscriminator(model_size=model_size, ngpus=ngpus, num_channels=num_channels,\n",
        "                               shift_factor=shift_factor, alpha=alpha)\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    return model\n",
        "\n",
        "\n",
        "# Save samles method\n",
        "def save_samples(\n",
        "        epoch_samples,\n",
        "        epoch,\n",
        "        output_dir,\n",
        "        model_gen,\n",
        "        model_dis,\n",
        "        last_channel_is_prob=True,\n",
        "):\n",
        "    \"\"\"\n",
        " Save output samples to disk\n",
        " \"\"\"\n",
        "    sample_dir = output_dir\n",
        "    if not os.path.exists(sample_dir):\n",
        "        os.makedirs(sample_dir)\n",
        "    samples = np.array(epoch_samples)\n",
        "    if last_channel_is_prob:\n",
        "        labels = np.take(samples, -1, axis=2)\n",
        "        samples = np.delete(samples, -1, axis=2)\n",
        "    seq = np.argmax(samples, axis=2)\n",
        "    df = pd.DataFrame(np.take(np.array(list(codes.keys())),\n",
        "                              indices=seq))\n",
        "    res = []\n",
        "    for i in range(len(seq)):\n",
        "        res.append(np.array(df.iloc[i]))\n",
        "        if last_channel_is_prob:\n",
        "            res.append(labels[i])\n",
        "    res_df = pd.DataFrame(res)\n",
        "    print(res_df)\n",
        "    res_path = os.path.join(sample_dir, '{}.csv'.format(epoch))\n",
        "    res_df.to_csv(res_path, index=False)\n",
        "    # save model\n",
        "    model_gen_output_path = os.path.join(sample_dir,\n",
        "                                         'model_gen_last.pkl')\n",
        "    model_dis_output_path = os.path.join(sample_dir,\n",
        "                                         'model_dis_last.pkl')\n",
        "    torch.save(model_gen.state_dict(), model_gen_output_path,\n",
        "               pickle_protocol=pk.HIGHEST_PROTOCOL)\n",
        "    torch.save(model_dis.state_dict(), model_dis_output_path,\n",
        "               pickle_protocol=pk.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "# Wasserstain training process\n",
        "LOGGER = logging.getLogger('g4gan')\n",
        "LOGGER.setLevel(logging.DEBUG)\n",
        "\n",
        "\n",
        "def compute_discr_loss_terms(\n",
        "        model_dis,\n",
        "        model_gen,\n",
        "        real_data_v,\n",
        "        noise_v,\n",
        "        batch_size,\n",
        "        latent_dim,\n",
        "        lmbda,\n",
        "        use_cuda,\n",
        "        use_binary_mask,\n",
        "        mask_v,\n",
        "        p_mask,\n",
        "        compute_grads=False,\n",
        "        last_channel_is_prob=True,\n",
        "):\n",
        "    # Convenient values for\n",
        "    one = torch.tensor(1, dtype=torch.float)\n",
        "    neg_one = one * -1\n",
        "    if use_cuda:\n",
        "        one = one.cuda()\n",
        "        neg_one = neg_one.cuda()\n",
        "    # Reset gradients\n",
        "    model_dis.zero_grad()\n",
        "    # Apply binary mask to real data\n",
        "    if use_binary_mask:\n",
        "        real_data_v = real_data_v * mask_v * (1 / p_mask)\n",
        "        if use_cuda:\n",
        "            real_data_v = real_data_v.cuda()\n",
        "        real_data_v = autograd.Variable(real_data_v)\n",
        "    # a) Compute loss contribution from real training data and backprop\n",
        "    # (negative of the empirical mean, w.r.t. the data distribution, of the discr.output)\n",
        "    D_real = model_dis(real_data_v)\n",
        "    D_real = D_real.mean()\n",
        "    # Negate since we want to _maximize_ this quantity\n",
        "    if compute_grads:\n",
        "        D_real.backward(neg_one)\n",
        "    # b) Compute loss contribution from generated data and backprop\n",
        "    # (empirical mean, w.r.t. the generator distribution, of the discr. output)\n",
        "    # Generate noise in latent space\n",
        "    # Generate data by passing noise through the generator\n",
        "    fake = autograd.Variable(model_gen(noise_v).data)\n",
        "    inputv = fake\n",
        "    # Apply binary mask to fake data\n",
        "    if use_binary_mask:\n",
        "        inputv = inputv * mask_v * (1 / p_mask)\n",
        "        if use_cuda:\n",
        "            inputv = inputv.cuda()\n",
        "        inputv = autograd.Variable(inputv)\n",
        "    D_fake = model_dis(inputv)\n",
        "    D_fake = D_fake.mean()\n",
        "    if compute_grads:\n",
        "        D_fake.backward(one)\n",
        "    # c) Compute gradient penalty and backprop\n",
        "    gradient_penalty = calc_gradient_penalty(model_dis,\n",
        "                                             real_data_v.data,\n",
        "                                             fake.data,\n",
        "                                             batch_size,\n",
        "                                             lmbda,\n",
        "                                             use_cuda=use_cuda,\n",
        "                                             )\n",
        "    if compute_grads:\n",
        "        gradient_penalty.backward(one)\n",
        "    # Compute metrics and record in batch history\n",
        "    D_cost = D_fake - D_real + gradient_penalty\n",
        "    Wasserstein_D = D_real - D_fake\n",
        "    return (D_cost, Wasserstein_D)\n",
        "\n",
        "\n",
        "def compute_gener_loss_terms(\n",
        "        model_dis,\n",
        "        model_gen,\n",
        "        batch_size,\n",
        "        latent_dim,\n",
        "        use_cuda,\n",
        "        use_binary_mask,\n",
        "        mask_v,\n",
        "        p_mask,\n",
        "        compute_grads=False,\n",
        "        last_channel_is_prob=True,\n",
        "):\n",
        "    # Convenient values for\n",
        "    one = torch.tensor(1, dtype=torch.float)\n",
        "    neg_one = one * -1\n",
        "    if use_cuda:\n",
        "        one = one.cuda()\n",
        "        neg_one = neg_one.cuda()\n",
        "    # Reset generator gradients\n",
        "    model_gen.zero_grad()\n",
        "    # Sample from the generator\n",
        "    noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
        "    if use_cuda:\n",
        "        noise = noise.cuda()\n",
        "    noise_v = autograd.Variable(noise)\n",
        "    fake = model_gen(noise_v)\n",
        "    # Apply binary mask to fake data\n",
        "    if use_binary_mask:\n",
        "        fake = fake * mask_v * (1 / p_mask)\n",
        "    if use_cuda:\n",
        "        fake = fake.cuda()\n",
        "    # Compute generator loss and backprop\n",
        "    # (negative of empirical mean (w.r.t generator distribution) of discriminator\n",
        "    G = model_dis(fake)\n",
        "    G = G.mean()\n",
        "\n",
        "    if compute_grads:\n",
        "        G.backward(neg_one)\n",
        "    G_cost = -G\n",
        "    return G_cost\n",
        "\n",
        "\n",
        "def np_to_input_var(data, use_cuda):\n",
        "    data = torch.Tensor(data)\n",
        "    if use_cuda:\n",
        "        data = data.cuda()\n",
        "    return autograd.Variable(data)\n",
        "\n",
        "\n",
        "# Adapted from https://github.com/caogang/wgan-gp/blob/master/gan_toy.py\n",
        "def calc_gradient_penalty(\n",
        "        model_dis,\n",
        "        real_data,\n",
        "        fake_data,\n",
        "        batch_size,\n",
        "        lmbda,\n",
        "        use_cuda=True,\n",
        "):\n",
        "    # Compute interpolation factors\n",
        "    alpha = torch.rand(batch_size, 1, 1)\n",
        "    alpha = alpha.expand(real_data.size())\n",
        "    alpha = (alpha.cuda() if use_cuda else alpha)\n",
        "    # Interpolate between real and fake data\n",
        "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
        "    if use_cuda:\n",
        "        interpolates = interpolates.cuda()\n",
        "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
        "    # Evaluate discriminator\n",
        "    disc_interpolates = model_dis(interpolates)\n",
        "    # Obtain gradients of the discriminator with respect to the inputs\n",
        "    gradients = autograd.grad(\n",
        "        outputs=disc_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=(torch.ones(disc_interpolates.size()).cuda() if use_cuda else\n",
        "                      torch.ones(disc_interpolates.size())),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    # Compute MSE between 1.0 and the gradient of the norm penalty to encourage\n",
        "\n",
        "    # to be a 1-Lipschitz function\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() \\\n",
        "                       * lmbda\n",
        "    return gradient_penalty"
      ],
      "metadata": {
        "id": "YfnaZajXhMWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, model_size):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.res_block = nn.Sequential(nn.ReLU(True),\n",
        "                                       nn.Conv1d(model_size, model_size, 5, padding=2),\n",
        "                                       nn.ReLU(True), nn.Conv1d(model_size, model_size, 5, padding=2))\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.res_block(input)\n",
        "        return input + 0.3 * output\n",
        "\n",
        "\n",
        "class G4GANGenerator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_size,\n",
        "            seq_len,\n",
        "            onehot_len,\n",
        "            last_channel_is_prob=False,\n",
        "    ):\n",
        "        super(G4GANGenerator, self).__init__()\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.seq_len = seq_len\n",
        "        self.onehot_len = onehot_len\n",
        "        self.last_channel_is_prob = last_channel_is_prob\n",
        "        self.fc1 = nn.Linear(128, self.model_size * self.seq_len)\n",
        "        self.block = nn.Sequential(ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size))\n",
        "        self.conv1 = nn.Conv1d(self.model_size, self.onehot_len, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output = self.fc1(noise)\n",
        "        output = output.view(-1, self.model_size, self.seq_len)\n",
        "        output = self.block(output)\n",
        "        output = self.conv1(output)\n",
        "        output = output.transpose(1, 2)\n",
        "        shape = output.size()\n",
        "        output = output.contiguous()\n",
        "        output = output.view(noise.shape[0] * self.seq_len, -1)\n",
        "        prob_channel = output.shape[-1]\n",
        "        if self.last_channel_is_prob:\n",
        "            prob_channel = -1\n",
        "            output1 = self.softmax(output[:, :prob_channel])\n",
        "            output2 = self.sigmoid(output[:, prob_channel]).view(output.shape[0], 1)\n",
        "            output = torch.cat((output1, output2), 1)\n",
        "        else:\n",
        "            output = self.softmax(output)\n",
        "        return output.view(shape)\n",
        "\n",
        "\n",
        "class G4GANDiscriminator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_size,\n",
        "            seq_len,\n",
        "            onehot_len,\n",
        "    ):\n",
        "        super(G4GANDiscriminator, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.seq_len = seq_len\n",
        "        self.onehot_len = onehot_len\n",
        "        self.block = nn.Sequential(ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size),\n",
        "                                   ResBlock(self.model_size))\n",
        "        self.conv1d = nn.Conv1d(self.onehot_len, self.model_size, 1)\n",
        "        self.linear = nn.Linear(self.seq_len * self.model_size, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.transpose(1, 2)\n",
        "        output = self.conv1d(output)\n",
        "        output = self.block(output)\n",
        "        output = output.view(-1, self.seq_len * self.model_size)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def load_g4gan_generator(\n",
        "        filepath,\n",
        "        model_size=64,\n",
        "        ngpus=1,\n",
        "        num_channels=5,\n",
        "        latent_dim=100,\n",
        "        post_proc_filt_len=512,\n",
        "        upsample=True,\n",
        "        last_channel_is_prob=True,\n",
        "        **kwargs\n",
        "):\n",
        "    model = G4GANGenerator(\n",
        "        model_size=model_size,\n",
        "        ngpus=ngpus,\n",
        "        num_channels=num_channels,\n",
        "        latent_dim=latent_dim,\n",
        "        post_proc_filt_len=post_proc_filt_len,\n",
        "        upsample=upsample,\n",
        "        last_channel_is_prob=last_channel_is_prob,\n",
        "    )\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_g4gan_discriminator(\n",
        "        filepath,\n",
        "        model_size=64,\n",
        "        ngpus=1,\n",
        "        num_channels=5,\n",
        "        shift_factor=2,\n",
        "        alpha=0.2,\n",
        "        **kwargs\n",
        "):\n",
        "    model = G4GANDiscriminator(model_size=model_size, ngpus=ngpus, num_channels=num_channels,\n",
        "                               shift_factor=shift_factor, alpha=alpha)\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    return model\n",
        "\n",
        "\n",
        "# Save samles method\n",
        "def save_samples(\n",
        "        epoch_samples,\n",
        "        epoch,\n",
        "        output_dir,\n",
        "        model_gen,\n",
        "        model_dis,\n",
        "        last_channel_is_prob=True,\n",
        "):\n",
        "    \"\"\"\n",
        " Save output samples to disk\n",
        " \"\"\"\n",
        "    sample_dir = output_dir\n",
        "    if not os.path.exists(sample_dir):\n",
        "        os.makedirs(sample_dir)\n",
        "    samples = np.array(epoch_samples)\n",
        "    if last_channel_is_prob:\n",
        "        labels = np.take(samples, -1, axis=2)\n",
        "        samples = np.delete(samples, -1, axis=2)\n",
        "    seq = np.argmax(samples, axis=2)\n",
        "    df = pd.DataFrame(np.take(np.array(list(codes.keys())),\n",
        "                              indices=seq))\n",
        "    res = []\n",
        "    for i in range(len(seq)):\n",
        "        res.append(np.array(df.iloc[i]))\n",
        "        if last_channel_is_prob:\n",
        "            res.append(labels[i])\n",
        "    res_df = pd.DataFrame(res)\n",
        "    print(res_df)\n",
        "    res_path = os.path.join(sample_dir, '{}.csv'.format(epoch))\n",
        "    res_df.to_csv(res_path, index=False)\n",
        "    # save model\n",
        "    model_gen_output_path = os.path.join(sample_dir,\n",
        "                                         'model_gen_last.pkl')\n",
        "    model_dis_output_path = os.path.join(sample_dir,\n",
        "                                         'model_dis_last.pkl')\n",
        "    torch.save(model_gen.state_dict(), model_gen_output_path,\n",
        "               pickle_protocol=pk.HIGHEST_PROTOCOL)\n",
        "    torch.save(model_dis.state_dict(), model_dis_output_path,\n",
        "               pickle_protocol=pk.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "# Wasserstain training process\n",
        "LOGGER = logging.getLogger('g4gan')\n",
        "LOGGER.setLevel(logging.DEBUG)\n",
        "\n",
        "\n",
        "def compute_discr_loss_terms(\n",
        "        model_dis,\n",
        "        model_gen,\n",
        "        real_data_v,\n",
        "        noise_v,\n",
        "        batch_size,\n",
        "        latent_dim,\n",
        "        lmbda,\n",
        "        use_cuda,\n",
        "        use_binary_mask,\n",
        "        p_mask,\n",
        "        compute_grads=False,\n",
        "        last_channel_is_prob=True,\n",
        "):\n",
        "    # Convenient values for\n",
        "    one = torch.tensor(1, dtype=torch.float)\n",
        "    neg_one = one * -1\n",
        "    if use_cuda:\n",
        "        one = one.cuda()\n",
        "        neg_one = neg_one.cuda()\n",
        "    # Reset gradients\n",
        "    model_dis.zero_grad()\n",
        "    # Apply binary mask to real data\n",
        "    if use_binary_mask:\n",
        "        real_data_v = real_data_v  * (1 / p_mask)\n",
        "        if use_cuda:\n",
        "            real_data_v = real_data_v.cuda()\n",
        "        real_data_v = autograd.Variable(real_data_v)\n",
        "    # a) Compute loss contribution from real training data and backprop\n",
        "    # (negative of the empirical mean, w.r.t. the data distribution, of the discr.output)\n",
        "    D_real = model_dis(real_data_v)\n",
        "    D_real = D_real.mean()\n",
        "    # Negate since we want to _maximize_ this quantity\n",
        "    if compute_grads:\n",
        "        D_real.backward(neg_one)\n",
        "    # b) Compute loss contribution from generated data and backprop\n",
        "    # (empirical mean, w.r.t. the generator distribution, of the discr. output)\n",
        "    # Generate noise in latent space\n",
        "    # Generate data by passing noise through the generator\n",
        "    fake = autograd.Variable(model_gen(noise_v).data)\n",
        "    inputv = fake\n",
        "    # Apply binary mask to fake data\n",
        "    if use_binary_mask:\n",
        "        inputv = inputv * (1 / p_mask)\n",
        "        if use_cuda:\n",
        "            inputv = inputv.cuda()\n",
        "        inputv = autograd.Variable(inputv)\n",
        "    D_fake = model_dis(inputv)\n",
        "    D_fake = D_fake.mean()\n",
        "    if compute_grads:\n",
        "        D_fake.backward(one)\n",
        "    # c) Compute gradient penalty and backprop\n",
        "    gradient_penalty = calc_gradient_penalty(model_dis,\n",
        "                                             real_data_v.data,\n",
        "                                             fake.data,\n",
        "                                             batch_size,\n",
        "                                             lmbda,\n",
        "                                             use_cuda=use_cuda,\n",
        "                                             )\n",
        "    if compute_grads:\n",
        "        gradient_penalty.backward(one)\n",
        "    # Compute metrics and record in batch history\n",
        "    D_cost = D_fake - D_real + gradient_penalty\n",
        "    Wasserstein_D = D_real - D_fake\n",
        "    return (D_cost, Wasserstein_D)\n",
        "\n",
        "\n",
        "def compute_gener_loss_terms(\n",
        "        model_dis,\n",
        "        model_gen,\n",
        "        batch_size,\n",
        "        latent_dim,\n",
        "        use_cuda,\n",
        "        use_binary_mask,\n",
        "        p_mask,\n",
        "        compute_grads=False,\n",
        "        last_channel_is_prob=True,\n",
        "):\n",
        "    # Convenient values for\n",
        "    one = torch.tensor(1, dtype=torch.float)\n",
        "    neg_one = one * -1\n",
        "    if use_cuda:\n",
        "        one = one.cuda()\n",
        "        neg_one = neg_one.cuda()\n",
        "    # Reset generator gradients\n",
        "    model_gen.zero_grad()\n",
        "    # Sample from the generator\n",
        "    noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
        "    if use_cuda:\n",
        "        noise = noise.cuda()\n",
        "    noise_v = autograd.Variable(noise)\n",
        "    fake = model_gen(noise_v)\n",
        "    # Apply binary mask to fake data\n",
        "    if use_binary_mask:\n",
        "        fake = fake  * (1 / p_mask)\n",
        "    if use_cuda:\n",
        "        fake = fake.cuda()\n",
        "    # Compute generator loss and backprop\n",
        "    # (negative of empirical mean (w.r.t generator distribution) of discriminator\n",
        "    G = model_dis(fake)\n",
        "    G = G.mean()\n",
        "\n",
        "    if compute_grads:\n",
        "        G.backward(neg_one)\n",
        "    G_cost = -G\n",
        "    return G_cost\n",
        "\n",
        "\n",
        "def np_to_input_var(data, use_cuda):\n",
        "    data = torch.Tensor(data)\n",
        "    if use_cuda:\n",
        "        data = data.cuda()\n",
        "    return autograd.Variable(data)\n",
        "\n",
        "\n",
        "# Adapted from https://github.com/caogang/wgan-gp/blob/master/gan_toy.py\n",
        "def calc_gradient_penalty(\n",
        "        model_dis,\n",
        "        real_data,\n",
        "        fake_data,\n",
        "        batch_size,\n",
        "        lmbda,\n",
        "        use_cuda=True,\n",
        "):\n",
        "    # Compute interpolation factors\n",
        "    alpha = torch.rand(batch_size, 1, 1)\n",
        "    alpha = alpha.expand(real_data.size())\n",
        "    alpha = (alpha.cuda() if use_cuda else alpha)\n",
        "    # Interpolate between real and fake data\n",
        "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
        "    if use_cuda:\n",
        "        interpolates = interpolates.cuda()\n",
        "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
        "    # Evaluate discriminator\n",
        "    disc_interpolates = model_dis(interpolates)\n",
        "    # Obtain gradients of the discriminator with respect to the inputs\n",
        "    gradients = autograd.grad(\n",
        "        outputs=disc_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=(torch.ones(disc_interpolates.size()).cuda() if use_cuda else\n",
        "                      torch.ones(disc_interpolates.size())),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    # Compute MSE between 1.0 and the gradient of the norm penalty to encourage\n",
        "\n",
        "    # to be a 1-Lipschitz function\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() \\\n",
        "                       * lmbda\n",
        "    return gradient_penalty"
      ],
      "metadata": {
        "id": "H7BC_Xz3_5qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_wgan(\n",
        "        model_gen,\n",
        "        model_dis,\n",
        "        train_gen,\n",
        "        valid_gen,\n",
        "        test_data,\n",
        "        num_epochs,\n",
        "        batches_per_epoch,\n",
        "        batch_size,\n",
        "        output_dir=None,\n",
        "        lmbda=0.1,\n",
        "        use_cuda=True,\n",
        "        discriminator_updates=5,\n",
        "        epochs_per_sample=10,\n",
        "        sample_size=20,\n",
        "        lr=1e-4,\n",
        "        beta_1=0.5,\n",
        "        beta_2=0.9,\n",
        "        latent_dim=100,\n",
        "        use_knn_score=False,\n",
        "        knn_score_sample_size=1000,\n",
        "        knn_score_real_data=None,\n",
        "        use_binary_mask=False,\n",
        "        start_p_mask=1,\n",
        "        mask_end_epoch=100,\n",
        "        last_channel_is_prob=True,\n",
        "):\n",
        "    if use_cuda:\n",
        "        model_gen = model_gen.cuda()\n",
        "        model_dis = model_dis.cuda()\n",
        "    # Initialize optimizers for each model\n",
        "    optimizer_gen = optim.Adam(model_gen.parameters(), lr=lr,\n",
        "                               betas=(beta_1, beta_2))\n",
        "    optimizer_dis = optim.Adam(model_dis.parameters(), lr=lr,\n",
        "                               betas=(beta_1, beta_2))\n",
        "    # Sample noise used for seeing the evolution of generated output samples\n",
        "\n",
        "    sample_noise = torch.Tensor(sample_size, latent_dim).uniform_(-1, 1)\n",
        "    if use_cuda:\n",
        "        sample_noise = sample_noise.cuda()\n",
        "    sample_noise_v = autograd.Variable(sample_noise)\n",
        "    samples = {}\n",
        "    history = []\n",
        "    plot_history = []\n",
        "    plot_knn_score_history = []\n",
        "    train_iter = iter(train_gen)\n",
        "    valid_iter = iter(valid_gen)\n",
        "    test_data_v = np_to_input_var(test_data, use_cuda)\n",
        "    # Sample noise for KNN\n",
        "    if use_knn_score and knn_score_real_data is not None:\n",
        "        LOGGER.info('Create KDTree')\n",
        "        X = np.argmax(knn_score_real_data, 2)\n",
        "\n",
        "        knn_tree = NearestNeighbors(n_neighbors=2, algorithm='auto',\n",
        "                                    metric=lambda a, b: \\\n",
        "                                        editdistance.eval(a, b))\n",
        "        knn_tree.fit(X)\n",
        "        knn_sample_noise = torch.Tensor(knn_score_sample_size,\n",
        "                                        latent_dim).uniform_(-1, 1)\n",
        "        if use_cuda:\n",
        "            knn_sample_noise = knn_sample_noise.cuda()\n",
        "        knn_sample_noise_v = autograd.Variable(knn_sample_noise)\n",
        "        # Score on train data\n",
        "        (dist, ind) = knn_tree.kneighbors(X[:], 2)\n",
        "        d_self = np.mean(np.take(dist, 1, axis=1))\n",
        "        d_train = np.mean(np.take(dist, 0, axis=1))\n",
        "        LOGGER.info('KNN score on train data:')\n",
        "        LOGGER.info('D_self: {}'.format(d_self))\n",
        "        LOGGER.info('D_train: {}'.format(d_train))\n",
        "        Y = np.argmax(test_data, 2)\n",
        "        knn_self_tree = NearestNeighbors(n_neighbors=2, algorithm='auto'\n",
        "                                         , metric=lambda a, b: editdistance.eval(a, b))\n",
        "        knn_self_tree.fit(Y)\n",
        "        (dist, ind) = knn_self_tree.kneighbors(Y[:], 2)\n",
        "        d_self = np.mean(np.take(dist, 1, axis=1))\n",
        "        (dist, ind) = knn_tree.kneighbors(Y[:], 1)\n",
        "        d_train = np.mean(np.take(dist, 0, axis=1))\n",
        "        LOGGER.info('KNN score on test data:')\n",
        "        LOGGER.info('D_self: {}'.format(d_self))\n",
        "        LOGGER.info('D_train: {}'.format(d_train))\n",
        "        best_self_knn_score = 0\n",
        "    # Set p_mask\n",
        "    p_mask = None\n",
        "    if use_binary_mask:\n",
        "        p_mask = start_p_mask\n",
        "        LOGGER.info('Setup p_mask: {}'.format(p_mask))\n",
        "     \n",
        "\n",
        "    # Loop over the dataset multiple times\n",
        "    for epoch in range(num_epochs):\n",
        "        LOGGER.info('Epoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "        epoch_history = []\n",
        "        for batch_idx in range(batches_per_epoch):\n",
        "            # Set model parameters to require gradients to be computed and stored\n",
        "            for p in model_dis.parameters():\n",
        "                p.requires_grad = True\n",
        "            # Initialize the metrics for this batch\n",
        "            batch_history = {'discriminator': [], 'generator': {}}\n",
        "            # Discriminator Training Phase:\n",
        "\n",
        "            # -> Train discriminator k times\n",
        "            for iter_d in range(discriminator_updates):\n",
        "                # Get real examples\n",
        "                real_data_v = np_to_input_var(next(train_iter),\n",
        "                                              use_cuda)\n",
        "                # Get valid examples\n",
        "                valid_data_v = np_to_input_var(next(valid_iter),\n",
        "                                               use_cuda)\n",
        "                # Get noise\n",
        "                noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
        "                if use_cuda:\n",
        "                    noise = noise.cuda()\n",
        "                noise_v = autograd.Variable(noise, volatile=True)  # totally freeze model_gen\n",
        "    # Get new batch of real training data\n",
        "    (D_cost_train, D_wass_train) = compute_discr_loss_terms(\n",
        "        model_dis,\n",
        "        model_gen,\n",
        "        real_data_v,\n",
        "        noise_v,\n",
        "        batch_size,\n",
        "        latent_dim,\n",
        "        lmbda,\n",
        "        use_cuda,\n",
        "        use_binary_mask,\n",
        "        p_mask,\n",
        "        compute_grads=True,\n",
        "        last_channel_is_prob=last_channel_is_prob,\n",
        "    )\n",
        "    # Update the discriminator\n",
        "    optimizer_dis.step()\n",
        "    (D_cost_valid, D_wass_valid) = compute_discr_loss_terms(\n",
        "        model_dis,\n",
        "        model_gen,\n",
        "        valid_data_v,\n",
        "        noise_v,\n",
        "        batch_size,\n",
        "        latent_dim,\n",
        "        lmbda,\n",
        "        use_cuda,\n",
        "        use_binary_mask,\n",
        "        p_mask,\n",
        "        compute_grads=False,\n",
        "        last_channel_is_prob=last_channel_is_prob,\n",
        "    )\n",
        "    if use_cuda:\n",
        "        D_cost_train = D_cost_train.cpu()\n",
        "        D_cost_valid = D_cost_valid.cpu()\n",
        "        D_wass_train = D_wass_train.cpu()\n",
        "        D_wass_valid = D_wass_valid.cpu()\n",
        "\n",
        "    batch_history['discriminator'].append({\n",
        "        'cost': np.asscalar(D_cost_train.data.numpy().astype(float)),\n",
        "        'wasserstein_cost':\n",
        "            np.asscalar(D_wass_train.data.numpy().astype(float)),\n",
        "        'cost_validation':\n",
        "            np.asscalar(D_cost_valid.data.numpy().astype(float)),\n",
        "        'wasserstein_cost_validation':\n",
        "            np.asscalar(D_wass_valid.data.numpy().astype(float)),\n",
        "    })\n",
        "    # ###########################\n",
        "    # (2) Update G network\n",
        "    # ##########################\n",
        "    # Prevent discriminator from computing gradients, since\n",
        "    # we are only updating the generator\n",
        "    for p in model_dis.parameters():\n",
        "        p.requires_grad = False\n",
        "    G_cost = compute_gener_loss_terms(\n",
        "        model_dis,\n",
        "        model_gen,\n",
        "        batch_size,\n",
        "        latent_dim,\n",
        "        use_cuda,\n",
        "        use_binary_mask,\n",
        "        p_mask,\n",
        "        compute_grads=True,\n",
        "        last_channel_is_prob=last_channel_is_prob,\n",
        "    )\n",
        "    # Update generator\n",
        "    optimizer_gen.step()\n",
        "    if use_cuda:\n",
        "        G_cost = G_cost.cpu()\n",
        "    # Record generator loss\n",
        "    batch_history['generator']['cost'] = \\\n",
        "        np.asscalar(G_cost.data.numpy().astype(float))\n",
        "    # Record batch metrics\n",
        "    epoch_history.append(batch_history)\n",
        "    # Update binary mask\n",
        "    if use_binary_mask:\n",
        "        next_epoch = epoch + 1\n",
        "        p_mask = (1 - start_p_mask) / mask_end_epoch * next_epoch \\\n",
        "                 + start_p_mask\n",
        "\n",
        "        if next_epoch == mask_end_epoch or p_mask > 1.:\n",
        "            p_mask = 1.\n",
        "        LOGGER.info('Update p_mask: {}'.format(p_mask))\n",
        "    # Record epoch metrics\n",
        "    history.append(epoch_history)\n",
        "    LOGGER.info(pprint.pformat(epoch_history[-1]))\n",
        "    # Plot\n",
        "    plot_history.append(epoch_history[-1]['discriminator'][-1])\n",
        "    plot_history[-1]['gen_cost'] = epoch_history[-1]['generator'\n",
        "    ]['cost']\n",
        "    pd.DataFrame(plot_history).plot()\n",
        "    plt.show()\n",
        "    # Calc KNN score\n",
        "    if use_knn_score and knn_tree is not None:\n",
        "        LOGGER.info('Calculate KNN score...')\n",
        "        # Self KNN score (D_self)\n",
        "        knn_samp_output = model_gen(knn_sample_noise_v)\n",
        "        if use_cuda:\n",
        "            knn_samp_output = knn_samp_output.cpu()\n",
        "        # One hot\n",
        "        prob_channel = knn_samp_output.shape[2]\n",
        "        if last_channel_is_prob:\n",
        "            prob_channel = -1\n",
        "        (values, indices) = knn_samp_output[:, :, :\n",
        "                                                  prob_channel].max(2)\n",
        "        knn_samp_output[:, :, :prob_channel] = 0\n",
        "        indices = indices.view(indices.shape[0], indices.shape[1],\n",
        "                               1)\n",
        "        knn_samp_output = knn_samp_output.scatter_(2, indices, 1)\n",
        "        knn_samp_output = knn_samp_output.data.numpy()\n",
        "        Y = np.argmax(knn_samp_output, 2)\n",
        "        knn_self_tree = NearestNeighbors(n_neighbors=2,\n",
        "                                         algorithm='auto', metric=lambda a, b: \\\n",
        "                editdistance.eval(a, b))\n",
        "        knn_self_tree.fit(Y)\n",
        "        (dist, ind) = knn_self_tree.kneighbors(Y[:], 2)\n",
        "        d_self = np.mean(np.take(dist, 1, axis=1))\n",
        "        # Train KNN score\n",
        "        (dist, ind) = knn_tree.kneighbors(Y[:], 1)\n",
        "        d_train = np.mean(np.take(dist, 0, axis=1))\n",
        "        # History and plot\n",
        "        LOGGER.info('D_self: {}'.format(d_self))\n",
        "        LOGGER.info('D_train: {}'.format(d_train))\n",
        "        plot_knn_score_history.append({'d_self': d_self,\n",
        "                                       'd_train': d_train})\n",
        "        pd.DataFrame(plot_knn_score_history).plot()\n",
        "\n",
        "        plt.show()\n",
        "        # Save best knn self score model\n",
        "        if output_dir and d_self > best_self_knn_score:\n",
        "            best_self_knn_score = d_self\n",
        "            # save model\n",
        "            model_gen_output_path = os.path.join(output_dir,\n",
        "\n",
        "                                                 'model_gen_best_knn_self_{}.pkl'.format(int(best_self_knn_score)))\n",
        "            model_dis_output_path = os.path.join(output_dir,\n",
        "\n",
        "                                                 'model_dis_best_knn_self_{}.pkl'.format(int(best_self_knn_score)))\n",
        "            torch.save(model_gen.state_dict(),\n",
        "                       model_gen_output_path,\n",
        "                       pickle_protocol=pk.HIGHEST_PROTOCOL)\n",
        "            torch.save(model_dis.state_dict(),\n",
        "                       model_dis_output_path,\n",
        "                       pickle_protocol=pk.HIGHEST_PROTOCOL)\n",
        "    if (epoch + 1) % epochs_per_sample == 0:\n",
        "        # Generate outputs for fixed latent samples\n",
        "        LOGGER.info('Generating samples...')\n",
        "        samp_output = model_gen(sample_noise_v)\n",
        "        if use_cuda:\n",
        "            samp_output = samp_output.cpu()\n",
        "        samples[epoch + 1] = samp_output.data.numpy()\n",
        "        if output_dir:\n",
        "            LOGGER.info('Saving samples...')\n",
        "            save_samples(\n",
        "                samples[epoch + 1],\n",
        "                epoch + 1,\n",
        "                output_dir,\n",
        "                model_gen,\n",
        "                model_dis,\n",
        "                last_channel_is_prob=last_channel_is_prob,\n",
        "            )\n",
        "    # # Get final discriminator loss\n",
        "    # Get noise\n",
        "    noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
        "    if use_cuda:\n",
        "        noise = noise.cuda()\n",
        "    noise_v = autograd.Variable(noise, volatile=True)  # totally freeze generator\n",
        "    final_discr_metrics = {\n",
        "        'cost_validation': 0,\n",
        "        'wasserstein_cost_validation': 0,\n",
        "        'cost_test': 0,\n",
        "        'wasserstein_cost_test': 0,\n",
        "    }\n",
        "    return model_gen, model_dis, history, final_discr_metrics, samples\n",
        "\n",
        "\n",
        "# Batch generator\n",
        "\n",
        "def batch_generator(data, batch_size, shuffle_each_epoch=True):\n",
        "    indices = np.arange(data.shape[0])\n",
        "    batch = []\n",
        "    while True:\n",
        "        if shuffle_each_epoch:\n",
        "            np.random.shuffle(indices)\n",
        "        for i in indices:\n",
        "            batch.append(i)\n",
        "            if len(batch) == batch_size:\n",
        "                yield data[batch]\n",
        "                batch = []\n",
        "\n",
        "\n",
        "def create_data_split(\n",
        "        g4_np_dataset_path,\n",
        "        valid_ratio,\n",
        "        test_ratio,\n",
        "        train_batch_size,\n",
        "        shuffle_each_epoch=True,\n",
        "        train_subset_size=100,\n",
        "):\n",
        "    data = np.load(g4_np_dataset_path)\n",
        "    num_g4 = data.shape[0]\n",
        "    num_valid = int(np.ceil(num_g4 * valid_ratio))\n",
        "    num_test = int(np.ceil(num_g4 * test_ratio))\n",
        "    num_train = num_g4 - num_valid - num_test\n",
        "    assert num_valid > 0\n",
        "    assert num_test > 0\n",
        "    assert num_train > 0\n",
        "    indices = np.arange(num_g4)\n",
        "    np.random.shuffle(indices)\n",
        "    train_data_indices = indices[:num_train]\n",
        "    valid_data_indices = indices[num_train:num_train + num_valid]\n",
        "    test_data_indices = indices[num_train + num_valid:]\n",
        "    train_data = data[train_data_indices]\n",
        "    valid_data = data[valid_data_indices]\n",
        "    test_data = data[test_data_indices]\n",
        "    train_gen = batch_generator(train_data, train_batch_size,\n",
        "                                shuffle_each_epoch)\n",
        "    valid_gen = batch_generator(valid_data, train_batch_size,\n",
        "                                shuffle_each_epoch)\n",
        "    train_subset = None\n",
        "    if train_subset_size > 0:\n",
        "        train_subset = train_data[:train_subset_size]\n",
        "    return (train_gen, valid_gen, test_data, train_subset)\n",
        "\n",
        "\n",
        "# Log\n",
        "def init_console_logger(logger, verbose=False):\n",
        "    # Log to stderr also\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    if verbose:\n",
        "        stream_handler.setLevel(logging.DEBUG)\n",
        "    else:\n",
        "        stream_handler.setLevel(logging.INFO)\n",
        "    formatter = \\\n",
        "        logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "                          )\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    logger.addHandler(stream_handler)"
      ],
      "metadata": {
        "id": "Ta5sp_dxTOyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "path = 'G4_Chip_seq_quadruplex_norm_quad_labeled.npy'\n",
        "channels = 6\n",
        "last_channel_is_prob = True\n",
        "# path = 'G4_Chip_seq_quadruplex_norm.npy'\n",
        "# channels = 5\n",
        "# last_channel_is_prob = False\n",
        "args = {}\n",
        "args['verbose'] = True\n",
        "args['batches_per_epoch'] = 109\n",
        "args['batch_size'] = 64\n",
        "args['latent_dim'] = 128\n",
        "args['ngpus'] = 1\n",
        "args['model_size'] = 32\n",
        "args['output_dir'] = 'models/'\n",
        "args['g4_np_data_path'] = path\n",
        "args['valid_ratio'] = 0.1\n",
        "args['test_ratio'] = 0.1\n",
        "args['shuffle_train_each_epoch'] = True\n",
        "args['post_proc_filt_len'] = None\n",
        "args['alpha'] = 0.2\n",
        "args['shift_factor'] = 2\n",
        "args['batch_shuffle'] = False\n",
        "args['num_epochs'] = 500\n",
        "args['learning_rate'] = 1e-4\n",
        "args['beta1'] = 0.5\n",
        "args['beta2'] = 0.9\n",
        "args['lmbda'] = 10.0\n",
        "args['discriminator_updates'] = 5\n",
        "args['epochs_per_sample'] = 5\n",
        "args['sample_size'] = 20\n",
        "args['num_channels'] = channels\n",
        "args['use_knn_score'] = True\n",
        "args['knn_score_sample_size'] = 100\n",
        "args['use_binary_mask'] = False\n",
        "args['start_p_mask'] = 0.3\n",
        "args['mask_end_epoch'] = 90\n",
        "args['last_channel_is_prob'] = last_channel_is_prob\n",
        "init_console_logger(LOGGER, args['verbose'])\n",
        "LOGGER.info('Initialized logger.')\n",
        "batch_size = args['batch_size']\n",
        "latent_dim = args['latent_dim']\n",
        "ngpus = args['ngpus']\n",
        "model_size = args['model_size']\n",
        "model_dir = os.path.join(args['output_dir'],\n",
        "                         datetime.datetime.now().strftime('%Y%m%d%H%M%S'\n",
        "                                                          ))\n",
        "args['model_dir'] = model_dir\n",
        "if not args['use_knn_score']:\n",
        "    args['knn_score_sample_size'] = 0\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "LOGGER.info('Saving configurations...')\n",
        "config_path = os.path.join(model_dir, 'config.json')\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(args, f)\n",
        "LOGGER.info('Loading G4 data...')\n",
        "g4_np_data_path = args['g4_np_data_path']\n",
        "(train_gen, valid_gen, test_data, train_subset) = create_data_split(\n",
        "    g4_np_data_path,\n",
        "    args['valid_ratio'],\n",
        "    args['test_ratio'],\n",
        "    batch_size,\n",
        "    args['shuffle_train_each_epoch'],\n",
        "    args['knn_score_sample_size'],\n",
        ")\n",
        "LOGGER.info('Creating models...')\n",
        "model_gen = G4GANGenerator(args['model_size'], 512, args['num_channels'\n",
        "],\n",
        "                           last_channel_is_prob=args['last_channel_is_prob'\n",
        "                           ])\n",
        "model_dis = G4GANDiscriminator(args['model_size'], 512,\n",
        "                               args['num_channels'])\n",
        "LOGGER.info(model_gen)\n",
        "LOGGER.info(model_dis)\n",
        "LOGGER.info('Starting training...')\n",
        "(model_gen, model_dis, history, final_discr_metrics, samples) = \\\n",
        "    train_wgan(\n",
        "        model_gen=model_gen,\n",
        "        model_dis=model_dis,\n",
        "        train_gen=train_gen,\n",
        "        valid_gen=valid_gen,\n",
        "        test_data=test_data,\n",
        "        num_epochs=args['num_epochs'],\n",
        "        batches_per_epoch=args['batches_per_epoch'],\n",
        "        batch_size=batch_size,\n",
        "        output_dir=model_dir,\n",
        "        lr=args['learning_rate'],\n",
        "        beta_1=args['beta1'],\n",
        "        beta_2=args['beta2'],\n",
        "        lmbda=args['lmbda'],\n",
        "        use_cuda=ngpus >= 1,\n",
        "        discriminator_updates=args['discriminator_updates'],\n",
        "        latent_dim=latent_dim,\n",
        "        epochs_per_sample=args['epochs_per_sample'],\n",
        "        sample_size=args['sample_size'],\n",
        "\n",
        "        use_knn_score=args['use_knn_score'],\n",
        "        knn_score_sample_size=args['knn_score_sample_size'],\n",
        "        knn_score_real_data=train_subset,\n",
        "        use_binary_mask=args['use_binary_mask'],\n",
        "        start_p_mask=args['start_p_mask'],\n",
        "        mask_end_epoch=args['mask_end_epoch'],\n",
        "        last_channel_is_prob=args['last_channel_is_prob'],\n",
        "    )\n",
        "LOGGER.info('Finished training.')\n",
        "LOGGER.info('Final discriminator loss on validation and test:')\n",
        "LOGGER.info(pprint.pformat(final_discr_metrics))\n",
        "LOGGER.info('Saving models...')\n",
        "model_gen_output_path = os.path.join(model_dir, 'model_gen.pkl')\n",
        "model_dis_output_path = os.path.join(model_dir, 'model_dis.pkl')\n",
        "torch.save(model_gen.state_dict(), model_gen_output_path,\n",
        "           pickle_protocol=pk.HIGHEST_PROTOCOL)\n",
        "torch.save(model_dis.state_dict(), model_dis_output_path,\n",
        "           pickle_protocol=pk.HIGHEST_PROTOCOL)\n",
        "LOGGER.info('Saving metrics...')\n",
        "history_output_path = os.path.join(model_dir, 'history.pkl')\n",
        "final_discr_metrics_output_path = os.path.join(model_dir,\n",
        "                                               'final_discr_metrics.pkl')\n",
        "with open(history_output_path, 'wb') as f:\n",
        "    pk.dump(history, f)\n",
        "with open(final_discr_metrics_output_path, 'wb') as f:\n",
        "    pk.dump(final_discr_metrics, f)\n",
        "LOGGER.info('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9VlImqT9TWJ4",
        "outputId": "55a2bea5-8cfb-40ef-c3b7-671470682ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-25 12:18:48,966 - g4gan - INFO - Initialized logger.\n",
            "INFO:g4gan:Initialized logger.\n",
            "2023-04-25 12:18:48,972 - g4gan - INFO - Saving configurations...\n",
            "INFO:g4gan:Saving configurations...\n",
            "2023-04-25 12:18:48,977 - g4gan - INFO - Loading G4 data...\n",
            "INFO:g4gan:Loading G4 data...\n",
            "2023-04-25 12:18:49,119 - g4gan - INFO - Creating models...\n",
            "INFO:g4gan:Creating models...\n",
            "2023-04-25 12:18:49,178 - g4gan - INFO - G4GANGenerator(\n",
            "  (fc1): Linear(in_features=128, out_features=16384, bias=True)\n",
            "  (block): Sequential(\n",
            "    (0): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (1): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (2): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (3): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (4): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1): Conv1d(32, 6, kernel_size=(1,), stride=(1,))\n",
            "  (softmax): Softmax(dim=1)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "INFO:g4gan:G4GANGenerator(\n",
            "  (fc1): Linear(in_features=128, out_features=16384, bias=True)\n",
            "  (block): Sequential(\n",
            "    (0): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (1): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (2): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (3): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (4): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1): Conv1d(32, 6, kernel_size=(1,), stride=(1,))\n",
            "  (softmax): Softmax(dim=1)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "2023-04-25 12:18:49,182 - g4gan - INFO - G4GANDiscriminator(\n",
            "  (block): Sequential(\n",
            "    (0): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (1): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (2): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (3): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (4): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1d): Conv1d(6, 32, kernel_size=(1,), stride=(1,))\n",
            "  (linear): Linear(in_features=16384, out_features=1, bias=True)\n",
            ")\n",
            "INFO:g4gan:G4GANDiscriminator(\n",
            "  (block): Sequential(\n",
            "    (0): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (1): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (2): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (3): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (4): ResBlock(\n",
            "      (res_block): Sequential(\n",
            "        (0): ReLU(inplace=True)\n",
            "        (1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1d): Conv1d(6, 32, kernel_size=(1,), stride=(1,))\n",
            "  (linear): Linear(in_features=16384, out_features=1, bias=True)\n",
            ")\n",
            "2023-04-25 12:18:49,188 - g4gan - INFO - Starting training...\n",
            "INFO:g4gan:Starting training...\n",
            "2023-04-25 12:18:49,538 - g4gan - INFO - Create KDTree\n",
            "INFO:g4gan:Create KDTree\n",
            "2023-04-25 12:18:50,356 - g4gan - INFO - KNN score on train data:\n",
            "INFO:g4gan:KNN score on train data:\n",
            "2023-04-25 12:18:50,361 - g4gan - INFO - D_self: 245.3\n",
            "INFO:g4gan:D_self: 245.3\n",
            "2023-04-25 12:18:50,365 - g4gan - INFO - D_train: 0.0\n",
            "INFO:g4gan:D_train: 0.0\n",
            "2023-04-25 12:20:10,696 - g4gan - INFO - KNN score on test data:\n",
            "INFO:g4gan:KNN score on test data:\n",
            "2023-04-25 12:20:10,702 - g4gan - INFO - D_self: 240.5475372279496\n",
            "INFO:g4gan:D_self: 240.5475372279496\n",
            "2023-04-25 12:20:10,706 - g4gan - INFO - D_train: 246.3459335624284\n",
            "INFO:g4gan:D_train: 246.3459335624284\n",
            "2023-04-25 12:20:10,708 - g4gan - INFO - Epoch: 1/500\n",
            "INFO:g4gan:Epoch: 1/500\n",
            "<ipython-input-9-14f7137be74b>:113: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  noise_v = autograd.Variable(noise, volatile=True)  # totally freeze model_gen\n",
            "2023-04-25 12:20:11,655 - g4gan - INFO - Epoch: 2/500\n",
            "INFO:g4gan:Epoch: 2/500\n",
            "2023-04-25 12:20:12,351 - g4gan - INFO - Epoch: 3/500\n",
            "INFO:g4gan:Epoch: 3/500\n",
            "2023-04-25 12:20:13,033 - g4gan - INFO - Epoch: 4/500\n",
            "INFO:g4gan:Epoch: 4/500\n",
            "2023-04-25 12:20:13,726 - g4gan - INFO - Epoch: 5/500\n",
            "INFO:g4gan:Epoch: 5/500\n",
            "2023-04-25 12:20:14,414 - g4gan - INFO - Epoch: 6/500\n",
            "INFO:g4gan:Epoch: 6/500\n",
            "2023-04-25 12:20:15,150 - g4gan - INFO - Epoch: 7/500\n",
            "INFO:g4gan:Epoch: 7/500\n",
            "2023-04-25 12:20:15,968 - g4gan - INFO - Epoch: 8/500\n",
            "INFO:g4gan:Epoch: 8/500\n",
            "2023-04-25 12:20:16,814 - g4gan - INFO - Epoch: 9/500\n",
            "INFO:g4gan:Epoch: 9/500\n",
            "2023-04-25 12:20:17,652 - g4gan - INFO - Epoch: 10/500\n",
            "INFO:g4gan:Epoch: 10/500\n",
            "2023-04-25 12:20:18,487 - g4gan - INFO - Epoch: 11/500\n",
            "INFO:g4gan:Epoch: 11/500\n",
            "2023-04-25 12:20:19,189 - g4gan - INFO - Epoch: 12/500\n",
            "INFO:g4gan:Epoch: 12/500\n",
            "2023-04-25 12:20:19,889 - g4gan - INFO - Epoch: 13/500\n",
            "INFO:g4gan:Epoch: 13/500\n",
            "2023-04-25 12:20:20,611 - g4gan - INFO - Epoch: 14/500\n",
            "INFO:g4gan:Epoch: 14/500\n",
            "2023-04-25 12:20:21,324 - g4gan - INFO - Epoch: 15/500\n",
            "INFO:g4gan:Epoch: 15/500\n",
            "2023-04-25 12:20:22,009 - g4gan - INFO - Epoch: 16/500\n",
            "INFO:g4gan:Epoch: 16/500\n",
            "2023-04-25 12:20:22,722 - g4gan - INFO - Epoch: 17/500\n",
            "INFO:g4gan:Epoch: 17/500\n",
            "2023-04-25 12:20:23,416 - g4gan - INFO - Epoch: 18/500\n",
            "INFO:g4gan:Epoch: 18/500\n",
            "2023-04-25 12:20:24,110 - g4gan - INFO - Epoch: 19/500\n",
            "INFO:g4gan:Epoch: 19/500\n",
            "2023-04-25 12:20:24,813 - g4gan - INFO - Epoch: 20/500\n",
            "INFO:g4gan:Epoch: 20/500\n",
            "2023-04-25 12:20:25,525 - g4gan - INFO - Epoch: 21/500\n",
            "INFO:g4gan:Epoch: 21/500\n",
            "2023-04-25 12:20:26,226 - g4gan - INFO - Epoch: 22/500\n",
            "INFO:g4gan:Epoch: 22/500\n",
            "2023-04-25 12:20:26,926 - g4gan - INFO - Epoch: 23/500\n",
            "INFO:g4gan:Epoch: 23/500\n",
            "2023-04-25 12:20:27,617 - g4gan - INFO - Epoch: 24/500\n",
            "INFO:g4gan:Epoch: 24/500\n",
            "2023-04-25 12:20:28,309 - g4gan - INFO - Epoch: 25/500\n",
            "INFO:g4gan:Epoch: 25/500\n",
            "2023-04-25 12:20:29,113 - g4gan - INFO - Epoch: 26/500\n",
            "INFO:g4gan:Epoch: 26/500\n",
            "2023-04-25 12:20:29,913 - g4gan - INFO - Epoch: 27/500\n",
            "INFO:g4gan:Epoch: 27/500\n",
            "2023-04-25 12:20:30,703 - g4gan - INFO - Epoch: 28/500\n",
            "INFO:g4gan:Epoch: 28/500\n",
            "2023-04-25 12:20:31,555 - g4gan - INFO - Epoch: 29/500\n",
            "INFO:g4gan:Epoch: 29/500\n",
            "2023-04-25 12:20:32,265 - g4gan - INFO - Epoch: 30/500\n",
            "INFO:g4gan:Epoch: 30/500\n",
            "2023-04-25 12:20:32,969 - g4gan - INFO - Epoch: 31/500\n",
            "INFO:g4gan:Epoch: 31/500\n",
            "2023-04-25 12:20:33,668 - g4gan - INFO - Epoch: 32/500\n",
            "INFO:g4gan:Epoch: 32/500\n",
            "2023-04-25 12:20:34,367 - g4gan - INFO - Epoch: 33/500\n",
            "INFO:g4gan:Epoch: 33/500\n",
            "2023-04-25 12:20:35,065 - g4gan - INFO - Epoch: 34/500\n",
            "INFO:g4gan:Epoch: 34/500\n",
            "2023-04-25 12:20:35,755 - g4gan - INFO - Epoch: 35/500\n",
            "INFO:g4gan:Epoch: 35/500\n",
            "2023-04-25 12:20:36,465 - g4gan - INFO - Epoch: 36/500\n",
            "INFO:g4gan:Epoch: 36/500\n",
            "2023-04-25 12:20:37,150 - g4gan - INFO - Epoch: 37/500\n",
            "INFO:g4gan:Epoch: 37/500\n",
            "2023-04-25 12:20:37,828 - g4gan - INFO - Epoch: 38/500\n",
            "INFO:g4gan:Epoch: 38/500\n",
            "2023-04-25 12:20:38,527 - g4gan - INFO - Epoch: 39/500\n",
            "INFO:g4gan:Epoch: 39/500\n",
            "2023-04-25 12:20:39,220 - g4gan - INFO - Epoch: 40/500\n",
            "INFO:g4gan:Epoch: 40/500\n",
            "2023-04-25 12:20:39,902 - g4gan - INFO - Epoch: 41/500\n",
            "INFO:g4gan:Epoch: 41/500\n",
            "2023-04-25 12:20:40,607 - g4gan - INFO - Epoch: 42/500\n",
            "INFO:g4gan:Epoch: 42/500\n",
            "2023-04-25 12:20:41,320 - g4gan - INFO - Epoch: 43/500\n",
            "INFO:g4gan:Epoch: 43/500\n",
            "2023-04-25 12:20:42,104 - g4gan - INFO - Epoch: 44/500\n",
            "INFO:g4gan:Epoch: 44/500\n",
            "2023-04-25 12:20:42,947 - g4gan - INFO - Epoch: 45/500\n",
            "INFO:g4gan:Epoch: 45/500\n",
            "2023-04-25 12:20:43,750 - g4gan - INFO - Epoch: 46/500\n",
            "INFO:g4gan:Epoch: 46/500\n",
            "2023-04-25 12:20:44,581 - g4gan - INFO - Epoch: 47/500\n",
            "INFO:g4gan:Epoch: 47/500\n",
            "2023-04-25 12:20:45,349 - g4gan - INFO - Epoch: 48/500\n",
            "INFO:g4gan:Epoch: 48/500\n",
            "2023-04-25 12:20:46,039 - g4gan - INFO - Epoch: 49/500\n",
            "INFO:g4gan:Epoch: 49/500\n",
            "2023-04-25 12:20:46,740 - g4gan - INFO - Epoch: 50/500\n",
            "INFO:g4gan:Epoch: 50/500\n",
            "2023-04-25 12:20:47,441 - g4gan - INFO - Epoch: 51/500\n",
            "INFO:g4gan:Epoch: 51/500\n",
            "2023-04-25 12:20:48,143 - g4gan - INFO - Epoch: 52/500\n",
            "INFO:g4gan:Epoch: 52/500\n",
            "2023-04-25 12:20:48,836 - g4gan - INFO - Epoch: 53/500\n",
            "INFO:g4gan:Epoch: 53/500\n",
            "2023-04-25 12:20:49,525 - g4gan - INFO - Epoch: 54/500\n",
            "INFO:g4gan:Epoch: 54/500\n",
            "2023-04-25 12:20:50,222 - g4gan - INFO - Epoch: 55/500\n",
            "INFO:g4gan:Epoch: 55/500\n",
            "2023-04-25 12:20:50,920 - g4gan - INFO - Epoch: 56/500\n",
            "INFO:g4gan:Epoch: 56/500\n",
            "2023-04-25 12:20:51,617 - g4gan - INFO - Epoch: 57/500\n",
            "INFO:g4gan:Epoch: 57/500\n",
            "2023-04-25 12:20:52,341 - g4gan - INFO - Epoch: 58/500\n",
            "INFO:g4gan:Epoch: 58/500\n",
            "2023-04-25 12:20:53,041 - g4gan - INFO - Epoch: 59/500\n",
            "INFO:g4gan:Epoch: 59/500\n",
            "2023-04-25 12:20:53,742 - g4gan - INFO - Epoch: 60/500\n",
            "INFO:g4gan:Epoch: 60/500\n",
            "2023-04-25 12:20:54,445 - g4gan - INFO - Epoch: 61/500\n",
            "INFO:g4gan:Epoch: 61/500\n",
            "2023-04-25 12:20:55,205 - g4gan - INFO - Epoch: 62/500\n",
            "INFO:g4gan:Epoch: 62/500\n",
            "2023-04-25 12:20:56,024 - g4gan - INFO - Epoch: 63/500\n",
            "INFO:g4gan:Epoch: 63/500\n",
            "2023-04-25 12:20:56,845 - g4gan - INFO - Epoch: 64/500\n",
            "INFO:g4gan:Epoch: 64/500\n",
            "2023-04-25 12:20:57,669 - g4gan - INFO - Epoch: 65/500\n",
            "INFO:g4gan:Epoch: 65/500\n",
            "2023-04-25 12:20:58,492 - g4gan - INFO - Epoch: 66/500\n",
            "INFO:g4gan:Epoch: 66/500\n",
            "2023-04-25 12:20:59,195 - g4gan - INFO - Epoch: 67/500\n",
            "INFO:g4gan:Epoch: 67/500\n",
            "2023-04-25 12:20:59,892 - g4gan - INFO - Epoch: 68/500\n",
            "INFO:g4gan:Epoch: 68/500\n",
            "2023-04-25 12:21:00,590 - g4gan - INFO - Epoch: 69/500\n",
            "INFO:g4gan:Epoch: 69/500\n",
            "2023-04-25 12:21:01,292 - g4gan - INFO - Epoch: 70/500\n",
            "INFO:g4gan:Epoch: 70/500\n",
            "2023-04-25 12:21:02,005 - g4gan - INFO - Epoch: 71/500\n",
            "INFO:g4gan:Epoch: 71/500\n",
            "2023-04-25 12:21:02,704 - g4gan - INFO - Epoch: 72/500\n",
            "INFO:g4gan:Epoch: 72/500\n",
            "2023-04-25 12:21:03,391 - g4gan - INFO - Epoch: 73/500\n",
            "INFO:g4gan:Epoch: 73/500\n",
            "2023-04-25 12:21:04,081 - g4gan - INFO - Epoch: 74/500\n",
            "INFO:g4gan:Epoch: 74/500\n",
            "2023-04-25 12:21:04,755 - g4gan - INFO - Epoch: 75/500\n",
            "INFO:g4gan:Epoch: 75/500\n",
            "2023-04-25 12:21:05,449 - g4gan - INFO - Epoch: 76/500\n",
            "INFO:g4gan:Epoch: 76/500\n",
            "2023-04-25 12:21:06,150 - g4gan - INFO - Epoch: 77/500\n",
            "INFO:g4gan:Epoch: 77/500\n",
            "2023-04-25 12:21:06,852 - g4gan - INFO - Epoch: 78/500\n",
            "INFO:g4gan:Epoch: 78/500\n",
            "2023-04-25 12:21:07,563 - g4gan - INFO - Epoch: 79/500\n",
            "INFO:g4gan:Epoch: 79/500\n",
            "2023-04-25 12:21:08,299 - g4gan - INFO - Epoch: 80/500\n",
            "INFO:g4gan:Epoch: 80/500\n",
            "2023-04-25 12:21:09,125 - g4gan - INFO - Epoch: 81/500\n",
            "INFO:g4gan:Epoch: 81/500\n",
            "2023-04-25 12:21:09,940 - g4gan - INFO - Epoch: 82/500\n",
            "INFO:g4gan:Epoch: 82/500\n",
            "2023-04-25 12:21:10,769 - g4gan - INFO - Epoch: 83/500\n",
            "INFO:g4gan:Epoch: 83/500\n",
            "2023-04-25 12:21:11,640 - g4gan - INFO - Epoch: 84/500\n",
            "INFO:g4gan:Epoch: 84/500\n",
            "2023-04-25 12:21:12,401 - g4gan - INFO - Epoch: 85/500\n",
            "INFO:g4gan:Epoch: 85/500\n",
            "2023-04-25 12:21:13,114 - g4gan - INFO - Epoch: 86/500\n",
            "INFO:g4gan:Epoch: 86/500\n",
            "2023-04-25 12:21:13,815 - g4gan - INFO - Epoch: 87/500\n",
            "INFO:g4gan:Epoch: 87/500\n",
            "2023-04-25 12:21:14,528 - g4gan - INFO - Epoch: 88/500\n",
            "INFO:g4gan:Epoch: 88/500\n",
            "2023-04-25 12:21:15,226 - g4gan - INFO - Epoch: 89/500\n",
            "INFO:g4gan:Epoch: 89/500\n",
            "2023-04-25 12:21:15,928 - g4gan - INFO - Epoch: 90/500\n",
            "INFO:g4gan:Epoch: 90/500\n",
            "2023-04-25 12:21:16,635 - g4gan - INFO - Epoch: 91/500\n",
            "INFO:g4gan:Epoch: 91/500\n",
            "2023-04-25 12:21:17,329 - g4gan - INFO - Epoch: 92/500\n",
            "INFO:g4gan:Epoch: 92/500\n",
            "2023-04-25 12:21:18,012 - g4gan - INFO - Epoch: 93/500\n",
            "INFO:g4gan:Epoch: 93/500\n",
            "2023-04-25 12:21:18,730 - g4gan - INFO - Epoch: 94/500\n",
            "INFO:g4gan:Epoch: 94/500\n",
            "2023-04-25 12:21:19,420 - g4gan - INFO - Epoch: 95/500\n",
            "INFO:g4gan:Epoch: 95/500\n",
            "2023-04-25 12:21:20,118 - g4gan - INFO - Epoch: 96/500\n",
            "INFO:g4gan:Epoch: 96/500\n",
            "2023-04-25 12:21:20,808 - g4gan - INFO - Epoch: 97/500\n",
            "INFO:g4gan:Epoch: 97/500\n",
            "2023-04-25 12:21:21,509 - g4gan - INFO - Epoch: 98/500\n",
            "INFO:g4gan:Epoch: 98/500\n",
            "2023-04-25 12:21:22,350 - g4gan - INFO - Epoch: 99/500\n",
            "INFO:g4gan:Epoch: 99/500\n",
            "2023-04-25 12:21:23,167 - g4gan - INFO - Epoch: 100/500\n",
            "INFO:g4gan:Epoch: 100/500\n",
            "2023-04-25 12:21:23,983 - g4gan - INFO - Epoch: 101/500\n",
            "INFO:g4gan:Epoch: 101/500\n",
            "2023-04-25 12:21:24,855 - g4gan - INFO - Epoch: 102/500\n",
            "INFO:g4gan:Epoch: 102/500\n",
            "2023-04-25 12:21:25,690 - g4gan - INFO - Epoch: 103/500\n",
            "INFO:g4gan:Epoch: 103/500\n",
            "2023-04-25 12:21:26,402 - g4gan - INFO - Epoch: 104/500\n",
            "INFO:g4gan:Epoch: 104/500\n",
            "2023-04-25 12:21:27,107 - g4gan - INFO - Epoch: 105/500\n",
            "INFO:g4gan:Epoch: 105/500\n",
            "2023-04-25 12:21:27,790 - g4gan - INFO - Epoch: 106/500\n",
            "INFO:g4gan:Epoch: 106/500\n",
            "2023-04-25 12:21:28,474 - g4gan - INFO - Epoch: 107/500\n",
            "INFO:g4gan:Epoch: 107/500\n",
            "2023-04-25 12:21:29,170 - g4gan - INFO - Epoch: 108/500\n",
            "INFO:g4gan:Epoch: 108/500\n",
            "2023-04-25 12:21:29,866 - g4gan - INFO - Epoch: 109/500\n",
            "INFO:g4gan:Epoch: 109/500\n",
            "2023-04-25 12:21:30,573 - g4gan - INFO - Epoch: 110/500\n",
            "INFO:g4gan:Epoch: 110/500\n",
            "2023-04-25 12:21:31,264 - g4gan - INFO - Epoch: 111/500\n",
            "INFO:g4gan:Epoch: 111/500\n",
            "2023-04-25 12:21:31,977 - g4gan - INFO - Epoch: 112/500\n",
            "INFO:g4gan:Epoch: 112/500\n",
            "2023-04-25 12:21:32,666 - g4gan - INFO - Epoch: 113/500\n",
            "INFO:g4gan:Epoch: 113/500\n",
            "2023-04-25 12:21:33,359 - g4gan - INFO - Epoch: 114/500\n",
            "INFO:g4gan:Epoch: 114/500\n",
            "2023-04-25 12:21:34,045 - g4gan - INFO - Epoch: 115/500\n",
            "INFO:g4gan:Epoch: 115/500\n",
            "2023-04-25 12:21:34,734 - g4gan - INFO - Epoch: 116/500\n",
            "INFO:g4gan:Epoch: 116/500\n",
            "2023-04-25 12:21:35,445 - g4gan - INFO - Epoch: 117/500\n",
            "INFO:g4gan:Epoch: 117/500\n",
            "2023-04-25 12:21:36,287 - g4gan - INFO - Epoch: 118/500\n",
            "INFO:g4gan:Epoch: 118/500\n",
            "2023-04-25 12:21:37,109 - g4gan - INFO - Epoch: 119/500\n",
            "INFO:g4gan:Epoch: 119/500\n",
            "2023-04-25 12:21:37,924 - g4gan - INFO - Epoch: 120/500\n",
            "INFO:g4gan:Epoch: 120/500\n",
            "2023-04-25 12:21:38,750 - g4gan - INFO - Epoch: 121/500\n",
            "INFO:g4gan:Epoch: 121/500\n",
            "2023-04-25 12:21:39,451 - g4gan - INFO - Epoch: 122/500\n",
            "INFO:g4gan:Epoch: 122/500\n",
            "2023-04-25 12:21:40,145 - g4gan - INFO - Epoch: 123/500\n",
            "INFO:g4gan:Epoch: 123/500\n",
            "2023-04-25 12:21:40,829 - g4gan - INFO - Epoch: 124/500\n",
            "INFO:g4gan:Epoch: 124/500\n",
            "2023-04-25 12:21:41,538 - g4gan - INFO - Epoch: 125/500\n",
            "INFO:g4gan:Epoch: 125/500\n",
            "2023-04-25 12:21:42,234 - g4gan - INFO - Epoch: 126/500\n",
            "INFO:g4gan:Epoch: 126/500\n",
            "2023-04-25 12:21:42,928 - g4gan - INFO - Epoch: 127/500\n",
            "INFO:g4gan:Epoch: 127/500\n",
            "2023-04-25 12:21:43,624 - g4gan - INFO - Epoch: 128/500\n",
            "INFO:g4gan:Epoch: 128/500\n",
            "2023-04-25 12:21:44,315 - g4gan - INFO - Epoch: 129/500\n",
            "INFO:g4gan:Epoch: 129/500\n",
            "2023-04-25 12:21:45,014 - g4gan - INFO - Epoch: 130/500\n",
            "INFO:g4gan:Epoch: 130/500\n",
            "2023-04-25 12:21:45,694 - g4gan - INFO - Epoch: 131/500\n",
            "INFO:g4gan:Epoch: 131/500\n",
            "2023-04-25 12:21:46,386 - g4gan - INFO - Epoch: 132/500\n",
            "INFO:g4gan:Epoch: 132/500\n",
            "2023-04-25 12:21:47,079 - g4gan - INFO - Epoch: 133/500\n",
            "INFO:g4gan:Epoch: 133/500\n",
            "2023-04-25 12:21:47,761 - g4gan - INFO - Epoch: 134/500\n",
            "INFO:g4gan:Epoch: 134/500\n",
            "2023-04-25 12:21:48,457 - g4gan - INFO - Epoch: 135/500\n",
            "INFO:g4gan:Epoch: 135/500\n",
            "2023-04-25 12:21:49,227 - g4gan - INFO - Epoch: 136/500\n",
            "INFO:g4gan:Epoch: 136/500\n",
            "2023-04-25 12:21:50,011 - g4gan - INFO - Epoch: 137/500\n",
            "INFO:g4gan:Epoch: 137/500\n",
            "2023-04-25 12:21:50,819 - g4gan - INFO - Epoch: 138/500\n",
            "INFO:g4gan:Epoch: 138/500\n",
            "2023-04-25 12:21:51,648 - g4gan - INFO - Epoch: 139/500\n",
            "INFO:g4gan:Epoch: 139/500\n",
            "2023-04-25 12:21:52,405 - g4gan - INFO - Epoch: 140/500\n",
            "INFO:g4gan:Epoch: 140/500\n",
            "2023-04-25 12:21:53,115 - g4gan - INFO - Epoch: 141/500\n",
            "INFO:g4gan:Epoch: 141/500\n",
            "2023-04-25 12:21:53,817 - g4gan - INFO - Epoch: 142/500\n",
            "INFO:g4gan:Epoch: 142/500\n",
            "2023-04-25 12:21:54,531 - g4gan - INFO - Epoch: 143/500\n",
            "INFO:g4gan:Epoch: 143/500\n",
            "2023-04-25 12:21:55,249 - g4gan - INFO - Epoch: 144/500\n",
            "INFO:g4gan:Epoch: 144/500\n",
            "2023-04-25 12:21:55,937 - g4gan - INFO - Epoch: 145/500\n",
            "INFO:g4gan:Epoch: 145/500\n",
            "2023-04-25 12:21:56,644 - g4gan - INFO - Epoch: 146/500\n",
            "INFO:g4gan:Epoch: 146/500\n",
            "2023-04-25 12:21:57,339 - g4gan - INFO - Epoch: 147/500\n",
            "INFO:g4gan:Epoch: 147/500\n",
            "2023-04-25 12:21:58,022 - g4gan - INFO - Epoch: 148/500\n",
            "INFO:g4gan:Epoch: 148/500\n",
            "2023-04-25 12:21:58,702 - g4gan - INFO - Epoch: 149/500\n",
            "INFO:g4gan:Epoch: 149/500\n",
            "2023-04-25 12:21:59,389 - g4gan - INFO - Epoch: 150/500\n",
            "INFO:g4gan:Epoch: 150/500\n",
            "2023-04-25 12:22:00,070 - g4gan - INFO - Epoch: 151/500\n",
            "INFO:g4gan:Epoch: 151/500\n",
            "2023-04-25 12:22:00,759 - g4gan - INFO - Epoch: 152/500\n",
            "INFO:g4gan:Epoch: 152/500\n",
            "2023-04-25 12:22:01,455 - g4gan - INFO - Epoch: 153/500\n",
            "INFO:g4gan:Epoch: 153/500\n",
            "2023-04-25 12:22:02,171 - g4gan - INFO - Epoch: 154/500\n",
            "INFO:g4gan:Epoch: 154/500\n",
            "2023-04-25 12:22:02,983 - g4gan - INFO - Epoch: 155/500\n",
            "INFO:g4gan:Epoch: 155/500\n",
            "2023-04-25 12:22:03,773 - g4gan - INFO - Epoch: 156/500\n",
            "INFO:g4gan:Epoch: 156/500\n",
            "2023-04-25 12:22:04,584 - g4gan - INFO - Epoch: 157/500\n",
            "INFO:g4gan:Epoch: 157/500\n",
            "2023-04-25 12:22:05,428 - g4gan - INFO - Epoch: 158/500\n",
            "INFO:g4gan:Epoch: 158/500\n",
            "2023-04-25 12:22:06,131 - g4gan - INFO - Epoch: 159/500\n",
            "INFO:g4gan:Epoch: 159/500\n",
            "2023-04-25 12:22:06,839 - g4gan - INFO - Epoch: 160/500\n",
            "INFO:g4gan:Epoch: 160/500\n",
            "2023-04-25 12:22:07,545 - g4gan - INFO - Epoch: 161/500\n",
            "INFO:g4gan:Epoch: 161/500\n",
            "2023-04-25 12:22:08,273 - g4gan - INFO - Epoch: 162/500\n",
            "INFO:g4gan:Epoch: 162/500\n",
            "2023-04-25 12:22:08,998 - g4gan - INFO - Epoch: 163/500\n",
            "INFO:g4gan:Epoch: 163/500\n",
            "2023-04-25 12:22:09,702 - g4gan - INFO - Epoch: 164/500\n",
            "INFO:g4gan:Epoch: 164/500\n",
            "2023-04-25 12:22:10,394 - g4gan - INFO - Epoch: 165/500\n",
            "INFO:g4gan:Epoch: 165/500\n",
            "2023-04-25 12:22:11,105 - g4gan - INFO - Epoch: 166/500\n",
            "INFO:g4gan:Epoch: 166/500\n",
            "2023-04-25 12:22:11,808 - g4gan - INFO - Epoch: 167/500\n",
            "INFO:g4gan:Epoch: 167/500\n",
            "2023-04-25 12:22:12,503 - g4gan - INFO - Epoch: 168/500\n",
            "INFO:g4gan:Epoch: 168/500\n",
            "2023-04-25 12:22:13,194 - g4gan - INFO - Epoch: 169/500\n",
            "INFO:g4gan:Epoch: 169/500\n",
            "2023-04-25 12:22:13,877 - g4gan - INFO - Epoch: 170/500\n",
            "INFO:g4gan:Epoch: 170/500\n",
            "2023-04-25 12:22:14,569 - g4gan - INFO - Epoch: 171/500\n",
            "INFO:g4gan:Epoch: 171/500\n",
            "2023-04-25 12:22:15,260 - g4gan - INFO - Epoch: 172/500\n",
            "INFO:g4gan:Epoch: 172/500\n",
            "2023-04-25 12:22:16,075 - g4gan - INFO - Epoch: 173/500\n",
            "INFO:g4gan:Epoch: 173/500\n",
            "2023-04-25 12:22:16,889 - g4gan - INFO - Epoch: 174/500\n",
            "INFO:g4gan:Epoch: 174/500\n",
            "2023-04-25 12:22:17,705 - g4gan - INFO - Epoch: 175/500\n",
            "INFO:g4gan:Epoch: 175/500\n",
            "2023-04-25 12:22:18,546 - g4gan - INFO - Epoch: 176/500\n",
            "INFO:g4gan:Epoch: 176/500\n",
            "2023-04-25 12:22:19,281 - g4gan - INFO - Epoch: 177/500\n",
            "INFO:g4gan:Epoch: 177/500\n",
            "2023-04-25 12:22:19,975 - g4gan - INFO - Epoch: 178/500\n",
            "INFO:g4gan:Epoch: 178/500\n",
            "2023-04-25 12:22:20,682 - g4gan - INFO - Epoch: 179/500\n",
            "INFO:g4gan:Epoch: 179/500\n",
            "2023-04-25 12:22:21,386 - g4gan - INFO - Epoch: 180/500\n",
            "INFO:g4gan:Epoch: 180/500\n",
            "2023-04-25 12:22:22,102 - g4gan - INFO - Epoch: 181/500\n",
            "INFO:g4gan:Epoch: 181/500\n",
            "2023-04-25 12:22:22,799 - g4gan - INFO - Epoch: 182/500\n",
            "INFO:g4gan:Epoch: 182/500\n",
            "2023-04-25 12:22:23,503 - g4gan - INFO - Epoch: 183/500\n",
            "INFO:g4gan:Epoch: 183/500\n",
            "2023-04-25 12:22:24,198 - g4gan - INFO - Epoch: 184/500\n",
            "INFO:g4gan:Epoch: 184/500\n",
            "2023-04-25 12:22:24,892 - g4gan - INFO - Epoch: 185/500\n",
            "INFO:g4gan:Epoch: 185/500\n",
            "2023-04-25 12:22:25,602 - g4gan - INFO - Epoch: 186/500\n",
            "INFO:g4gan:Epoch: 186/500\n",
            "2023-04-25 12:22:26,342 - g4gan - INFO - Epoch: 187/500\n",
            "INFO:g4gan:Epoch: 187/500\n",
            "2023-04-25 12:22:27,071 - g4gan - INFO - Epoch: 188/500\n",
            "INFO:g4gan:Epoch: 188/500\n",
            "2023-04-25 12:22:27,782 - g4gan - INFO - Epoch: 189/500\n",
            "INFO:g4gan:Epoch: 189/500\n",
            "2023-04-25 12:22:28,501 - g4gan - INFO - Epoch: 190/500\n",
            "INFO:g4gan:Epoch: 190/500\n",
            "2023-04-25 12:22:29,326 - g4gan - INFO - Epoch: 191/500\n",
            "INFO:g4gan:Epoch: 191/500\n",
            "2023-04-25 12:22:30,134 - g4gan - INFO - Epoch: 192/500\n",
            "INFO:g4gan:Epoch: 192/500\n",
            "2023-04-25 12:22:30,951 - g4gan - INFO - Epoch: 193/500\n",
            "INFO:g4gan:Epoch: 193/500\n",
            "2023-04-25 12:22:31,666 - g4gan - INFO - Epoch: 194/500\n",
            "INFO:g4gan:Epoch: 194/500\n",
            "2023-04-25 12:22:32,376 - g4gan - INFO - Epoch: 195/500\n",
            "INFO:g4gan:Epoch: 195/500\n",
            "2023-04-25 12:22:33,077 - g4gan - INFO - Epoch: 196/500\n",
            "INFO:g4gan:Epoch: 196/500\n",
            "2023-04-25 12:22:33,781 - g4gan - INFO - Epoch: 197/500\n",
            "INFO:g4gan:Epoch: 197/500\n",
            "2023-04-25 12:22:34,480 - g4gan - INFO - Epoch: 198/500\n",
            "INFO:g4gan:Epoch: 198/500\n",
            "2023-04-25 12:22:35,188 - g4gan - INFO - Epoch: 199/500\n",
            "INFO:g4gan:Epoch: 199/500\n",
            "2023-04-25 12:22:35,890 - g4gan - INFO - Epoch: 200/500\n",
            "INFO:g4gan:Epoch: 200/500\n",
            "2023-04-25 12:22:36,591 - g4gan - INFO - Epoch: 201/500\n",
            "INFO:g4gan:Epoch: 201/500\n",
            "2023-04-25 12:22:37,295 - g4gan - INFO - Epoch: 202/500\n",
            "INFO:g4gan:Epoch: 202/500\n",
            "2023-04-25 12:22:38,008 - g4gan - INFO - Epoch: 203/500\n",
            "INFO:g4gan:Epoch: 203/500\n",
            "2023-04-25 12:22:38,719 - g4gan - INFO - Epoch: 204/500\n",
            "INFO:g4gan:Epoch: 204/500\n",
            "2023-04-25 12:22:39,430 - g4gan - INFO - Epoch: 205/500\n",
            "INFO:g4gan:Epoch: 205/500\n",
            "2023-04-25 12:22:40,134 - g4gan - INFO - Epoch: 206/500\n",
            "INFO:g4gan:Epoch: 206/500\n",
            "2023-04-25 12:22:40,837 - g4gan - INFO - Epoch: 207/500\n",
            "INFO:g4gan:Epoch: 207/500\n",
            "2023-04-25 12:22:41,670 - g4gan - INFO - Epoch: 208/500\n",
            "INFO:g4gan:Epoch: 208/500\n",
            "2023-04-25 12:22:42,518 - g4gan - INFO - Epoch: 209/500\n",
            "INFO:g4gan:Epoch: 209/500\n",
            "2023-04-25 12:22:43,271 - g4gan - INFO - Epoch: 210/500\n",
            "INFO:g4gan:Epoch: 210/500\n",
            "2023-04-25 12:22:43,982 - g4gan - INFO - Epoch: 211/500\n",
            "INFO:g4gan:Epoch: 211/500\n",
            "2023-04-25 12:22:44,688 - g4gan - INFO - Epoch: 212/500\n",
            "INFO:g4gan:Epoch: 212/500\n",
            "2023-04-25 12:22:45,402 - g4gan - INFO - Epoch: 213/500\n",
            "INFO:g4gan:Epoch: 213/500\n",
            "2023-04-25 12:22:46,107 - g4gan - INFO - Epoch: 214/500\n",
            "INFO:g4gan:Epoch: 214/500\n",
            "2023-04-25 12:22:46,802 - g4gan - INFO - Epoch: 215/500\n",
            "INFO:g4gan:Epoch: 215/500\n",
            "2023-04-25 12:22:47,506 - g4gan - INFO - Epoch: 216/500\n",
            "INFO:g4gan:Epoch: 216/500\n",
            "2023-04-25 12:22:48,217 - g4gan - INFO - Epoch: 217/500\n",
            "INFO:g4gan:Epoch: 217/500\n",
            "2023-04-25 12:22:48,914 - g4gan - INFO - Epoch: 218/500\n",
            "INFO:g4gan:Epoch: 218/500\n",
            "2023-04-25 12:22:49,610 - g4gan - INFO - Epoch: 219/500\n",
            "INFO:g4gan:Epoch: 219/500\n",
            "2023-04-25 12:22:50,301 - g4gan - INFO - Epoch: 220/500\n",
            "INFO:g4gan:Epoch: 220/500\n",
            "2023-04-25 12:22:50,999 - g4gan - INFO - Epoch: 221/500\n",
            "INFO:g4gan:Epoch: 221/500\n",
            "2023-04-25 12:22:51,723 - g4gan - INFO - Epoch: 222/500\n",
            "INFO:g4gan:Epoch: 222/500\n",
            "2023-04-25 12:22:52,430 - g4gan - INFO - Epoch: 223/500\n",
            "INFO:g4gan:Epoch: 223/500\n",
            "2023-04-25 12:22:53,226 - g4gan - INFO - Epoch: 224/500\n",
            "INFO:g4gan:Epoch: 224/500\n",
            "2023-04-25 12:22:54,057 - g4gan - INFO - Epoch: 225/500\n",
            "INFO:g4gan:Epoch: 225/500\n",
            "2023-04-25 12:22:54,900 - g4gan - INFO - Epoch: 226/500\n",
            "INFO:g4gan:Epoch: 226/500\n",
            "2023-04-25 12:22:55,598 - g4gan - INFO - Epoch: 227/500\n",
            "INFO:g4gan:Epoch: 227/500\n",
            "2023-04-25 12:22:56,293 - g4gan - INFO - Epoch: 228/500\n",
            "INFO:g4gan:Epoch: 228/500\n",
            "2023-04-25 12:22:56,995 - g4gan - INFO - Epoch: 229/500\n",
            "INFO:g4gan:Epoch: 229/500\n",
            "2023-04-25 12:22:57,715 - g4gan - INFO - Epoch: 230/500\n",
            "INFO:g4gan:Epoch: 230/500\n",
            "2023-04-25 12:22:58,427 - g4gan - INFO - Epoch: 231/500\n",
            "INFO:g4gan:Epoch: 231/500\n",
            "2023-04-25 12:22:59,130 - g4gan - INFO - Epoch: 232/500\n",
            "INFO:g4gan:Epoch: 232/500\n",
            "2023-04-25 12:22:59,834 - g4gan - INFO - Epoch: 233/500\n",
            "INFO:g4gan:Epoch: 233/500\n",
            "2023-04-25 12:23:00,534 - g4gan - INFO - Epoch: 234/500\n",
            "INFO:g4gan:Epoch: 234/500\n",
            "2023-04-25 12:23:01,232 - g4gan - INFO - Epoch: 235/500\n",
            "INFO:g4gan:Epoch: 235/500\n",
            "2023-04-25 12:23:01,944 - g4gan - INFO - Epoch: 236/500\n",
            "INFO:g4gan:Epoch: 236/500\n",
            "2023-04-25 12:23:02,636 - g4gan - INFO - Epoch: 237/500\n",
            "INFO:g4gan:Epoch: 237/500\n",
            "2023-04-25 12:23:03,337 - g4gan - INFO - Epoch: 238/500\n",
            "INFO:g4gan:Epoch: 238/500\n",
            "2023-04-25 12:23:04,037 - g4gan - INFO - Epoch: 239/500\n",
            "INFO:g4gan:Epoch: 239/500\n",
            "2023-04-25 12:23:04,726 - g4gan - INFO - Epoch: 240/500\n",
            "INFO:g4gan:Epoch: 240/500\n",
            "2023-04-25 12:23:05,537 - g4gan - INFO - Epoch: 241/500\n",
            "INFO:g4gan:Epoch: 241/500\n",
            "2023-04-25 12:23:06,362 - g4gan - INFO - Epoch: 242/500\n",
            "INFO:g4gan:Epoch: 242/500\n",
            "2023-04-25 12:23:07,147 - g4gan - INFO - Epoch: 243/500\n",
            "INFO:g4gan:Epoch: 243/500\n",
            "2023-04-25 12:23:07,831 - g4gan - INFO - Epoch: 244/500\n",
            "INFO:g4gan:Epoch: 244/500\n",
            "2023-04-25 12:23:08,565 - g4gan - INFO - Epoch: 245/500\n",
            "INFO:g4gan:Epoch: 245/500\n",
            "2023-04-25 12:23:09,272 - g4gan - INFO - Epoch: 246/500\n",
            "INFO:g4gan:Epoch: 246/500\n",
            "2023-04-25 12:23:09,973 - g4gan - INFO - Epoch: 247/500\n",
            "INFO:g4gan:Epoch: 247/500\n",
            "2023-04-25 12:23:10,674 - g4gan - INFO - Epoch: 248/500\n",
            "INFO:g4gan:Epoch: 248/500\n",
            "2023-04-25 12:23:11,378 - g4gan - INFO - Epoch: 249/500\n",
            "INFO:g4gan:Epoch: 249/500\n",
            "2023-04-25 12:23:12,077 - g4gan - INFO - Epoch: 250/500\n",
            "INFO:g4gan:Epoch: 250/500\n",
            "2023-04-25 12:23:12,789 - g4gan - INFO - Epoch: 251/500\n",
            "INFO:g4gan:Epoch: 251/500\n",
            "2023-04-25 12:23:13,492 - g4gan - INFO - Epoch: 252/500\n",
            "INFO:g4gan:Epoch: 252/500\n",
            "2023-04-25 12:23:14,181 - g4gan - INFO - Epoch: 253/500\n",
            "INFO:g4gan:Epoch: 253/500\n",
            "2023-04-25 12:23:14,885 - g4gan - INFO - Epoch: 254/500\n",
            "INFO:g4gan:Epoch: 254/500\n",
            "2023-04-25 12:23:15,590 - g4gan - INFO - Epoch: 255/500\n",
            "INFO:g4gan:Epoch: 255/500\n",
            "2023-04-25 12:23:16,296 - g4gan - INFO - Epoch: 256/500\n",
            "INFO:g4gan:Epoch: 256/500\n",
            "2023-04-25 12:23:17,039 - g4gan - INFO - Epoch: 257/500\n",
            "INFO:g4gan:Epoch: 257/500\n",
            "2023-04-25 12:23:17,865 - g4gan - INFO - Epoch: 258/500\n",
            "INFO:g4gan:Epoch: 258/500\n",
            "2023-04-25 12:23:18,769 - g4gan - INFO - Epoch: 259/500\n",
            "INFO:g4gan:Epoch: 259/500\n",
            "2023-04-25 12:23:19,524 - g4gan - INFO - Epoch: 260/500\n",
            "INFO:g4gan:Epoch: 260/500\n",
            "2023-04-25 12:23:20,226 - g4gan - INFO - Epoch: 261/500\n",
            "INFO:g4gan:Epoch: 261/500\n",
            "2023-04-25 12:23:20,924 - g4gan - INFO - Epoch: 262/500\n",
            "INFO:g4gan:Epoch: 262/500\n",
            "2023-04-25 12:23:21,621 - g4gan - INFO - Epoch: 263/500\n",
            "INFO:g4gan:Epoch: 263/500\n",
            "2023-04-25 12:23:22,321 - g4gan - INFO - Epoch: 264/500\n",
            "INFO:g4gan:Epoch: 264/500\n",
            "2023-04-25 12:23:23,031 - g4gan - INFO - Epoch: 265/500\n",
            "INFO:g4gan:Epoch: 265/500\n",
            "2023-04-25 12:23:23,735 - g4gan - INFO - Epoch: 266/500\n",
            "INFO:g4gan:Epoch: 266/500\n",
            "2023-04-25 12:23:24,475 - g4gan - INFO - Epoch: 267/500\n",
            "INFO:g4gan:Epoch: 267/500\n",
            "2023-04-25 12:23:25,171 - g4gan - INFO - Epoch: 268/500\n",
            "INFO:g4gan:Epoch: 268/500\n",
            "2023-04-25 12:23:25,878 - g4gan - INFO - Epoch: 269/500\n",
            "INFO:g4gan:Epoch: 269/500\n",
            "2023-04-25 12:23:26,583 - g4gan - INFO - Epoch: 270/500\n",
            "INFO:g4gan:Epoch: 270/500\n",
            "2023-04-25 12:23:27,292 - g4gan - INFO - Epoch: 271/500\n",
            "INFO:g4gan:Epoch: 271/500\n",
            "2023-04-25 12:23:27,989 - g4gan - INFO - Epoch: 272/500\n",
            "INFO:g4gan:Epoch: 272/500\n",
            "2023-04-25 12:23:28,696 - g4gan - INFO - Epoch: 273/500\n",
            "INFO:g4gan:Epoch: 273/500\n",
            "2023-04-25 12:23:29,480 - g4gan - INFO - Epoch: 274/500\n",
            "INFO:g4gan:Epoch: 274/500\n",
            "2023-04-25 12:23:30,295 - g4gan - INFO - Epoch: 275/500\n",
            "INFO:g4gan:Epoch: 275/500\n",
            "2023-04-25 12:23:31,119 - g4gan - INFO - Epoch: 276/500\n",
            "INFO:g4gan:Epoch: 276/500\n",
            "2023-04-25 12:23:31,843 - g4gan - INFO - Epoch: 277/500\n",
            "INFO:g4gan:Epoch: 277/500\n",
            "2023-04-25 12:23:32,558 - g4gan - INFO - Epoch: 278/500\n",
            "INFO:g4gan:Epoch: 278/500\n",
            "2023-04-25 12:23:33,273 - g4gan - INFO - Epoch: 279/500\n",
            "INFO:g4gan:Epoch: 279/500\n",
            "2023-04-25 12:23:33,965 - g4gan - INFO - Epoch: 280/500\n",
            "INFO:g4gan:Epoch: 280/500\n",
            "2023-04-25 12:23:34,665 - g4gan - INFO - Epoch: 281/500\n",
            "INFO:g4gan:Epoch: 281/500\n",
            "2023-04-25 12:23:35,388 - g4gan - INFO - Epoch: 282/500\n",
            "INFO:g4gan:Epoch: 282/500\n",
            "2023-04-25 12:23:36,102 - g4gan - INFO - Epoch: 283/500\n",
            "INFO:g4gan:Epoch: 283/500\n",
            "2023-04-25 12:23:36,800 - g4gan - INFO - Epoch: 284/500\n",
            "INFO:g4gan:Epoch: 284/500\n",
            "2023-04-25 12:23:37,509 - g4gan - INFO - Epoch: 285/500\n",
            "INFO:g4gan:Epoch: 285/500\n",
            "2023-04-25 12:23:38,215 - g4gan - INFO - Epoch: 286/500\n",
            "INFO:g4gan:Epoch: 286/500\n",
            "2023-04-25 12:23:38,921 - g4gan - INFO - Epoch: 287/500\n",
            "INFO:g4gan:Epoch: 287/500\n",
            "2023-04-25 12:23:39,627 - g4gan - INFO - Epoch: 288/500\n",
            "INFO:g4gan:Epoch: 288/500\n",
            "2023-04-25 12:23:40,326 - g4gan - INFO - Epoch: 289/500\n",
            "INFO:g4gan:Epoch: 289/500\n",
            "2023-04-25 12:23:41,025 - g4gan - INFO - Epoch: 290/500\n",
            "INFO:g4gan:Epoch: 290/500\n",
            "2023-04-25 12:23:41,848 - g4gan - INFO - Epoch: 291/500\n",
            "INFO:g4gan:Epoch: 291/500\n",
            "2023-04-25 12:23:42,679 - g4gan - INFO - Epoch: 292/500\n",
            "INFO:g4gan:Epoch: 292/500\n",
            "2023-04-25 12:23:43,449 - g4gan - INFO - Epoch: 293/500\n",
            "INFO:g4gan:Epoch: 293/500\n",
            "2023-04-25 12:23:44,141 - g4gan - INFO - Epoch: 294/500\n",
            "INFO:g4gan:Epoch: 294/500\n",
            "2023-04-25 12:23:44,841 - g4gan - INFO - Epoch: 295/500\n",
            "INFO:g4gan:Epoch: 295/500\n",
            "2023-04-25 12:23:45,542 - g4gan - INFO - Epoch: 296/500\n",
            "INFO:g4gan:Epoch: 296/500\n",
            "2023-04-25 12:23:46,242 - g4gan - INFO - Epoch: 297/500\n",
            "INFO:g4gan:Epoch: 297/500\n",
            "2023-04-25 12:23:46,938 - g4gan - INFO - Epoch: 298/500\n",
            "INFO:g4gan:Epoch: 298/500\n",
            "2023-04-25 12:23:47,643 - g4gan - INFO - Epoch: 299/500\n",
            "INFO:g4gan:Epoch: 299/500\n",
            "2023-04-25 12:23:48,362 - g4gan - INFO - Epoch: 300/500\n",
            "INFO:g4gan:Epoch: 300/500\n",
            "2023-04-25 12:23:49,070 - g4gan - INFO - Epoch: 301/500\n",
            "INFO:g4gan:Epoch: 301/500\n",
            "2023-04-25 12:23:49,770 - g4gan - INFO - Epoch: 302/500\n",
            "INFO:g4gan:Epoch: 302/500\n",
            "2023-04-25 12:23:50,485 - g4gan - INFO - Epoch: 303/500\n",
            "INFO:g4gan:Epoch: 303/500\n",
            "2023-04-25 12:23:51,196 - g4gan - INFO - Epoch: 304/500\n",
            "INFO:g4gan:Epoch: 304/500\n",
            "2023-04-25 12:23:51,899 - g4gan - INFO - Epoch: 305/500\n",
            "INFO:g4gan:Epoch: 305/500\n",
            "2023-04-25 12:23:52,602 - g4gan - INFO - Epoch: 306/500\n",
            "INFO:g4gan:Epoch: 306/500\n",
            "2023-04-25 12:23:53,386 - g4gan - INFO - Epoch: 307/500\n",
            "INFO:g4gan:Epoch: 307/500\n",
            "2023-04-25 12:23:54,218 - g4gan - INFO - Epoch: 308/500\n",
            "INFO:g4gan:Epoch: 308/500\n",
            "2023-04-25 12:23:55,088 - g4gan - INFO - Epoch: 309/500\n",
            "INFO:g4gan:Epoch: 309/500\n",
            "2023-04-25 12:23:55,806 - g4gan - INFO - Epoch: 310/500\n",
            "INFO:g4gan:Epoch: 310/500\n",
            "2023-04-25 12:23:56,508 - g4gan - INFO - Epoch: 311/500\n",
            "INFO:g4gan:Epoch: 311/500\n",
            "2023-04-25 12:23:57,213 - g4gan - INFO - Epoch: 312/500\n",
            "INFO:g4gan:Epoch: 312/500\n",
            "2023-04-25 12:23:57,910 - g4gan - INFO - Epoch: 313/500\n",
            "INFO:g4gan:Epoch: 313/500\n",
            "2023-04-25 12:23:58,607 - g4gan - INFO - Epoch: 314/500\n",
            "INFO:g4gan:Epoch: 314/500\n",
            "2023-04-25 12:23:59,306 - g4gan - INFO - Epoch: 315/500\n",
            "INFO:g4gan:Epoch: 315/500\n",
            "2023-04-25 12:24:00,009 - g4gan - INFO - Epoch: 316/500\n",
            "INFO:g4gan:Epoch: 316/500\n",
            "2023-04-25 12:24:00,702 - g4gan - INFO - Epoch: 317/500\n",
            "INFO:g4gan:Epoch: 317/500\n",
            "2023-04-25 12:24:01,392 - g4gan - INFO - Epoch: 318/500\n",
            "INFO:g4gan:Epoch: 318/500\n",
            "2023-04-25 12:24:02,089 - g4gan - INFO - Epoch: 319/500\n",
            "INFO:g4gan:Epoch: 319/500\n",
            "2023-04-25 12:24:02,798 - g4gan - INFO - Epoch: 320/500\n",
            "INFO:g4gan:Epoch: 320/500\n",
            "2023-04-25 12:24:03,504 - g4gan - INFO - Epoch: 321/500\n",
            "INFO:g4gan:Epoch: 321/500\n",
            "2023-04-25 12:24:04,198 - g4gan - INFO - Epoch: 322/500\n",
            "INFO:g4gan:Epoch: 322/500\n",
            "2023-04-25 12:24:04,892 - g4gan - INFO - Epoch: 323/500\n",
            "INFO:g4gan:Epoch: 323/500\n",
            "2023-04-25 12:24:05,695 - g4gan - INFO - Epoch: 324/500\n",
            "INFO:g4gan:Epoch: 324/500\n",
            "2023-04-25 12:24:06,509 - g4gan - INFO - Epoch: 325/500\n",
            "INFO:g4gan:Epoch: 325/500\n",
            "2023-04-25 12:24:07,345 - g4gan - INFO - Epoch: 326/500\n",
            "INFO:g4gan:Epoch: 326/500\n",
            "2023-04-25 12:24:08,049 - g4gan - INFO - Epoch: 327/500\n",
            "INFO:g4gan:Epoch: 327/500\n",
            "2023-04-25 12:24:08,777 - g4gan - INFO - Epoch: 328/500\n",
            "INFO:g4gan:Epoch: 328/500\n",
            "2023-04-25 12:24:09,495 - g4gan - INFO - Epoch: 329/500\n",
            "INFO:g4gan:Epoch: 329/500\n",
            "2023-04-25 12:24:10,212 - g4gan - INFO - Epoch: 330/500\n",
            "INFO:g4gan:Epoch: 330/500\n",
            "2023-04-25 12:24:10,910 - g4gan - INFO - Epoch: 331/500\n",
            "INFO:g4gan:Epoch: 331/500\n",
            "2023-04-25 12:24:11,613 - g4gan - INFO - Epoch: 332/500\n",
            "INFO:g4gan:Epoch: 332/500\n",
            "2023-04-25 12:24:12,324 - g4gan - INFO - Epoch: 333/500\n",
            "INFO:g4gan:Epoch: 333/500\n",
            "2023-04-25 12:24:13,029 - g4gan - INFO - Epoch: 334/500\n",
            "INFO:g4gan:Epoch: 334/500\n",
            "2023-04-25 12:24:13,725 - g4gan - INFO - Epoch: 335/500\n",
            "INFO:g4gan:Epoch: 335/500\n",
            "2023-04-25 12:24:14,417 - g4gan - INFO - Epoch: 336/500\n",
            "INFO:g4gan:Epoch: 336/500\n",
            "2023-04-25 12:24:15,109 - g4gan - INFO - Epoch: 337/500\n",
            "INFO:g4gan:Epoch: 337/500\n",
            "2023-04-25 12:24:15,802 - g4gan - INFO - Epoch: 338/500\n",
            "INFO:g4gan:Epoch: 338/500\n",
            "2023-04-25 12:24:16,501 - g4gan - INFO - Epoch: 339/500\n",
            "INFO:g4gan:Epoch: 339/500\n",
            "2023-04-25 12:24:17,202 - g4gan - INFO - Epoch: 340/500\n",
            "INFO:g4gan:Epoch: 340/500\n",
            "2023-04-25 12:24:18,025 - g4gan - INFO - Epoch: 341/500\n",
            "INFO:g4gan:Epoch: 341/500\n",
            "2023-04-25 12:24:18,845 - g4gan - INFO - Epoch: 342/500\n",
            "INFO:g4gan:Epoch: 342/500\n",
            "2023-04-25 12:24:19,611 - g4gan - INFO - Epoch: 343/500\n",
            "INFO:g4gan:Epoch: 343/500\n",
            "2023-04-25 12:24:20,326 - g4gan - INFO - Epoch: 344/500\n",
            "INFO:g4gan:Epoch: 344/500\n",
            "2023-04-25 12:24:21,027 - g4gan - INFO - Epoch: 345/500\n",
            "INFO:g4gan:Epoch: 345/500\n",
            "2023-04-25 12:24:21,746 - g4gan - INFO - Epoch: 346/500\n",
            "INFO:g4gan:Epoch: 346/500\n",
            "2023-04-25 12:24:22,447 - g4gan - INFO - Epoch: 347/500\n",
            "INFO:g4gan:Epoch: 347/500\n",
            "2023-04-25 12:24:23,153 - g4gan - INFO - Epoch: 348/500\n",
            "INFO:g4gan:Epoch: 348/500\n",
            "2023-04-25 12:24:23,849 - g4gan - INFO - Epoch: 349/500\n",
            "INFO:g4gan:Epoch: 349/500\n",
            "2023-04-25 12:24:24,546 - g4gan - INFO - Epoch: 350/500\n",
            "INFO:g4gan:Epoch: 350/500\n",
            "2023-04-25 12:24:25,270 - g4gan - INFO - Epoch: 351/500\n",
            "INFO:g4gan:Epoch: 351/500\n",
            "2023-04-25 12:24:25,962 - g4gan - INFO - Epoch: 352/500\n",
            "INFO:g4gan:Epoch: 352/500\n",
            "2023-04-25 12:24:26,660 - g4gan - INFO - Epoch: 353/500\n",
            "INFO:g4gan:Epoch: 353/500\n",
            "2023-04-25 12:24:27,373 - g4gan - INFO - Epoch: 354/500\n",
            "INFO:g4gan:Epoch: 354/500\n",
            "2023-04-25 12:24:28,074 - g4gan - INFO - Epoch: 355/500\n",
            "INFO:g4gan:Epoch: 355/500\n",
            "2023-04-25 12:24:28,759 - g4gan - INFO - Epoch: 356/500\n",
            "INFO:g4gan:Epoch: 356/500\n",
            "2023-04-25 12:24:29,509 - g4gan - INFO - Epoch: 357/500\n",
            "INFO:g4gan:Epoch: 357/500\n",
            "2023-04-25 12:24:30,317 - g4gan - INFO - Epoch: 358/500\n",
            "INFO:g4gan:Epoch: 358/500\n",
            "2023-04-25 12:24:31,151 - g4gan - INFO - Epoch: 359/500\n",
            "INFO:g4gan:Epoch: 359/500\n",
            "2023-04-25 12:24:31,854 - g4gan - INFO - Epoch: 360/500\n",
            "INFO:g4gan:Epoch: 360/500\n",
            "2023-04-25 12:24:32,555 - g4gan - INFO - Epoch: 361/500\n",
            "INFO:g4gan:Epoch: 361/500\n",
            "2023-04-25 12:24:33,256 - g4gan - INFO - Epoch: 362/500\n",
            "INFO:g4gan:Epoch: 362/500\n",
            "2023-04-25 12:24:33,961 - g4gan - INFO - Epoch: 363/500\n",
            "INFO:g4gan:Epoch: 363/500\n",
            "2023-04-25 12:24:34,672 - g4gan - INFO - Epoch: 364/500\n",
            "INFO:g4gan:Epoch: 364/500\n",
            "2023-04-25 12:24:35,373 - g4gan - INFO - Epoch: 365/500\n",
            "INFO:g4gan:Epoch: 365/500\n",
            "2023-04-25 12:24:36,071 - g4gan - INFO - Epoch: 366/500\n",
            "INFO:g4gan:Epoch: 366/500\n",
            "2023-04-25 12:24:36,782 - g4gan - INFO - Epoch: 367/500\n",
            "INFO:g4gan:Epoch: 367/500\n",
            "2023-04-25 12:24:37,522 - g4gan - INFO - Epoch: 368/500\n",
            "INFO:g4gan:Epoch: 368/500\n",
            "2023-04-25 12:24:38,240 - g4gan - INFO - Epoch: 369/500\n",
            "INFO:g4gan:Epoch: 369/500\n",
            "2023-04-25 12:24:38,963 - g4gan - INFO - Epoch: 370/500\n",
            "INFO:g4gan:Epoch: 370/500\n",
            "2023-04-25 12:24:39,676 - g4gan - INFO - Epoch: 371/500\n",
            "INFO:g4gan:Epoch: 371/500\n",
            "2023-04-25 12:24:40,397 - g4gan - INFO - Epoch: 372/500\n",
            "INFO:g4gan:Epoch: 372/500\n",
            "2023-04-25 12:24:41,093 - g4gan - INFO - Epoch: 373/500\n",
            "INFO:g4gan:Epoch: 373/500\n",
            "2023-04-25 12:24:41,903 - g4gan - INFO - Epoch: 374/500\n",
            "INFO:g4gan:Epoch: 374/500\n",
            "2023-04-25 12:24:42,733 - g4gan - INFO - Epoch: 375/500\n",
            "INFO:g4gan:Epoch: 375/500\n",
            "2023-04-25 12:24:43,537 - g4gan - INFO - Epoch: 376/500\n",
            "INFO:g4gan:Epoch: 376/500\n",
            "2023-04-25 12:24:44,245 - g4gan - INFO - Epoch: 377/500\n",
            "INFO:g4gan:Epoch: 377/500\n",
            "2023-04-25 12:24:44,950 - g4gan - INFO - Epoch: 378/500\n",
            "INFO:g4gan:Epoch: 378/500\n",
            "2023-04-25 12:24:45,641 - g4gan - INFO - Epoch: 379/500\n",
            "INFO:g4gan:Epoch: 379/500\n",
            "2023-04-25 12:24:46,329 - g4gan - INFO - Epoch: 380/500\n",
            "INFO:g4gan:Epoch: 380/500\n",
            "2023-04-25 12:24:47,027 - g4gan - INFO - Epoch: 381/500\n",
            "INFO:g4gan:Epoch: 381/500\n",
            "2023-04-25 12:24:47,729 - g4gan - INFO - Epoch: 382/500\n",
            "INFO:g4gan:Epoch: 382/500\n",
            "2023-04-25 12:24:48,438 - g4gan - INFO - Epoch: 383/500\n",
            "INFO:g4gan:Epoch: 383/500\n",
            "2023-04-25 12:24:49,148 - g4gan - INFO - Epoch: 384/500\n",
            "INFO:g4gan:Epoch: 384/500\n",
            "2023-04-25 12:24:49,870 - g4gan - INFO - Epoch: 385/500\n",
            "INFO:g4gan:Epoch: 385/500\n",
            "2023-04-25 12:24:50,581 - g4gan - INFO - Epoch: 386/500\n",
            "INFO:g4gan:Epoch: 386/500\n",
            "2023-04-25 12:24:51,300 - g4gan - INFO - Epoch: 387/500\n",
            "INFO:g4gan:Epoch: 387/500\n",
            "2023-04-25 12:24:52,006 - g4gan - INFO - Epoch: 388/500\n",
            "INFO:g4gan:Epoch: 388/500\n",
            "2023-04-25 12:24:52,703 - g4gan - INFO - Epoch: 389/500\n",
            "INFO:g4gan:Epoch: 389/500\n",
            "2023-04-25 12:24:53,432 - g4gan - INFO - Epoch: 390/500\n",
            "INFO:g4gan:Epoch: 390/500\n",
            "2023-04-25 12:24:54,271 - g4gan - INFO - Epoch: 391/500\n",
            "INFO:g4gan:Epoch: 391/500\n",
            "2023-04-25 12:24:55,161 - g4gan - INFO - Epoch: 392/500\n",
            "INFO:g4gan:Epoch: 392/500\n",
            "2023-04-25 12:24:55,900 - g4gan - INFO - Epoch: 393/500\n",
            "INFO:g4gan:Epoch: 393/500\n",
            "2023-04-25 12:24:56,596 - g4gan - INFO - Epoch: 394/500\n",
            "INFO:g4gan:Epoch: 394/500\n",
            "2023-04-25 12:24:57,302 - g4gan - INFO - Epoch: 395/500\n",
            "INFO:g4gan:Epoch: 395/500\n",
            "2023-04-25 12:24:58,003 - g4gan - INFO - Epoch: 396/500\n",
            "INFO:g4gan:Epoch: 396/500\n",
            "2023-04-25 12:24:58,728 - g4gan - INFO - Epoch: 397/500\n",
            "INFO:g4gan:Epoch: 397/500\n",
            "2023-04-25 12:24:59,448 - g4gan - INFO - Epoch: 398/500\n",
            "INFO:g4gan:Epoch: 398/500\n",
            "2023-04-25 12:25:00,147 - g4gan - INFO - Epoch: 399/500\n",
            "INFO:g4gan:Epoch: 399/500\n",
            "2023-04-25 12:25:00,840 - g4gan - INFO - Epoch: 400/500\n",
            "INFO:g4gan:Epoch: 400/500\n",
            "2023-04-25 12:25:01,541 - g4gan - INFO - Epoch: 401/500\n",
            "INFO:g4gan:Epoch: 401/500\n",
            "2023-04-25 12:25:02,233 - g4gan - INFO - Epoch: 402/500\n",
            "INFO:g4gan:Epoch: 402/500\n",
            "2023-04-25 12:25:02,937 - g4gan - INFO - Epoch: 403/500\n",
            "INFO:g4gan:Epoch: 403/500\n",
            "2023-04-25 12:25:03,617 - g4gan - INFO - Epoch: 404/500\n",
            "INFO:g4gan:Epoch: 404/500\n",
            "2023-04-25 12:25:04,299 - g4gan - INFO - Epoch: 405/500\n",
            "INFO:g4gan:Epoch: 405/500\n",
            "2023-04-25 12:25:04,992 - g4gan - INFO - Epoch: 406/500\n",
            "INFO:g4gan:Epoch: 406/500\n",
            "2023-04-25 12:25:05,774 - g4gan - INFO - Epoch: 407/500\n",
            "INFO:g4gan:Epoch: 407/500\n",
            "2023-04-25 12:25:06,599 - g4gan - INFO - Epoch: 408/500\n",
            "INFO:g4gan:Epoch: 408/500\n",
            "2023-04-25 12:25:07,446 - g4gan - INFO - Epoch: 409/500\n",
            "INFO:g4gan:Epoch: 409/500\n",
            "2023-04-25 12:25:08,165 - g4gan - INFO - Epoch: 410/500\n",
            "INFO:g4gan:Epoch: 410/500\n",
            "2023-04-25 12:25:08,877 - g4gan - INFO - Epoch: 411/500\n",
            "INFO:g4gan:Epoch: 411/500\n",
            "2023-04-25 12:25:09,588 - g4gan - INFO - Epoch: 412/500\n",
            "INFO:g4gan:Epoch: 412/500\n",
            "2023-04-25 12:25:10,303 - g4gan - INFO - Epoch: 413/500\n",
            "INFO:g4gan:Epoch: 413/500\n",
            "2023-04-25 12:25:10,988 - g4gan - INFO - Epoch: 414/500\n",
            "INFO:g4gan:Epoch: 414/500\n",
            "2023-04-25 12:25:11,705 - g4gan - INFO - Epoch: 415/500\n",
            "INFO:g4gan:Epoch: 415/500\n",
            "2023-04-25 12:25:12,409 - g4gan - INFO - Epoch: 416/500\n",
            "INFO:g4gan:Epoch: 416/500\n",
            "2023-04-25 12:25:13,105 - g4gan - INFO - Epoch: 417/500\n",
            "INFO:g4gan:Epoch: 417/500\n",
            "2023-04-25 12:25:13,813 - g4gan - INFO - Epoch: 418/500\n",
            "INFO:g4gan:Epoch: 418/500\n",
            "2023-04-25 12:25:14,514 - g4gan - INFO - Epoch: 419/500\n",
            "INFO:g4gan:Epoch: 419/500\n",
            "2023-04-25 12:25:15,218 - g4gan - INFO - Epoch: 420/500\n",
            "INFO:g4gan:Epoch: 420/500\n",
            "2023-04-25 12:25:15,921 - g4gan - INFO - Epoch: 421/500\n",
            "INFO:g4gan:Epoch: 421/500\n",
            "2023-04-25 12:25:16,631 - g4gan - INFO - Epoch: 422/500\n",
            "INFO:g4gan:Epoch: 422/500\n",
            "2023-04-25 12:25:17,338 - g4gan - INFO - Epoch: 423/500\n",
            "INFO:g4gan:Epoch: 423/500\n",
            "2023-04-25 12:25:18,158 - g4gan - INFO - Epoch: 424/500\n",
            "INFO:g4gan:Epoch: 424/500\n",
            "2023-04-25 12:25:18,986 - g4gan - INFO - Epoch: 425/500\n",
            "INFO:g4gan:Epoch: 425/500\n",
            "2023-04-25 12:25:19,781 - g4gan - INFO - Epoch: 426/500\n",
            "INFO:g4gan:Epoch: 426/500\n",
            "2023-04-25 12:25:20,481 - g4gan - INFO - Epoch: 427/500\n",
            "INFO:g4gan:Epoch: 427/500\n",
            "2023-04-25 12:25:21,181 - g4gan - INFO - Epoch: 428/500\n",
            "INFO:g4gan:Epoch: 428/500\n",
            "2023-04-25 12:25:21,880 - g4gan - INFO - Epoch: 429/500\n",
            "INFO:g4gan:Epoch: 429/500\n",
            "2023-04-25 12:25:22,583 - g4gan - INFO - Epoch: 430/500\n",
            "INFO:g4gan:Epoch: 430/500\n",
            "2023-04-25 12:25:23,281 - g4gan - INFO - Epoch: 431/500\n",
            "INFO:g4gan:Epoch: 431/500\n",
            "2023-04-25 12:25:23,971 - g4gan - INFO - Epoch: 432/500\n",
            "INFO:g4gan:Epoch: 432/500\n",
            "2023-04-25 12:25:24,668 - g4gan - INFO - Epoch: 433/500\n",
            "INFO:g4gan:Epoch: 433/500\n",
            "2023-04-25 12:25:25,369 - g4gan - INFO - Epoch: 434/500\n",
            "INFO:g4gan:Epoch: 434/500\n",
            "2023-04-25 12:25:26,072 - g4gan - INFO - Epoch: 435/500\n",
            "INFO:g4gan:Epoch: 435/500\n",
            "2023-04-25 12:25:26,781 - g4gan - INFO - Epoch: 436/500\n",
            "INFO:g4gan:Epoch: 436/500\n",
            "2023-04-25 12:25:27,505 - g4gan - INFO - Epoch: 437/500\n",
            "INFO:g4gan:Epoch: 437/500\n",
            "2023-04-25 12:25:28,208 - g4gan - INFO - Epoch: 438/500\n",
            "INFO:g4gan:Epoch: 438/500\n",
            "2023-04-25 12:25:28,904 - g4gan - INFO - Epoch: 439/500\n",
            "INFO:g4gan:Epoch: 439/500\n",
            "2023-04-25 12:25:29,660 - g4gan - INFO - Epoch: 440/500\n",
            "INFO:g4gan:Epoch: 440/500\n",
            "2023-04-25 12:25:30,489 - g4gan - INFO - Epoch: 441/500\n",
            "INFO:g4gan:Epoch: 441/500\n",
            "2023-04-25 12:25:31,355 - g4gan - INFO - Epoch: 442/500\n",
            "INFO:g4gan:Epoch: 442/500\n",
            "2023-04-25 12:25:32,072 - g4gan - INFO - Epoch: 443/500\n",
            "INFO:g4gan:Epoch: 443/500\n",
            "2023-04-25 12:25:32,770 - g4gan - INFO - Epoch: 444/500\n",
            "INFO:g4gan:Epoch: 444/500\n",
            "2023-04-25 12:25:33,467 - g4gan - INFO - Epoch: 445/500\n",
            "INFO:g4gan:Epoch: 445/500\n",
            "2023-04-25 12:25:34,170 - g4gan - INFO - Epoch: 446/500\n",
            "INFO:g4gan:Epoch: 446/500\n",
            "2023-04-25 12:25:34,881 - g4gan - INFO - Epoch: 447/500\n",
            "INFO:g4gan:Epoch: 447/500\n",
            "2023-04-25 12:25:35,587 - g4gan - INFO - Epoch: 448/500\n",
            "INFO:g4gan:Epoch: 448/500\n",
            "2023-04-25 12:25:36,302 - g4gan - INFO - Epoch: 449/500\n",
            "INFO:g4gan:Epoch: 449/500\n",
            "2023-04-25 12:25:36,993 - g4gan - INFO - Epoch: 450/500\n",
            "INFO:g4gan:Epoch: 450/500\n",
            "2023-04-25 12:25:37,680 - g4gan - INFO - Epoch: 451/500\n",
            "INFO:g4gan:Epoch: 451/500\n",
            "2023-04-25 12:25:38,371 - g4gan - INFO - Epoch: 452/500\n",
            "INFO:g4gan:Epoch: 452/500\n",
            "2023-04-25 12:25:39,078 - g4gan - INFO - Epoch: 453/500\n",
            "INFO:g4gan:Epoch: 453/500\n",
            "2023-04-25 12:25:39,770 - g4gan - INFO - Epoch: 454/500\n",
            "INFO:g4gan:Epoch: 454/500\n",
            "2023-04-25 12:25:40,464 - g4gan - INFO - Epoch: 455/500\n",
            "INFO:g4gan:Epoch: 455/500\n",
            "2023-04-25 12:25:41,171 - g4gan - INFO - Epoch: 456/500\n",
            "INFO:g4gan:Epoch: 456/500\n",
            "2023-04-25 12:25:41,994 - g4gan - INFO - Epoch: 457/500\n",
            "INFO:g4gan:Epoch: 457/500\n",
            "2023-04-25 12:25:42,823 - g4gan - INFO - Epoch: 458/500\n",
            "INFO:g4gan:Epoch: 458/500\n",
            "2023-04-25 12:25:43,646 - g4gan - INFO - Epoch: 459/500\n",
            "INFO:g4gan:Epoch: 459/500\n",
            "2023-04-25 12:25:44,353 - g4gan - INFO - Epoch: 460/500\n",
            "INFO:g4gan:Epoch: 460/500\n",
            "2023-04-25 12:25:45,059 - g4gan - INFO - Epoch: 461/500\n",
            "INFO:g4gan:Epoch: 461/500\n",
            "2023-04-25 12:25:45,765 - g4gan - INFO - Epoch: 462/500\n",
            "INFO:g4gan:Epoch: 462/500\n",
            "2023-04-25 12:25:46,468 - g4gan - INFO - Epoch: 463/500\n",
            "INFO:g4gan:Epoch: 463/500\n",
            "2023-04-25 12:25:47,171 - g4gan - INFO - Epoch: 464/500\n",
            "INFO:g4gan:Epoch: 464/500\n",
            "2023-04-25 12:25:47,868 - g4gan - INFO - Epoch: 465/500\n",
            "INFO:g4gan:Epoch: 465/500\n",
            "2023-04-25 12:25:48,569 - g4gan - INFO - Epoch: 466/500\n",
            "INFO:g4gan:Epoch: 466/500\n",
            "2023-04-25 12:25:49,264 - g4gan - INFO - Epoch: 467/500\n",
            "INFO:g4gan:Epoch: 467/500\n",
            "2023-04-25 12:25:49,973 - g4gan - INFO - Epoch: 468/500\n",
            "INFO:g4gan:Epoch: 468/500\n",
            "2023-04-25 12:25:50,670 - g4gan - INFO - Epoch: 469/500\n",
            "INFO:g4gan:Epoch: 469/500\n",
            "2023-04-25 12:25:51,388 - g4gan - INFO - Epoch: 470/500\n",
            "INFO:g4gan:Epoch: 470/500\n",
            "2023-04-25 12:25:52,095 - g4gan - INFO - Epoch: 471/500\n",
            "INFO:g4gan:Epoch: 471/500\n",
            "2023-04-25 12:25:52,801 - g4gan - INFO - Epoch: 472/500\n",
            "INFO:g4gan:Epoch: 472/500\n",
            "2023-04-25 12:25:53,505 - g4gan - INFO - Epoch: 473/500\n",
            "INFO:g4gan:Epoch: 473/500\n",
            "2023-04-25 12:25:54,346 - g4gan - INFO - Epoch: 474/500\n",
            "INFO:g4gan:Epoch: 474/500\n",
            "2023-04-25 12:25:55,170 - g4gan - INFO - Epoch: 475/500\n",
            "INFO:g4gan:Epoch: 475/500\n",
            "2023-04-25 12:25:55,940 - g4gan - INFO - Epoch: 476/500\n",
            "INFO:g4gan:Epoch: 476/500\n",
            "2023-04-25 12:25:56,628 - g4gan - INFO - Epoch: 477/500\n",
            "INFO:g4gan:Epoch: 477/500\n",
            "2023-04-25 12:25:57,348 - g4gan - INFO - Epoch: 478/500\n",
            "INFO:g4gan:Epoch: 478/500\n",
            "2023-04-25 12:25:58,048 - g4gan - INFO - Epoch: 479/500\n",
            "INFO:g4gan:Epoch: 479/500\n",
            "2023-04-25 12:25:58,750 - g4gan - INFO - Epoch: 480/500\n",
            "INFO:g4gan:Epoch: 480/500\n",
            "2023-04-25 12:25:59,448 - g4gan - INFO - Epoch: 481/500\n",
            "INFO:g4gan:Epoch: 481/500\n",
            "2023-04-25 12:26:00,135 - g4gan - INFO - Epoch: 482/500\n",
            "INFO:g4gan:Epoch: 482/500\n",
            "2023-04-25 12:26:00,830 - g4gan - INFO - Epoch: 483/500\n",
            "INFO:g4gan:Epoch: 483/500\n",
            "2023-04-25 12:26:01,536 - g4gan - INFO - Epoch: 484/500\n",
            "INFO:g4gan:Epoch: 484/500\n",
            "2023-04-25 12:26:02,239 - g4gan - INFO - Epoch: 485/500\n",
            "INFO:g4gan:Epoch: 485/500\n",
            "2023-04-25 12:26:02,939 - g4gan - INFO - Epoch: 486/500\n",
            "INFO:g4gan:Epoch: 486/500\n",
            "2023-04-25 12:26:03,642 - g4gan - INFO - Epoch: 487/500\n",
            "INFO:g4gan:Epoch: 487/500\n",
            "2023-04-25 12:26:04,339 - g4gan - INFO - Epoch: 488/500\n",
            "INFO:g4gan:Epoch: 488/500\n",
            "2023-04-25 12:26:05,044 - g4gan - INFO - Epoch: 489/500\n",
            "INFO:g4gan:Epoch: 489/500\n",
            "2023-04-25 12:26:05,804 - g4gan - INFO - Epoch: 490/500\n",
            "INFO:g4gan:Epoch: 490/500\n",
            "2023-04-25 12:26:06,638 - g4gan - INFO - Epoch: 491/500\n",
            "INFO:g4gan:Epoch: 491/500\n",
            "2023-04-25 12:26:07,485 - g4gan - INFO - Epoch: 492/500\n",
            "INFO:g4gan:Epoch: 492/500\n",
            "2023-04-25 12:26:08,231 - g4gan - INFO - Epoch: 493/500\n",
            "INFO:g4gan:Epoch: 493/500\n",
            "2023-04-25 12:26:08,943 - g4gan - INFO - Epoch: 494/500\n",
            "INFO:g4gan:Epoch: 494/500\n",
            "2023-04-25 12:26:09,643 - g4gan - INFO - Epoch: 495/500\n",
            "INFO:g4gan:Epoch: 495/500\n",
            "2023-04-25 12:26:10,327 - g4gan - INFO - Epoch: 496/500\n",
            "INFO:g4gan:Epoch: 496/500\n",
            "2023-04-25 12:26:11,023 - g4gan - INFO - Epoch: 497/500\n",
            "INFO:g4gan:Epoch: 497/500\n",
            "2023-04-25 12:26:11,727 - g4gan - INFO - Epoch: 498/500\n",
            "INFO:g4gan:Epoch: 498/500\n",
            "2023-04-25 12:26:12,454 - g4gan - INFO - Epoch: 499/500\n",
            "INFO:g4gan:Epoch: 499/500\n",
            "2023-04-25 12:26:13,163 - g4gan - INFO - Epoch: 500/500\n",
            "INFO:g4gan:Epoch: 500/500\n",
            "<ipython-input-9-14f7137be74b>:152: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  'cost': np.asscalar(D_cost_train.data.numpy().astype(float)),\n",
            "<ipython-input-9-14f7137be74b>:154: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  np.asscalar(D_wass_train.data.numpy().astype(float)),\n",
            "<ipython-input-9-14f7137be74b>:156: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  np.asscalar(D_cost_valid.data.numpy().astype(float)),\n",
            "<ipython-input-9-14f7137be74b>:158: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  np.asscalar(D_wass_valid.data.numpy().astype(float)),\n",
            "<ipython-input-9-14f7137be74b>:184: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  np.asscalar(G_cost.data.numpy().astype(float))\n",
            "2023-04-25 12:26:15,660 - g4gan - INFO - {'discriminator': [{'cost': 8.21246337890625,\n",
            "                    'cost_validation': 8.115935325622559,\n",
            "                    'wasserstein_cost': -0.019944552332162857,\n",
            "                    'wasserstein_cost_validation': 0.047249484807252884}],\n",
            " 'generator': {'cost': 0.03858613222837448}}\n",
            "INFO:g4gan:{'discriminator': [{'cost': 8.21246337890625,\n",
            "                    'cost_validation': 8.115935325622559,\n",
            "                    'wasserstein_cost': -0.019944552332162857,\n",
            "                    'wasserstein_cost_validation': 0.047249484807252884}],\n",
            " 'generator': {'cost': 0.03858613222837448}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-25 12:26:15,903 - g4gan - INFO - Calculate KNN score...\n",
            "INFO:g4gan:Calculate KNN score...\n",
            "2023-04-25 12:26:17,518 - g4gan - INFO - D_self: 202.91\n",
            "INFO:g4gan:D_self: 202.91\n",
            "2023-04-25 12:26:17,525 - g4gan - INFO - D_train: 327.41\n",
            "INFO:g4gan:D_train: 327.41\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-25 12:26:17,768 - g4gan - INFO - Generating samples...\n",
            "INFO:g4gan:Generating samples...\n",
            "2023-04-25 12:26:17,779 - g4gan - INFO - Saving samples...\n",
            "INFO:g4gan:Saving samples...\n",
            "<ipython-input-9-14f7137be74b>:279: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  noise_v = autograd.Variable(noise, volatile=True)  # totally freeze generator\n",
            "2023-04-25 12:26:17,874 - g4gan - INFO - Finished training.\n",
            "INFO:g4gan:Finished training.\n",
            "2023-04-25 12:26:17,878 - g4gan - INFO - Final discriminator loss on validation and test:\n",
            "INFO:g4gan:Final discriminator loss on validation and test:\n",
            "2023-04-25 12:26:17,881 - g4gan - INFO - {'cost_test': 0,\n",
            " 'cost_validation': 0,\n",
            " 'wasserstein_cost_test': 0,\n",
            " 'wasserstein_cost_validation': 0}\n",
            "INFO:g4gan:{'cost_test': 0,\n",
            " 'cost_validation': 0,\n",
            " 'wasserstein_cost_test': 0,\n",
            " 'wasserstein_cost_validation': 0}\n",
            "2023-04-25 12:26:17,884 - g4gan - INFO - Saving models...\n",
            "INFO:g4gan:Saving models...\n",
            "2023-04-25 12:26:17,916 - g4gan - INFO - Saving metrics...\n",
            "INFO:g4gan:Saving metrics...\n",
            "2023-04-25 12:26:17,918 - g4gan - INFO - Done!\n",
            "INFO:g4gan:Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0         1         2         3         4         5         6    \\\n",
            "0          C         N         C         N         N         N         N   \n",
            "1   0.565237  0.521915  0.558802  0.585062  0.508376  0.579926  0.485593   \n",
            "2          C         C         C         G         N         C         N   \n",
            "3    0.50101   0.56391  0.516895  0.562523   0.46555  0.539541  0.497191   \n",
            "4          N         N         C         C         C         C         C   \n",
            "5    0.49821  0.530073   0.52147  0.523455  0.486246  0.536692  0.532464   \n",
            "6          N         C         N         N         N         N         C   \n",
            "7   0.494064  0.507345  0.533235  0.526975  0.515166   0.51163  0.517465   \n",
            "8          N         C         N         C         G         C         C   \n",
            "9   0.596625  0.542371  0.501561  0.523107  0.478076  0.554793  0.533181   \n",
            "10         C         C         C         C         N         N         G   \n",
            "11  0.550846  0.510943  0.529528  0.517295  0.548841  0.509957   0.51101   \n",
            "12         C         N         G         C         N         N         C   \n",
            "13  0.589014  0.553486  0.555534  0.564604  0.532168  0.578645  0.529081   \n",
            "14         C         C         N         N         A         N         C   \n",
            "15  0.495211  0.591753  0.523872  0.563063   0.53911  0.544269  0.588377   \n",
            "16         C         C         C         C         C         N         C   \n",
            "17  0.521103  0.557203  0.593047  0.540703  0.512444  0.558258  0.534725   \n",
            "18         A         C         N         N         C         N         C   \n",
            "19  0.476887  0.567653  0.569496  0.535351  0.546532  0.545476  0.549637   \n",
            "20         C         N         C         A         C         G         G   \n",
            "21  0.497721  0.540552  0.578393  0.574541  0.483597  0.519683  0.538951   \n",
            "22         C         C         C         A         N         C         N   \n",
            "23  0.550293  0.518485  0.471666  0.567391  0.557734  0.545856  0.527443   \n",
            "24         N         G         N         G         N         C         N   \n",
            "25  0.573319   0.53182  0.539396  0.527356  0.530977  0.503993  0.552325   \n",
            "26         A         C         G         N         N         N         N   \n",
            "27  0.586739  0.504163  0.541493  0.543545  0.607702  0.519816  0.559917   \n",
            "28         C         N         N         G         N         N         N   \n",
            "29  0.571645  0.477604  0.513985  0.455555   0.53653  0.623158  0.551733   \n",
            "30         C         N         N         C         C         N         C   \n",
            "31  0.518625   0.56037  0.558689  0.546036  0.592143  0.513631  0.548528   \n",
            "32         N         C         C         N         C         N         A   \n",
            "33  0.505565  0.533457  0.512873   0.54721  0.512355   0.53514  0.482871   \n",
            "34         C         N         N         N         N         C         G   \n",
            "35  0.585757  0.502673  0.544279  0.551364  0.554299  0.559453   0.53674   \n",
            "36         N         N         N         C         N         N         N   \n",
            "37  0.529322   0.52601  0.561681   0.57484   0.51665  0.554301  0.517105   \n",
            "38         N         C         C         C         N         N         G   \n",
            "39  0.561555  0.554736  0.582199  0.531321  0.588841  0.487409  0.514041   \n",
            "\n",
            "         7         8         9    ...       502       503       504       505  \\\n",
            "0          C         G         C  ...         G         A         N         C   \n",
            "1   0.544269  0.589376  0.491992  ...  0.553628  0.554994  0.534675  0.546724   \n",
            "2          N         N         N  ...         N         C         G         C   \n",
            "3   0.527618  0.514235  0.522624  ...   0.53505  0.493595  0.553503  0.565613   \n",
            "4          N         N         C  ...         N         C         G         N   \n",
            "5   0.509828  0.544005  0.541203  ...  0.573618  0.549569  0.550784  0.519161   \n",
            "6          C         C         N  ...         N         G         C         C   \n",
            "7   0.501844  0.529095  0.552622  ...   0.54359  0.521702  0.574553   0.52561   \n",
            "8          N         C         N  ...         C         C         T         C   \n",
            "9   0.518238  0.517558  0.553566  ...  0.539995  0.549091  0.521094  0.483563   \n",
            "10         G         C         N  ...         C         C         N         C   \n",
            "11  0.524593  0.551061  0.508033  ...  0.532202  0.518806  0.567515  0.538977   \n",
            "12         G         G         C  ...         G         N         G         C   \n",
            "13   0.58199  0.538422  0.500601  ...  0.540147  0.540268  0.482855  0.550554   \n",
            "14         C         G         C  ...         G         N         C         N   \n",
            "15  0.495672  0.551282  0.531047  ...  0.552042   0.55857  0.564924   0.53923   \n",
            "16         C         N         N  ...         C         G         N         G   \n",
            "17  0.566735  0.579957  0.605382  ...  0.539002  0.606171  0.554338  0.493096   \n",
            "18         C         N         N  ...         N         C         G         N   \n",
            "19  0.493366  0.501113  0.544232  ...  0.557008  0.514439  0.548022  0.545215   \n",
            "20         C         N         C  ...         N         C         N         N   \n",
            "21  0.564852  0.526091  0.533261  ...   0.54374   0.52806  0.498372  0.568832   \n",
            "22         C         A         N  ...         C         C         N         C   \n",
            "23  0.557315  0.512347  0.547837  ...  0.524104  0.581166  0.558204  0.550027   \n",
            "24         N         C         C  ...         N         C         C         N   \n",
            "25  0.515369  0.531976  0.479693  ...  0.510242  0.517362  0.544969  0.538505   \n",
            "26         C         N         N  ...         N         N         N         N   \n",
            "27  0.527651  0.523984  0.516834  ...  0.535765  0.544879  0.515163  0.596448   \n",
            "28         N         G         C  ...         N         N         G         G   \n",
            "29   0.51731  0.584558  0.514873  ...  0.572886  0.561747  0.585799  0.523758   \n",
            "30         N         N         N  ...         G         C         C         N   \n",
            "31  0.506653  0.542052  0.577463  ...  0.584572  0.557602  0.566546  0.580132   \n",
            "32         N         G         C  ...         N         C         N         G   \n",
            "33  0.590932  0.579421   0.51247  ...  0.541739  0.539931  0.494106  0.564605   \n",
            "34         C         N         C  ...         N         N         N         N   \n",
            "35  0.537731  0.550656  0.540303  ...  0.535973  0.545789  0.528982  0.536078   \n",
            "36         N         C         N  ...         C         G         G         N   \n",
            "37  0.502153  0.541652  0.517477  ...   0.53792  0.489303  0.601306  0.529333   \n",
            "38         C         N         N  ...         C         N         N         C   \n",
            "39  0.566639  0.524631  0.480374  ...  0.564153   0.52799  0.576293  0.552564   \n",
            "\n",
            "         506       507       508       509       510       511  \n",
            "0          N         G         A         G         C         C  \n",
            "1   0.534159   0.56006  0.552701  0.563051  0.572664  0.560296  \n",
            "2          N         N         N         N         G         N  \n",
            "3   0.543966   0.53292   0.53596  0.516806  0.559381  0.515352  \n",
            "4          N         G         N         N         A         N  \n",
            "5   0.530136  0.542189  0.607655  0.587585  0.578215  0.567244  \n",
            "6          G         N         C         G         N         N  \n",
            "7    0.57903  0.581389  0.579702  0.559516  0.513858  0.592376  \n",
            "8          A         G         C         C         C         C  \n",
            "9   0.589267  0.605236  0.504065  0.517004  0.566914  0.544425  \n",
            "10         N         C         N         C         C         C  \n",
            "11  0.562401  0.575346    0.5428  0.514333  0.558747  0.567507  \n",
            "12         G         G         G         G         C         G  \n",
            "13  0.568816  0.570799  0.606324  0.522741  0.575907  0.580476  \n",
            "14         C         N         C         C         C         C  \n",
            "15  0.493719  0.556139  0.535096  0.486737  0.558478  0.574512  \n",
            "16         C         C         N         N         N         C  \n",
            "17  0.519866  0.538537  0.588337  0.519944  0.565217  0.502281  \n",
            "18         C         N         A         N         N         G  \n",
            "19  0.543151  0.548353  0.574283   0.54017  0.561917  0.559145  \n",
            "20         C         N         G         C         C         C  \n",
            "21  0.575536  0.547519  0.522421  0.560141  0.573605  0.514186  \n",
            "22         N         N         C         C         N         N  \n",
            "23  0.521022  0.559872  0.553849  0.625788  0.540709  0.476957  \n",
            "24         C         N         G         N         C         N  \n",
            "25  0.559113   0.55601  0.575377  0.576449    0.4891  0.568134  \n",
            "26         C         C         N         G         C         G  \n",
            "27  0.519659  0.547154  0.587381  0.531331  0.547592   0.50763  \n",
            "28         C         C         N         C         C         N  \n",
            "29  0.583685  0.541083  0.514194  0.543506  0.539315   0.58554  \n",
            "30         C         N         C         C         C         C  \n",
            "31  0.471589  0.536969  0.536627   0.59106  0.514072  0.529857  \n",
            "32         C         N         N         T         N         A  \n",
            "33   0.56197  0.512834  0.573813  0.494542  0.511963  0.583469  \n",
            "34         N         N         G         C         C         N  \n",
            "35  0.578152  0.532035   0.57186  0.490029  0.524253  0.527889  \n",
            "36         C         N         N         N         N         C  \n",
            "37  0.507879  0.542462  0.495886  0.578144   0.51127  0.499279  \n",
            "38         C         C         C         C         C         G  \n",
            "39  0.562471  0.553407  0.590052  0.521472  0.571599  0.557632  \n",
            "\n",
            "[40 rows x 512 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SAA0v6ctXMQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_path = 'g4gan_big.pkl'\n",
        "model_size = 32\n",
        "seq_len = 512\n",
        "onehot_len = 5\n",
        "hidden_state_len = 128\n",
        "g4gan = load_g4gan_generator(weights_path, model_size, seq_len,\n",
        " onehot_len)\n",
        "g4gan.eval()"
      ],
      "metadata": {
        "id": "elTlFLY1XO6t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "4a454493-02f3-4956-f640-fbe4e4ed4358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4d2a63625bf8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0monehot_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhidden_state_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m g4gan = load_g4gan_generator(weights_path, model_size, seq_len,\n\u001b[0m\u001b[1;32m      7\u001b[0m  onehot_len)\n\u001b[1;32m      8\u001b[0m \u001b[0mg4gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a3cdab856222>\u001b[0m in \u001b[0;36mload_g4gan_generator\u001b[0;34m(filepath, model_size, num_channels, latent_dim, post_proc_filt_len, upsample, last_channel_is_prob, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m ):\n\u001b[0;32m---> 96\u001b[0;31m     model = G4GANGenerator(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mmodel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mngpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mngpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ngpus'"
          ]
        }
      ]
    }
  ]
}